{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Problème Technique GAC",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7RMjTu_l7uO",
        "colab_type": "text"
      },
      "source": [
        "Objectif approximer la fonction f:x->2*cos(x)+4 sur [0,10]\n",
        "grâce à un percpetron multicouche(soit un réseaux de neuronnes)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pA6ycWMb0xC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "af91cfa9-ab7f-467f-f637-fe2b0b985695"
      },
      "source": [
        "# example of creating a univariate dataset with a given mapping function\n",
        "from matplotlib import pyplot\n",
        "from numpy import cos\n",
        "# define the input data\n",
        "x = [i/100 for i in range(0,1000)]\n",
        "# define the output data\n",
        "y = [2.0*cos(i)+4 for i in x]\n",
        "\n",
        "# plot the input versus the output\n",
        "pyplot.scatter(x,y)\n",
        "pyplot.title('Input (x) versus Output (y)')\n",
        "pyplot.xlabel('Input Variable (x)')\n",
        "pyplot.ylabel('Output Variable (y)')\n",
        "pyplot.show()\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3D5KYQGb3F4",
        "colab_type": "text"
      },
      "source": [
        "La fonction à approximer correpsond donc à un signal sinusoïdale. La prédiction sera faite via le réseau de neuronnes.\n",
        "L'étude du signal par un réseaux de neuronnes nécessite le formatage de notre data set x vecteurs des données en entrée et y vecteurs des données attendues en résultats afin de pouvoir calculer l'erreur d'approximation faite par le réseaux."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHrf1YvYoXgJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bb8de5cd-7348-4fd6-8eb4-f3f903e6cb6a"
      },
      "source": [
        "# define the dataset\n",
        "import numpy as np\n",
        "x = np.asarray([i/100 for i in range(0,1001)])\n",
        "y = np.asarray([2.0*cos(i)+4 for i in x])\n",
        "print(x.min(), x.max(), y.min(), y.max())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0 10.0 2.0000025365449208 6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX30l4Wjo67s",
        "colab_type": "text"
      },
      "source": [
        "Les données x et y sont reformatés sous forme de vecteurs afin d'être adapté aux algorithme d'apprentissage automatique sous python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhbi8nAHpHsm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3b81aa66-0c83-4c3d-ecd2-9040b436787e"
      },
      "source": [
        "import sklearn.preprocessing\n",
        "\n",
        "## prediction sur un x_test plus grand\n",
        "x_t = np.asarray([i/100 for i in range(0,1001)])\n",
        "x_t=x_t.reshape((len(x_t), 1))\n",
        "scale_xt = sklearn.preprocessing.MinMaxScaler()\n",
        "x_t = scale_xt.fit_transform(x_t)\n",
        "##\n",
        "\n",
        "x = x.reshape((len(x), 1))\n",
        "y = y.reshape((len(y), 1))\n",
        "x_t=x_t.reshape(len(x_t), 1)\n",
        "\n",
        "# separately scale the input and output variables\n",
        "scale_x = sklearn.preprocessing.MinMaxScaler()\n",
        "x = scale_x.fit_transform(x)\n",
        "scale_y = sklearn.preprocessing.MinMaxScaler()\n",
        "y = scale_y.fit_transform(y)\n",
        "print(x.min(), x.max(), y.min(), y.max())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0 1.0 0.0 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6gb_Nb3rb-S",
        "colab_type": "text"
      },
      "source": [
        "Normalisation des data input et output pour faciliter la prédiction par le réseaux de neurones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EViK4L2Rtfke",
        "colab_type": "text"
      },
      "source": [
        "Construction du réseaux de neurones pour modéliser la fonction f.\n",
        "Un perceptron c’est 3 couches de neurones : couche d’entrée, couche(s) cachée(s), couche de sortie\n",
        "formule de sortie d'un neurone caché y=factivation(b+∑iwi⋅xi)\n",
        "neurone de sortie sera y=∑iwi⋅xi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xgco_qAlsvGY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "588c7461-7bfd-4fb0-e833-652b861c54b4"
      },
      "source": [
        "import keras\n",
        "from keras.layers import Dense, Activation\n",
        "# design the neural network model\n",
        "##initialisation du réseaux de neuronnes vides\n",
        "model = keras.Sequential()\n",
        "#première couche cachée avec 10 noeuds\n",
        "model.add(Dense(10, input_dim=1, activation='softplus', kernel_initializer='he_uniform'))\n",
        "### fonction d'activation est une fonction mathématique appliquée à un signal en sortie d'un neurone artificiel\n",
        "## Softplus activation function, softplus(x) = log(exp(x) + 1) remplacement de la fonction d'activation relu par softplus\n",
        "### résultats beaucoup plus précis \n",
        "#ajout deuxièle couche cachée avec 10 noeuds\n",
        "model.add(Dense(10, activation='softplus', kernel_initializer='he_uniform'))\n",
        "#ajout troisième couche cachée\n",
        "model.add(Dense(10, activation='softplus', kernel_initializer='he_uniform'))\n",
        "# fonction d'activation softplus plus efficace que relu\n",
        "#ajout 4eme\n",
        "#model.add(Dense(10, activation='relu', kernel_initializer='he_uniform'))\n",
        "#ajout couche de sortie\n",
        "model.add(Dense(1))\n",
        "##La couche de sortie (output layer) : cette couche représente le résultat final de notre réseau, sa prédiction."
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvT9u2Mkteyh",
        "colab_type": "text"
      },
      "source": [
        "Phase d’apprentissage :\n",
        " -repose intégralement sur la \"descente de gradient\" .\n",
        "On va adapter le modèle du signal en se basant sur une erreur d'approximation MSE soit l'erreure quadratique moyenne entre les valeurs de sorties attendues et les prédections calculées par le réseaux de neuronnes. \n",
        "De plus on optimise le modèle grâce à l'optimizer 'adam' disponible dans la librairie keras. Cet optimizer permet d'effectuer un gradiant descendant qui permet de diminuer l'erreur d'approximation donc d'optimiser le modèle.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i00TG6p3wDwt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9e4cefcd-2082-4472-8207-63bdfb9516f4"
      },
      "source": [
        "# define the loss function and optimization algorithm\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "##argument loss déterlinant le type d'erreur qu'on choisit de calculer pour déterminer la précision du modèle\n",
        "##optimizer: adam optimisation algorithme ce choix d'optimizer détermine la méthode d'apprentissage et donc de réduction de l'erreur du modèle\n",
        "#### cet algorithme remplace la méthode du gradiant descendant\n",
        "\n",
        "## version alternative avec la méthode du gradiant descendant\n",
        "#model.compile(loss='mse', optimizer='sgd')\n",
        "\n",
        "# fit the model on the training dataset\n",
        "model.fit(x, y, epochs=500, batch_size=10, verbose=0)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f3394b6fd68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icrCI7fWxmeC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3f180298-b0d6-4005-87f2-dbc657f02f62"
      },
      "source": [
        "# make predictions for the input data\n",
        "import numpy as np\n",
        "#x_t=np.random.uniform(10, 30,2001)\n",
        "print(len(x_t))\n",
        "\n",
        "yhat = model.predict(x)\n",
        "#print(yhat)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oLVotGkx4_q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "9e9ac145-d5a2-428b-a2bd-72369c9186e5"
      },
      "source": [
        "# inverse transforms\n",
        "x_plot = scale_x.inverse_transform(x)\n",
        "y_plot = scale_y.inverse_transform(y)\n",
        "yhat_plot = scale_y.inverse_transform(yhat)\n",
        "\n",
        "##\n",
        "#xt_plot = scale_xt.inverse_transform(x_t)\n",
        "\n",
        "\n",
        "\n",
        "#print(len(xt_plot))\n",
        "print(len(yhat))\n",
        "pyplot.scatter(x_plot,y_plot, label='Actual')\n",
        "# plot x vs yhat\n",
        "pyplot.scatter(x_plot,yhat_plot, label='Predicted')\n",
        "# plot x vs yhat\n",
        "pyplot.title('Input (x) versus Output (y)')\n",
        "pyplot.xlabel('Input Variable (x)')\n",
        "pyplot.ylabel('Output Variable (y)')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np-ZKRXb412E",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQvgyZZs42MI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "277d4213-8cd2-4a44-e842-23456044a237"
      },
      "source": [
        "import keras\n",
        "from keras.layers import Dense, Activation\n",
        "\n",
        "import numpy as np\n",
        "# define the input data\n",
        "x = [i/100 for i in range(0,1000)]\n",
        "# define the output data\n",
        "y = [2.0*cos(i)+4 for i in x]\n",
        "\n",
        "xtrain = x[0:750]\n",
        "ytrain = y[0:750]\n",
        "\n",
        "xtest = x[751:1000]\n",
        "ytest = y[751:1000]\n",
        "model = keras.Sequential()\n",
        "\n",
        "model.add(Dense(10, input_dim=1, activation='relu'))\n",
        "model.add(Dense(10, activation='softplus', kernel_initializer='he_uniform'))\n",
        "#ajout troisième couche cachée\n",
        "model.add(Dense(10, activation='softplus', kernel_initializer='he_uniform'))\n",
        "model.add(Dense(1))\n",
        "model.summary()\n",
        "sgd = keras.optimizers.Adam(lr=0.01, decay=1e-6)\n",
        "model.compile(loss='mean_squared_error', \n",
        "          optimizer=sgd, \n",
        "          metrics=['mae'])\n",
        "\n",
        "history = model.fit(xtrain, ytrain,\n",
        "                batch_size=10, \n",
        "                epochs=700, \n",
        "                verbose=1, \n",
        "                validation_split=0.1)\n",
        "\n",
        "\n",
        "yhat = model.predict(xtrain)\n",
        "print(ytest)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_37 (Dense)             (None, 10)                20        \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 251\n",
            "Trainable params: 251\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 675 samples, validate on 75 samples\n",
            "Epoch 1/700\n",
            "675/675 [==============================] - 0s 325us/step - loss: 6.1578 - mae: 1.9014 - val_loss: 0.2802 - val_mae: 0.4458\n",
            "Epoch 2/700\n",
            "675/675 [==============================] - 0s 176us/step - loss: 0.7590 - mae: 0.7558 - val_loss: 0.4344 - val_mae: 0.5255\n",
            "Epoch 3/700\n",
            "675/675 [==============================] - 0s 149us/step - loss: 0.3833 - mae: 0.5097 - val_loss: 1.0717 - val_mae: 0.8985\n",
            "Epoch 4/700\n",
            "675/675 [==============================] - 0s 143us/step - loss: 0.2171 - mae: 0.3738 - val_loss: 2.7210 - val_mae: 1.5512\n",
            "Epoch 5/700\n",
            "675/675 [==============================] - 0s 185us/step - loss: 0.1102 - mae: 0.2723 - val_loss: 3.3450 - val_mae: 1.7290\n",
            "Epoch 6/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 0.0706 - mae: 0.2031 - val_loss: 3.5429 - val_mae: 1.7835\n",
            "Epoch 7/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0611 - mae: 0.1804 - val_loss: 3.9780 - val_mae: 1.9020\n",
            "Epoch 8/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 0.0614 - mae: 0.1852 - val_loss: 3.8965 - val_mae: 1.8813\n",
            "Epoch 9/700\n",
            "675/675 [==============================] - 0s 143us/step - loss: 0.0494 - mae: 0.1563 - val_loss: 3.6397 - val_mae: 1.8122\n",
            "Epoch 10/700\n",
            "675/675 [==============================] - 0s 149us/step - loss: 0.0493 - mae: 0.1544 - val_loss: 3.6292 - val_mae: 1.8096\n",
            "Epoch 11/700\n",
            "675/675 [==============================] - 0s 159us/step - loss: 0.0492 - mae: 0.1530 - val_loss: 3.9160 - val_mae: 1.8880\n",
            "Epoch 12/700\n",
            "675/675 [==============================] - 0s 162us/step - loss: 0.0529 - mae: 0.1529 - val_loss: 2.9443 - val_mae: 1.6149\n",
            "Epoch 13/700\n",
            "675/675 [==============================] - 0s 126us/step - loss: 0.0496 - mae: 0.1604 - val_loss: 4.7650 - val_mae: 2.1007\n",
            "Epoch 14/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0491 - mae: 0.1575 - val_loss: 3.5892 - val_mae: 1.8032\n",
            "Epoch 15/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0483 - mae: 0.1493 - val_loss: 3.8087 - val_mae: 1.8634\n",
            "Epoch 16/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0466 - mae: 0.1537 - val_loss: 4.6990 - val_mae: 2.0870\n",
            "Epoch 17/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0538 - mae: 0.1670 - val_loss: 3.5322 - val_mae: 1.7893\n",
            "Epoch 18/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0388 - mae: 0.1362 - val_loss: 4.1134 - val_mae: 1.9451\n",
            "Epoch 19/700\n",
            "675/675 [==============================] - 0s 143us/step - loss: 0.0382 - mae: 0.1355 - val_loss: 3.4232 - val_mae: 1.7627\n",
            "Epoch 20/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0361 - mae: 0.1354 - val_loss: 3.1979 - val_mae: 1.7000\n",
            "Epoch 21/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 0.0283 - mae: 0.1144 - val_loss: 2.6532 - val_mae: 1.5357\n",
            "Epoch 22/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0278 - mae: 0.1142 - val_loss: 2.8923 - val_mae: 1.6130\n",
            "Epoch 23/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0270 - mae: 0.1101 - val_loss: 2.8016 - val_mae: 1.5879\n",
            "Epoch 24/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 0.0220 - mae: 0.0976 - val_loss: 2.8198 - val_mae: 1.5959\n",
            "Epoch 25/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 0.0251 - mae: 0.1131 - val_loss: 3.1721 - val_mae: 1.7036\n",
            "Epoch 26/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 0.0307 - mae: 0.1249 - val_loss: 2.2815 - val_mae: 1.4241\n",
            "Epoch 27/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0267 - mae: 0.1182 - val_loss: 1.7772 - val_mae: 1.2390\n",
            "Epoch 28/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0191 - mae: 0.0966 - val_loss: 2.0857 - val_mae: 1.3592\n",
            "Epoch 29/700\n",
            "675/675 [==============================] - 0s 145us/step - loss: 0.0162 - mae: 0.0864 - val_loss: 2.1087 - val_mae: 1.3706\n",
            "Epoch 30/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0361 - mae: 0.1427 - val_loss: 2.2410 - val_mae: 1.4189\n",
            "Epoch 31/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 0.0158 - mae: 0.0861 - val_loss: 1.8796 - val_mae: 1.2889\n",
            "Epoch 32/700\n",
            "675/675 [==============================] - 0s 158us/step - loss: 0.0134 - mae: 0.0776 - val_loss: 2.1231 - val_mae: 1.3819\n",
            "Epoch 33/700\n",
            "675/675 [==============================] - 0s 169us/step - loss: 0.0177 - mae: 0.0982 - val_loss: 1.0154 - val_mae: 0.9051\n",
            "Epoch 34/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0122 - mae: 0.0753 - val_loss: 1.6663 - val_mae: 1.2119\n",
            "Epoch 35/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 0.0122 - mae: 0.0755 - val_loss: 1.3746 - val_mae: 1.0896\n",
            "Epoch 36/700\n",
            "675/675 [==============================] - 0s 150us/step - loss: 0.0135 - mae: 0.0817 - val_loss: 1.1205 - val_mae: 0.9703\n",
            "Epoch 37/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0176 - mae: 0.1001 - val_loss: 1.5410 - val_mae: 1.1659\n",
            "Epoch 38/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0097 - mae: 0.0721 - val_loss: 1.3868 - val_mae: 1.1014\n",
            "Epoch 39/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 0.0090 - mae: 0.0695 - val_loss: 1.2707 - val_mae: 1.0509\n",
            "Epoch 40/700\n",
            "675/675 [==============================] - 0s 166us/step - loss: 0.0088 - mae: 0.0692 - val_loss: 1.4216 - val_mae: 1.1219\n",
            "Epoch 41/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0073 - mae: 0.0622 - val_loss: 1.1804 - val_mae: 1.0118\n",
            "Epoch 42/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 0.0083 - mae: 0.0659 - val_loss: 1.3775 - val_mae: 1.1058\n",
            "Epoch 43/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 0.0110 - mae: 0.0789 - val_loss: 0.8490 - val_mae: 0.8380\n",
            "Epoch 44/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 0.0163 - mae: 0.0970 - val_loss: 1.1914 - val_mae: 1.0210\n",
            "Epoch 45/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 0.0086 - mae: 0.0693 - val_loss: 1.2292 - val_mae: 1.0398\n",
            "Epoch 46/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 0.0063 - mae: 0.0589 - val_loss: 0.9537 - val_mae: 0.9017\n",
            "Epoch 47/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0076 - mae: 0.0616 - val_loss: 0.9911 - val_mae: 0.9230\n",
            "Epoch 48/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0056 - mae: 0.0542 - val_loss: 1.0698 - val_mae: 0.9663\n",
            "Epoch 49/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 0.0060 - mae: 0.0539 - val_loss: 1.0097 - val_mae: 0.9373\n",
            "Epoch 50/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 0.0065 - mae: 0.0531 - val_loss: 0.1401 - val_mae: 0.3089\n",
            "Epoch 51/700\n",
            "675/675 [==============================] - 0s 144us/step - loss: 0.0123 - mae: 0.0786 - val_loss: 0.6130 - val_mae: 0.6984\n",
            "Epoch 52/700\n",
            "675/675 [==============================] - 0s 150us/step - loss: 0.0094 - mae: 0.0735 - val_loss: 0.6201 - val_mae: 0.7053\n",
            "Epoch 53/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0078 - mae: 0.0685 - val_loss: 0.4839 - val_mae: 0.6025\n",
            "Epoch 54/700\n",
            "675/675 [==============================] - 0s 147us/step - loss: 0.0056 - mae: 0.0556 - val_loss: 1.0196 - val_mae: 0.9472\n",
            "Epoch 55/700\n",
            "675/675 [==============================] - 0s 149us/step - loss: 0.0072 - mae: 0.0622 - val_loss: 0.8805 - val_mae: 0.8722\n",
            "Epoch 56/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0048 - mae: 0.0527 - val_loss: 0.7075 - val_mae: 0.7706\n",
            "Epoch 57/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 0.0083 - mae: 0.0704 - val_loss: 1.0242 - val_mae: 0.9529\n",
            "Epoch 58/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0102 - mae: 0.0791 - val_loss: 0.6734 - val_mae: 0.7474\n",
            "Epoch 59/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0088 - mae: 0.0710 - val_loss: 0.9298 - val_mae: 0.9020\n",
            "Epoch 60/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 0.0102 - mae: 0.0772 - val_loss: 0.7008 - val_mae: 0.7647\n",
            "Epoch 61/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0132 - mae: 0.0861 - val_loss: 0.9656 - val_mae: 0.9206\n",
            "Epoch 62/700\n",
            "675/675 [==============================] - 0s 149us/step - loss: 0.0080 - mae: 0.0685 - val_loss: 0.8464 - val_mae: 0.8547\n",
            "Epoch 63/700\n",
            "675/675 [==============================] - 0s 146us/step - loss: 0.0048 - mae: 0.0517 - val_loss: 0.7080 - val_mae: 0.7741\n",
            "Epoch 64/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 0.0039 - mae: 0.0464 - val_loss: 0.6193 - val_mae: 0.7184\n",
            "Epoch 65/700\n",
            "675/675 [==============================] - 0s 146us/step - loss: 0.0036 - mae: 0.0455 - val_loss: 0.5998 - val_mae: 0.7061\n",
            "Epoch 66/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 0.0054 - mae: 0.0521 - val_loss: 0.4586 - val_mae: 0.5993\n",
            "Epoch 67/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 0.0051 - mae: 0.0567 - val_loss: 0.5769 - val_mae: 0.6906\n",
            "Epoch 68/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 0.0061 - mae: 0.0571 - val_loss: 0.4562 - val_mae: 0.6003\n",
            "Epoch 69/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 0.0038 - mae: 0.0452 - val_loss: 0.6062 - val_mae: 0.7140\n",
            "Epoch 70/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0059 - mae: 0.0547 - val_loss: 0.5464 - val_mae: 0.6721\n",
            "Epoch 71/700\n",
            "675/675 [==============================] - 0s 145us/step - loss: 0.0043 - mae: 0.0477 - val_loss: 0.2687 - val_mae: 0.4251\n",
            "Epoch 72/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0091 - mae: 0.0725 - val_loss: 0.2322 - val_mae: 0.3852\n",
            "Epoch 73/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0080 - mae: 0.0718 - val_loss: 0.5710 - val_mae: 0.6899\n",
            "Epoch 74/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0045 - mae: 0.0516 - val_loss: 0.5662 - val_mae: 0.6900\n",
            "Epoch 75/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0092 - mae: 0.0718 - val_loss: 0.6879 - val_mae: 0.7717\n",
            "Epoch 76/700\n",
            "675/675 [==============================] - 0s 155us/step - loss: 0.0062 - mae: 0.0558 - val_loss: 0.4358 - val_mae: 0.5892\n",
            "Epoch 77/700\n",
            "675/675 [==============================] - 0s 154us/step - loss: 0.0024 - mae: 0.0357 - val_loss: 0.4439 - val_mae: 0.5995\n",
            "Epoch 78/700\n",
            "675/675 [==============================] - 0s 162us/step - loss: 0.0072 - mae: 0.0642 - val_loss: 0.8541 - val_mae: 0.8694\n",
            "Epoch 79/700\n",
            "675/675 [==============================] - 0s 152us/step - loss: 0.0184 - mae: 0.0988 - val_loss: 0.6059 - val_mae: 0.7100\n",
            "Epoch 80/700\n",
            "675/675 [==============================] - 0s 163us/step - loss: 0.0097 - mae: 0.0664 - val_loss: 0.6408 - val_mae: 0.7328\n",
            "Epoch 81/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0041 - mae: 0.0499 - val_loss: 0.4363 - val_mae: 0.5820\n",
            "Epoch 82/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 0.0064 - mae: 0.0601 - val_loss: 0.4538 - val_mae: 0.6000\n",
            "Epoch 83/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 0.0032 - mae: 0.0431 - val_loss: 0.7009 - val_mae: 0.7809\n",
            "Epoch 84/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0027 - mae: 0.0394 - val_loss: 0.2451 - val_mae: 0.4050\n",
            "Epoch 85/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0037 - mae: 0.0443 - val_loss: 0.1853 - val_mae: 0.3403\n",
            "Epoch 86/700\n",
            "675/675 [==============================] - 0s 144us/step - loss: 0.0050 - mae: 0.0545 - val_loss: 0.5166 - val_mae: 0.6583\n",
            "Epoch 87/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 0.0035 - mae: 0.0442 - val_loss: 0.5252 - val_mae: 0.6680\n",
            "Epoch 88/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0058 - mae: 0.0546 - val_loss: 0.6063 - val_mae: 0.7264\n",
            "Epoch 89/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0076 - mae: 0.0636 - val_loss: 0.8811 - val_mae: 0.8942\n",
            "Epoch 90/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0036 - mae: 0.0465 - val_loss: 0.4802 - val_mae: 0.6364\n",
            "Epoch 91/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0050 - mae: 0.0518 - val_loss: 0.2616 - val_mae: 0.4360\n",
            "Epoch 92/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0052 - mae: 0.0520 - val_loss: 0.5265 - val_mae: 0.6739\n",
            "Epoch 93/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 0.0038 - mae: 0.0457 - val_loss: 0.4895 - val_mae: 0.6448\n",
            "Epoch 94/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0038 - mae: 0.0467 - val_loss: 0.2592 - val_mae: 0.4362\n",
            "Epoch 95/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0037 - mae: 0.0474 - val_loss: 0.2512 - val_mae: 0.4312\n",
            "Epoch 96/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 0.0030 - mae: 0.0425 - val_loss: 0.3320 - val_mae: 0.5164\n",
            "Epoch 97/700\n",
            "675/675 [==============================] - 0s 155us/step - loss: 0.0034 - mae: 0.0450 - val_loss: 0.5130 - val_mae: 0.6694\n",
            "Epoch 98/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0031 - mae: 0.0427 - val_loss: 0.1213 - val_mae: 0.2719\n",
            "Epoch 99/700\n",
            "675/675 [==============================] - 0s 152us/step - loss: 0.0032 - mae: 0.0434 - val_loss: 0.2723 - val_mae: 0.4621\n",
            "Epoch 100/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0043 - mae: 0.0499 - val_loss: 0.8222 - val_mae: 0.8707\n",
            "Epoch 101/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 0.0630 - mae: 0.1800 - val_loss: 0.8101 - val_mae: 0.8267\n",
            "Epoch 102/700\n",
            "675/675 [==============================] - 0s 149us/step - loss: 0.0242 - mae: 0.1175 - val_loss: 0.8356 - val_mae: 0.8385\n",
            "Epoch 103/700\n",
            "675/675 [==============================] - 0s 150us/step - loss: 0.0040 - mae: 0.0454 - val_loss: 0.7858 - val_mae: 0.8195\n",
            "Epoch 104/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0063 - mae: 0.0555 - val_loss: 0.3935 - val_mae: 0.5413\n",
            "Epoch 105/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 0.0067 - mae: 0.0616 - val_loss: 0.6377 - val_mae: 0.7310\n",
            "Epoch 106/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0084 - mae: 0.0707 - val_loss: 0.6830 - val_mae: 0.7611\n",
            "Epoch 107/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0032 - mae: 0.0422 - val_loss: 0.4329 - val_mae: 0.5819\n",
            "Epoch 108/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 0.0047 - mae: 0.0472 - val_loss: 0.3585 - val_mae: 0.5187\n",
            "Epoch 109/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 0.0109 - mae: 0.0753 - val_loss: 0.4588 - val_mae: 0.6078\n",
            "Epoch 110/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 0.0137 - mae: 0.0893 - val_loss: 0.8416 - val_mae: 0.8620\n",
            "Epoch 111/700\n",
            "675/675 [==============================] - 0s 127us/step - loss: 0.0056 - mae: 0.0576 - val_loss: 0.6004 - val_mae: 0.7149\n",
            "Epoch 112/700\n",
            "675/675 [==============================] - 0s 128us/step - loss: 0.0041 - mae: 0.0468 - val_loss: 0.3186 - val_mae: 0.4874\n",
            "Epoch 113/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 0.0027 - mae: 0.0393 - val_loss: 0.5439 - val_mae: 0.6840\n",
            "Epoch 114/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 0.0024 - mae: 0.0365 - val_loss: 0.3576 - val_mae: 0.5386\n",
            "Epoch 115/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 0.0025 - mae: 0.0379 - val_loss: 0.3025 - val_mae: 0.4884\n",
            "Epoch 116/700\n",
            "675/675 [==============================] - 0s 126us/step - loss: 0.0034 - mae: 0.0452 - val_loss: 0.2411 - val_mae: 0.4259\n",
            "Epoch 117/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0029 - mae: 0.0409 - val_loss: 0.2235 - val_mae: 0.4021\n",
            "Epoch 118/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 0.0035 - mae: 0.0463 - val_loss: 0.2717 - val_mae: 0.4636\n",
            "Epoch 119/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 0.0031 - mae: 0.0428 - val_loss: 0.1681 - val_mae: 0.3387\n",
            "Epoch 120/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 0.0125 - mae: 0.0878 - val_loss: 0.3466 - val_mae: 0.5318\n",
            "Epoch 121/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0032 - mae: 0.0442 - val_loss: 0.2576 - val_mae: 0.4441\n",
            "Epoch 122/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0017 - mae: 0.0303 - val_loss: 0.3465 - val_mae: 0.5371\n",
            "Epoch 123/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0027 - mae: 0.0411 - val_loss: 0.2264 - val_mae: 0.4174\n",
            "Epoch 124/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 0.0025 - mae: 0.0387 - val_loss: 0.2016 - val_mae: 0.3907\n",
            "Epoch 125/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0029 - mae: 0.0400 - val_loss: 0.1851 - val_mae: 0.3717\n",
            "Epoch 126/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0011 - mae: 0.0256 - val_loss: 0.2382 - val_mae: 0.4386\n",
            "Epoch 127/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 0.0026 - mae: 0.0392 - val_loss: 0.2578 - val_mae: 0.4599\n",
            "Epoch 128/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 0.0045 - mae: 0.0521 - val_loss: 0.2263 - val_mae: 0.4185\n",
            "Epoch 129/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0054 - mae: 0.0532 - val_loss: 0.5052 - val_mae: 0.6751\n",
            "Epoch 130/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0105 - mae: 0.0786 - val_loss: 0.1455 - val_mae: 0.3014\n",
            "Epoch 131/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0032 - mae: 0.0451 - val_loss: 0.3306 - val_mae: 0.5232\n",
            "Epoch 132/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 0.0018 - mae: 0.0327 - val_loss: 0.1607 - val_mae: 0.3339\n",
            "Epoch 133/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0015 - mae: 0.0303 - val_loss: 0.1741 - val_mae: 0.3599\n",
            "Epoch 134/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0080 - mae: 0.0683 - val_loss: 0.0901 - val_mae: 0.2332\n",
            "Epoch 135/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 0.0049 - mae: 0.0524 - val_loss: 0.3075 - val_mae: 0.4990\n",
            "Epoch 136/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0016 - mae: 0.0311 - val_loss: 0.3931 - val_mae: 0.5838\n",
            "Epoch 137/700\n",
            "675/675 [==============================] - 0s 125us/step - loss: 0.0048 - mae: 0.0512 - val_loss: 0.2553 - val_mae: 0.4562\n",
            "Epoch 138/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 0.0044 - mae: 0.0489 - val_loss: 0.2179 - val_mae: 0.4082\n",
            "Epoch 139/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0035 - mae: 0.0448 - val_loss: 0.2989 - val_mae: 0.4933\n",
            "Epoch 140/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 0.0015 - mae: 0.0292 - val_loss: 0.1347 - val_mae: 0.2964\n",
            "Epoch 141/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 0.0015 - mae: 0.0290 - val_loss: 0.1685 - val_mae: 0.3555\n",
            "Epoch 142/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0012 - mae: 0.0264 - val_loss: 0.1606 - val_mae: 0.3476\n",
            "Epoch 143/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0018 - mae: 0.0332 - val_loss: 0.0972 - val_mae: 0.2457\n",
            "Epoch 144/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0074 - mae: 0.0676 - val_loss: 0.1224 - val_mae: 0.2809\n",
            "Epoch 145/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0082 - mae: 0.0683 - val_loss: 0.8487 - val_mae: 0.8896\n",
            "Epoch 146/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 0.0119 - mae: 0.0783 - val_loss: 0.0574 - val_mae: 0.2095\n",
            "Epoch 147/700\n",
            "675/675 [==============================] - 0s 156us/step - loss: 0.0101 - mae: 0.0722 - val_loss: 0.4420 - val_mae: 0.6104\n",
            "Epoch 148/700\n",
            "675/675 [==============================] - 0s 161us/step - loss: 0.0018 - mae: 0.0326 - val_loss: 0.2495 - val_mae: 0.4434\n",
            "Epoch 149/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0037 - mae: 0.0458 - val_loss: 0.3510 - val_mae: 0.5446\n",
            "Epoch 150/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0027 - mae: 0.0407 - val_loss: 0.2184 - val_mae: 0.4104\n",
            "Epoch 151/700\n",
            "675/675 [==============================] - 0s 147us/step - loss: 0.0036 - mae: 0.0465 - val_loss: 0.3339 - val_mae: 0.5352\n",
            "Epoch 152/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0069 - mae: 0.0609 - val_loss: 0.4970 - val_mae: 0.6582\n",
            "Epoch 153/700\n",
            "675/675 [==============================] - 0s 144us/step - loss: 0.0073 - mae: 0.0684 - val_loss: 0.4726 - val_mae: 0.6395\n",
            "Epoch 154/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 0.0187 - mae: 0.1028 - val_loss: 1.3558 - val_mae: 1.1213\n",
            "Epoch 155/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0115 - mae: 0.0810 - val_loss: 0.3958 - val_mae: 0.5540\n",
            "Epoch 156/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 0.0066 - mae: 0.0625 - val_loss: 0.6762 - val_mae: 0.7774\n",
            "Epoch 157/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 0.0104 - mae: 0.0750 - val_loss: 0.4122 - val_mae: 0.5866\n",
            "Epoch 158/700\n",
            "675/675 [==============================] - 0s 165us/step - loss: 0.0019 - mae: 0.0346 - val_loss: 0.2271 - val_mae: 0.4140\n",
            "Epoch 159/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 0.0025 - mae: 0.0391 - val_loss: 0.1620 - val_mae: 0.3359\n",
            "Epoch 160/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 0.0022 - mae: 0.0352 - val_loss: 0.3439 - val_mae: 0.5413\n",
            "Epoch 161/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0011 - mae: 0.0251 - val_loss: 0.1321 - val_mae: 0.2984\n",
            "Epoch 162/700\n",
            "675/675 [==============================] - 0s 147us/step - loss: 0.0011 - mae: 0.0247 - val_loss: 0.3145 - val_mae: 0.5170\n",
            "Epoch 163/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0022 - mae: 0.0351 - val_loss: 0.1701 - val_mae: 0.3621\n",
            "Epoch 164/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 0.0014 - mae: 0.0287 - val_loss: 0.1308 - val_mae: 0.3059\n",
            "Epoch 165/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0023 - mae: 0.0375 - val_loss: 0.2563 - val_mae: 0.4681\n",
            "Epoch 166/700\n",
            "675/675 [==============================] - 0s 143us/step - loss: 0.0022 - mae: 0.0358 - val_loss: 0.1601 - val_mae: 0.3510\n",
            "Epoch 167/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0022 - mae: 0.0312 - val_loss: 0.0377 - val_mae: 0.1597\n",
            "Epoch 168/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0091 - mae: 0.0701 - val_loss: 0.0487 - val_mae: 0.1844\n",
            "Epoch 169/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 0.0091 - mae: 0.0710 - val_loss: 0.3041 - val_mae: 0.5027\n",
            "Epoch 170/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 0.0016 - mae: 0.0316 - val_loss: 0.0828 - val_mae: 0.2231\n",
            "Epoch 171/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0033 - mae: 0.0421 - val_loss: 0.0673 - val_mae: 0.1994\n",
            "Epoch 172/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 0.0147 - mae: 0.0913 - val_loss: 0.7254 - val_mae: 0.8021\n",
            "Epoch 173/700\n",
            "675/675 [==============================] - 0s 148us/step - loss: 0.0036 - mae: 0.0482 - val_loss: 0.2329 - val_mae: 0.4210\n",
            "Epoch 174/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0014 - mae: 0.0296 - val_loss: 0.1965 - val_mae: 0.3870\n",
            "Epoch 175/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 0.0030 - mae: 0.0387 - val_loss: 0.1378 - val_mae: 0.3073\n",
            "Epoch 176/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 0.0039 - mae: 0.0478 - val_loss: 0.1764 - val_mae: 0.3682\n",
            "Epoch 177/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0035 - mae: 0.0475 - val_loss: 0.1231 - val_mae: 0.2913\n",
            "Epoch 178/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0119 - mae: 0.0840 - val_loss: 0.0523 - val_mae: 0.1915\n",
            "Epoch 179/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0102 - mae: 0.0768 - val_loss: 0.3856 - val_mae: 0.5713\n",
            "Epoch 180/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0019 - mae: 0.0334 - val_loss: 0.2563 - val_mae: 0.4550\n",
            "Epoch 181/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 0.0015 - mae: 0.0295 - val_loss: 0.1711 - val_mae: 0.3582\n",
            "Epoch 182/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0011 - mae: 0.0268 - val_loss: 0.2564 - val_mae: 0.4636\n",
            "Epoch 183/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 8.9043e-04 - mae: 0.0234 - val_loss: 0.1002 - val_mae: 0.2540\n",
            "Epoch 184/700\n",
            "675/675 [==============================] - 0s 151us/step - loss: 0.0011 - mae: 0.0255 - val_loss: 0.1122 - val_mae: 0.2824\n",
            "Epoch 185/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0011 - mae: 0.0260 - val_loss: 0.1147 - val_mae: 0.2894\n",
            "Epoch 186/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 0.0020 - mae: 0.0316 - val_loss: 0.1593 - val_mae: 0.3511\n",
            "Epoch 187/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 9.4424e-04 - mae: 0.0240 - val_loss: 0.1026 - val_mae: 0.2672\n",
            "Epoch 188/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0036 - mae: 0.0372 - val_loss: 0.2061 - val_mae: 0.4102\n",
            "Epoch 189/700\n",
            "675/675 [==============================] - 0s 158us/step - loss: 0.0163 - mae: 0.0997 - val_loss: 0.3509 - val_mae: 0.5342\n",
            "Epoch 190/700\n",
            "675/675 [==============================] - 0s 153us/step - loss: 0.0191 - mae: 0.1078 - val_loss: 0.6091 - val_mae: 0.7212\n",
            "Epoch 191/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0052 - mae: 0.0521 - val_loss: 0.3025 - val_mae: 0.4923\n",
            "Epoch 192/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0023 - mae: 0.0355 - val_loss: 0.2799 - val_mae: 0.4745\n",
            "Epoch 193/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0022 - mae: 0.0355 - val_loss: 0.4724 - val_mae: 0.6503\n",
            "Epoch 194/700\n",
            "675/675 [==============================] - 0s 144us/step - loss: 0.0015 - mae: 0.0287 - val_loss: 0.1603 - val_mae: 0.3406\n",
            "Epoch 195/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 6.3079e-04 - mae: 0.0194 - val_loss: 0.1586 - val_mae: 0.3427\n",
            "Epoch 196/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 7.4308e-04 - mae: 0.0209 - val_loss: 0.1200 - val_mae: 0.2883\n",
            "Epoch 197/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 8.8366e-04 - mae: 0.0234 - val_loss: 0.1251 - val_mae: 0.3039\n",
            "Epoch 198/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 5.7294e-04 - mae: 0.0185 - val_loss: 0.0883 - val_mae: 0.2360\n",
            "Epoch 199/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0036 - mae: 0.0437 - val_loss: 0.0836 - val_mae: 0.2233\n",
            "Epoch 200/700\n",
            "675/675 [==============================] - 0s 127us/step - loss: 0.0042 - mae: 0.0449 - val_loss: 0.2265 - val_mae: 0.4196\n",
            "Epoch 201/700\n",
            "675/675 [==============================] - 0s 145us/step - loss: 0.0011 - mae: 0.0251 - val_loss: 0.1589 - val_mae: 0.3441\n",
            "Epoch 202/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0014 - mae: 0.0302 - val_loss: 0.1184 - val_mae: 0.2847\n",
            "Epoch 203/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 8.0153e-04 - mae: 0.0209 - val_loss: 0.0724 - val_mae: 0.2066\n",
            "Epoch 204/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 0.0019 - mae: 0.0345 - val_loss: 0.0603 - val_mae: 0.1872\n",
            "Epoch 205/700\n",
            "675/675 [==============================] - 0s 152us/step - loss: 0.0032 - mae: 0.0447 - val_loss: 0.0962 - val_mae: 0.2570\n",
            "Epoch 206/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 0.0021 - mae: 0.0349 - val_loss: 0.0589 - val_mae: 0.1851\n",
            "Epoch 207/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0022 - mae: 0.0343 - val_loss: 0.0961 - val_mae: 0.2447\n",
            "Epoch 208/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 9.2143e-04 - mae: 0.0238 - val_loss: 0.1515 - val_mae: 0.3423\n",
            "Epoch 209/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 6.4236e-04 - mae: 0.0191 - val_loss: 0.1332 - val_mae: 0.3194\n",
            "Epoch 210/700\n",
            "675/675 [==============================] - 0s 146us/step - loss: 0.0011 - mae: 0.0256 - val_loss: 0.1026 - val_mae: 0.2682\n",
            "Epoch 211/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 8.3729e-04 - mae: 0.0221 - val_loss: 0.0935 - val_mae: 0.2539\n",
            "Epoch 212/700\n",
            "675/675 [==============================] - 0s 151us/step - loss: 0.0033 - mae: 0.0394 - val_loss: 0.0394 - val_mae: 0.1744\n",
            "Epoch 213/700\n",
            "675/675 [==============================] - 0s 152us/step - loss: 0.0051 - mae: 0.0550 - val_loss: 0.1454 - val_mae: 0.3193\n",
            "Epoch 214/700\n",
            "675/675 [==============================] - 0s 146us/step - loss: 0.0038 - mae: 0.0465 - val_loss: 0.1888 - val_mae: 0.3872\n",
            "Epoch 215/700\n",
            "675/675 [==============================] - 0s 150us/step - loss: 0.0089 - mae: 0.0719 - val_loss: 0.6910 - val_mae: 0.7890\n",
            "Epoch 216/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0051 - mae: 0.0546 - val_loss: 0.1976 - val_mae: 0.3845\n",
            "Epoch 217/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0025 - mae: 0.0374 - val_loss: 0.2356 - val_mae: 0.4343\n",
            "Epoch 218/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0169 - mae: 0.0889 - val_loss: 0.2768 - val_mae: 0.4170\n",
            "Epoch 219/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0323 - mae: 0.1352 - val_loss: 0.6789 - val_mae: 0.7565\n",
            "Epoch 220/700\n",
            "675/675 [==============================] - 0s 151us/step - loss: 0.0031 - mae: 0.0429 - val_loss: 0.2216 - val_mae: 0.3996\n",
            "Epoch 221/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 0.0010 - mae: 0.0252 - val_loss: 0.2083 - val_mae: 0.3949\n",
            "Epoch 222/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 8.8650e-04 - mae: 0.0219 - val_loss: 0.2017 - val_mae: 0.3946\n",
            "Epoch 223/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 9.6395e-04 - mae: 0.0241 - val_loss: 0.3197 - val_mae: 0.5216\n",
            "Epoch 224/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 0.0011 - mae: 0.0245 - val_loss: 0.1412 - val_mae: 0.3176\n",
            "Epoch 225/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 9.5747e-04 - mae: 0.0248 - val_loss: 0.1316 - val_mae: 0.3102\n",
            "Epoch 226/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 4.3308e-04 - mae: 0.0159 - val_loss: 0.1751 - val_mae: 0.3753\n",
            "Epoch 227/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0046 - mae: 0.0461 - val_loss: 0.4722 - val_mae: 0.6484\n",
            "Epoch 228/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0041 - mae: 0.0492 - val_loss: 0.2906 - val_mae: 0.4855\n",
            "Epoch 229/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0011 - mae: 0.0246 - val_loss: 0.1511 - val_mae: 0.3258\n",
            "Epoch 230/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 9.6094e-04 - mae: 0.0227 - val_loss: 0.2181 - val_mae: 0.4179\n",
            "Epoch 231/700\n",
            "675/675 [==============================] - 0s 148us/step - loss: 5.6956e-04 - mae: 0.0183 - val_loss: 0.0854 - val_mae: 0.2261\n",
            "Epoch 232/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 6.3969e-04 - mae: 0.0191 - val_loss: 0.1224 - val_mae: 0.2949\n",
            "Epoch 233/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 4.1672e-04 - mae: 0.0159 - val_loss: 0.1478 - val_mae: 0.3354\n",
            "Epoch 234/700\n",
            "675/675 [==============================] - 0s 147us/step - loss: 5.4878e-04 - mae: 0.0179 - val_loss: 0.1568 - val_mae: 0.3500\n",
            "Epoch 235/700\n",
            "675/675 [==============================] - 0s 161us/step - loss: 0.0035 - mae: 0.0432 - val_loss: 0.1502 - val_mae: 0.3323\n",
            "Epoch 236/700\n",
            "675/675 [==============================] - 0s 194us/step - loss: 9.2654e-04 - mae: 0.0240 - val_loss: 0.1164 - val_mae: 0.2850\n",
            "Epoch 237/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 4.0980e-04 - mae: 0.0161 - val_loss: 0.1144 - val_mae: 0.2846\n",
            "Epoch 238/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 9.0959e-04 - mae: 0.0236 - val_loss: 0.1253 - val_mae: 0.3035\n",
            "Epoch 239/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 0.0020 - mae: 0.0312 - val_loss: 0.0668 - val_mae: 0.1985\n",
            "Epoch 240/700\n",
            "675/675 [==============================] - 0s 151us/step - loss: 0.0044 - mae: 0.0474 - val_loss: 0.1766 - val_mae: 0.3593\n",
            "Epoch 241/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0014 - mae: 0.0290 - val_loss: 0.2217 - val_mae: 0.4222\n",
            "Epoch 242/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 0.0030 - mae: 0.0410 - val_loss: 0.1516 - val_mae: 0.3182\n",
            "Epoch 243/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0061 - mae: 0.0539 - val_loss: 0.3187 - val_mae: 0.5181\n",
            "Epoch 244/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 0.0147 - mae: 0.0908 - val_loss: 0.2214 - val_mae: 0.3745\n",
            "Epoch 245/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0039 - mae: 0.0449 - val_loss: 0.2949 - val_mae: 0.4879\n",
            "Epoch 246/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 0.0015 - mae: 0.0296 - val_loss: 0.1633 - val_mae: 0.3386\n",
            "Epoch 247/700\n",
            "675/675 [==============================] - 0s 150us/step - loss: 5.1410e-04 - mae: 0.0172 - val_loss: 0.1879 - val_mae: 0.3795\n",
            "Epoch 248/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 5.4181e-04 - mae: 0.0183 - val_loss: 0.1713 - val_mae: 0.3642\n",
            "Epoch 249/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 7.3972e-04 - mae: 0.0210 - val_loss: 0.1097 - val_mae: 0.2698\n",
            "Epoch 250/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 8.1054e-04 - mae: 0.0211 - val_loss: 0.0826 - val_mae: 0.2210\n",
            "Epoch 251/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0031 - mae: 0.0384 - val_loss: 0.1948 - val_mae: 0.3886\n",
            "Epoch 252/700\n",
            "675/675 [==============================] - 0s 156us/step - loss: 0.0014 - mae: 0.0294 - val_loss: 0.1097 - val_mae: 0.2622\n",
            "Epoch 253/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 6.4270e-04 - mae: 0.0178 - val_loss: 0.1598 - val_mae: 0.3457\n",
            "Epoch 254/700\n",
            "675/675 [==============================] - 0s 150us/step - loss: 5.3583e-04 - mae: 0.0175 - val_loss: 0.1407 - val_mae: 0.3208\n",
            "Epoch 255/700\n",
            "675/675 [==============================] - 0s 181us/step - loss: 3.8459e-04 - mae: 0.0153 - val_loss: 0.1433 - val_mae: 0.3291\n",
            "Epoch 256/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 5.2427e-04 - mae: 0.0178 - val_loss: 0.1234 - val_mae: 0.3021\n",
            "Epoch 257/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 4.1091e-04 - mae: 0.0165 - val_loss: 0.1324 - val_mae: 0.3166\n",
            "Epoch 258/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 6.8858e-04 - mae: 0.0203 - val_loss: 0.1912 - val_mae: 0.3956\n",
            "Epoch 259/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 0.0017 - mae: 0.0316 - val_loss: 0.1307 - val_mae: 0.3102\n",
            "Epoch 260/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 0.0092 - mae: 0.0561 - val_loss: 0.2734 - val_mae: 0.4579\n",
            "Epoch 261/700\n",
            "675/675 [==============================] - 0s 147us/step - loss: 0.0128 - mae: 0.0833 - val_loss: 0.2591 - val_mae: 0.4513\n",
            "Epoch 262/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0012 - mae: 0.0276 - val_loss: 0.2289 - val_mae: 0.4271\n",
            "Epoch 263/700\n",
            "675/675 [==============================] - 0s 148us/step - loss: 4.1940e-04 - mae: 0.0162 - val_loss: 0.1564 - val_mae: 0.3385\n",
            "Epoch 264/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 5.5864e-04 - mae: 0.0190 - val_loss: 0.1410 - val_mae: 0.3215\n",
            "Epoch 265/700\n",
            "675/675 [==============================] - 0s 151us/step - loss: 0.0017 - mae: 0.0324 - val_loss: 0.2010 - val_mae: 0.3951\n",
            "Epoch 266/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 0.0011 - mae: 0.0253 - val_loss: 0.1420 - val_mae: 0.3136\n",
            "Epoch 267/700\n",
            "675/675 [==============================] - 0s 150us/step - loss: 0.0026 - mae: 0.0395 - val_loss: 0.0559 - val_mae: 0.1865\n",
            "Epoch 268/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 0.0072 - mae: 0.0637 - val_loss: 0.1911 - val_mae: 0.3617\n",
            "Epoch 269/700\n",
            "675/675 [==============================] - 0s 153us/step - loss: 0.0046 - mae: 0.0541 - val_loss: 0.1845 - val_mae: 0.3663\n",
            "Epoch 270/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 7.3445e-04 - mae: 0.0210 - val_loss: 0.1692 - val_mae: 0.3531\n",
            "Epoch 271/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 0.0012 - mae: 0.0270 - val_loss: 0.1835 - val_mae: 0.3726\n",
            "Epoch 272/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 6.5721e-04 - mae: 0.0203 - val_loss: 0.1103 - val_mae: 0.2680\n",
            "Epoch 273/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 6.4669e-04 - mae: 0.0195 - val_loss: 0.1550 - val_mae: 0.3416\n",
            "Epoch 274/700\n",
            "675/675 [==============================] - 0s 147us/step - loss: 0.0046 - mae: 0.0454 - val_loss: 0.1080 - val_mae: 0.2578\n",
            "Epoch 275/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0032 - mae: 0.0407 - val_loss: 0.1685 - val_mae: 0.3576\n",
            "Epoch 276/700\n",
            "675/675 [==============================] - 0s 145us/step - loss: 9.0058e-04 - mae: 0.0232 - val_loss: 0.1964 - val_mae: 0.4038\n",
            "Epoch 277/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 0.0041 - mae: 0.0516 - val_loss: 0.1278 - val_mae: 0.3007\n",
            "Epoch 278/700\n",
            "675/675 [==============================] - 0s 199us/step - loss: 0.0015 - mae: 0.0301 - val_loss: 0.1260 - val_mae: 0.3033\n",
            "Epoch 279/700\n",
            "675/675 [==============================] - 0s 156us/step - loss: 4.4081e-04 - mae: 0.0168 - val_loss: 0.1194 - val_mae: 0.2920\n",
            "Epoch 280/700\n",
            "675/675 [==============================] - 0s 150us/step - loss: 0.0011 - mae: 0.0261 - val_loss: 0.0561 - val_mae: 0.1802\n",
            "Epoch 281/700\n",
            "675/675 [==============================] - 0s 143us/step - loss: 0.0014 - mae: 0.0289 - val_loss: 0.0437 - val_mae: 0.1632\n",
            "Epoch 282/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0034 - mae: 0.0411 - val_loss: 0.2241 - val_mae: 0.4213\n",
            "Epoch 283/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0026 - mae: 0.0357 - val_loss: 0.3322 - val_mae: 0.5268\n",
            "Epoch 284/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0011 - mae: 0.0261 - val_loss: 0.1590 - val_mae: 0.3481\n",
            "Epoch 285/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0057 - mae: 0.0495 - val_loss: 0.3607 - val_mae: 0.5465\n",
            "Epoch 286/700\n",
            "675/675 [==============================] - 0s 154us/step - loss: 0.0045 - mae: 0.0535 - val_loss: 0.1511 - val_mae: 0.3101\n",
            "Epoch 287/700\n",
            "675/675 [==============================] - 0s 148us/step - loss: 0.0024 - mae: 0.0355 - val_loss: 0.1761 - val_mae: 0.3526\n",
            "Epoch 288/700\n",
            "675/675 [==============================] - 0s 154us/step - loss: 0.0014 - mae: 0.0289 - val_loss: 0.1452 - val_mae: 0.3248\n",
            "Epoch 289/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 9.5542e-04 - mae: 0.0236 - val_loss: 0.1938 - val_mae: 0.3939\n",
            "Epoch 290/700\n",
            "675/675 [==============================] - 0s 182us/step - loss: 9.4646e-04 - mae: 0.0235 - val_loss: 0.0587 - val_mae: 0.1852\n",
            "Epoch 291/700\n",
            "675/675 [==============================] - 0s 146us/step - loss: 0.0011 - mae: 0.0242 - val_loss: 0.1636 - val_mae: 0.3562\n",
            "Epoch 292/700\n",
            "675/675 [==============================] - 0s 147us/step - loss: 0.0012 - mae: 0.0269 - val_loss: 0.1130 - val_mae: 0.2749\n",
            "Epoch 293/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 0.0017 - mae: 0.0333 - val_loss: 0.2479 - val_mae: 0.4553\n",
            "Epoch 294/700\n",
            "675/675 [==============================] - 0s 162us/step - loss: 0.0257 - mae: 0.1239 - val_loss: 0.2295 - val_mae: 0.3820\n",
            "Epoch 295/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 0.0081 - mae: 0.0648 - val_loss: 0.2379 - val_mae: 0.4228\n",
            "Epoch 296/700\n",
            "675/675 [==============================] - 0s 150us/step - loss: 0.0012 - mae: 0.0261 - val_loss: 0.1815 - val_mae: 0.3616\n",
            "Epoch 297/700\n",
            "675/675 [==============================] - 0s 157us/step - loss: 7.9990e-04 - mae: 0.0223 - val_loss: 0.1751 - val_mae: 0.3601\n",
            "Epoch 298/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 6.9431e-04 - mae: 0.0204 - val_loss: 0.1620 - val_mae: 0.3478\n",
            "Epoch 299/700\n",
            "675/675 [==============================] - 0s 154us/step - loss: 6.0449e-04 - mae: 0.0192 - val_loss: 0.1932 - val_mae: 0.3940\n",
            "Epoch 300/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 7.0598e-04 - mae: 0.0198 - val_loss: 0.1135 - val_mae: 0.2786\n",
            "Epoch 301/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 5.0642e-04 - mae: 0.0175 - val_loss: 0.0860 - val_mae: 0.2290\n",
            "Epoch 302/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 8.4742e-04 - mae: 0.0225 - val_loss: 0.1947 - val_mae: 0.3979\n",
            "Epoch 303/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 7.7757e-04 - mae: 0.0199 - val_loss: 0.1190 - val_mae: 0.2909\n",
            "Epoch 304/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 6.9224e-04 - mae: 0.0212 - val_loss: 0.1662 - val_mae: 0.3631\n",
            "Epoch 305/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 7.2710e-04 - mae: 0.0211 - val_loss: 0.1122 - val_mae: 0.2820\n",
            "Epoch 306/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 8.6966e-04 - mae: 0.0230 - val_loss: 0.0706 - val_mae: 0.2030\n",
            "Epoch 307/700\n",
            "675/675 [==============================] - 0s 148us/step - loss: 0.0043 - mae: 0.0519 - val_loss: 0.3826 - val_mae: 0.5802\n",
            "Epoch 308/700\n",
            "675/675 [==============================] - 0s 155us/step - loss: 0.0095 - mae: 0.0760 - val_loss: 0.3049 - val_mae: 0.4976\n",
            "Epoch 309/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0016 - mae: 0.0282 - val_loss: 0.2557 - val_mae: 0.4611\n",
            "Epoch 310/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0011 - mae: 0.0245 - val_loss: 0.1335 - val_mae: 0.3041\n",
            "Epoch 311/700\n",
            "675/675 [==============================] - 0s 149us/step - loss: 0.0015 - mae: 0.0290 - val_loss: 0.2326 - val_mae: 0.4313\n",
            "Epoch 312/700\n",
            "675/675 [==============================] - 0s 151us/step - loss: 0.0013 - mae: 0.0279 - val_loss: 0.1375 - val_mae: 0.3155\n",
            "Epoch 313/700\n",
            "675/675 [==============================] - 0s 146us/step - loss: 6.5525e-04 - mae: 0.0190 - val_loss: 0.1147 - val_mae: 0.2808\n",
            "Epoch 314/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 0.0016 - mae: 0.0314 - val_loss: 0.1100 - val_mae: 0.2641\n",
            "Epoch 315/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 7.3708e-04 - mae: 0.0218 - val_loss: 0.0824 - val_mae: 0.2219\n",
            "Epoch 316/700\n",
            "675/675 [==============================] - 0s 146us/step - loss: 9.2282e-04 - mae: 0.0239 - val_loss: 0.1711 - val_mae: 0.3642\n",
            "Epoch 317/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 8.5611e-04 - mae: 0.0209 - val_loss: 0.0915 - val_mae: 0.2366\n",
            "Epoch 318/700\n",
            "675/675 [==============================] - 0s 160us/step - loss: 7.3114e-04 - mae: 0.0211 - val_loss: 0.1581 - val_mae: 0.3505\n",
            "Epoch 319/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 8.4735e-04 - mae: 0.0233 - val_loss: 0.0990 - val_mae: 0.2573\n",
            "Epoch 320/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 7.7466e-04 - mae: 0.0217 - val_loss: 0.0735 - val_mae: 0.2093\n",
            "Epoch 321/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0021 - mae: 0.0356 - val_loss: 0.1177 - val_mae: 0.2889\n",
            "Epoch 322/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 6.3586e-04 - mae: 0.0190 - val_loss: 0.1188 - val_mae: 0.2955\n",
            "Epoch 323/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 0.0024 - mae: 0.0337 - val_loss: 0.0769 - val_mae: 0.2157\n",
            "Epoch 324/700\n",
            "675/675 [==============================] - 0s 149us/step - loss: 0.0011 - mae: 0.0263 - val_loss: 0.1217 - val_mae: 0.3009\n",
            "Epoch 325/700\n",
            "675/675 [==============================] - 0s 155us/step - loss: 8.1533e-04 - mae: 0.0225 - val_loss: 0.0966 - val_mae: 0.2579\n",
            "Epoch 326/700\n",
            "675/675 [==============================] - 0s 164us/step - loss: 7.5245e-04 - mae: 0.0224 - val_loss: 0.0588 - val_mae: 0.1838\n",
            "Epoch 327/700\n",
            "675/675 [==============================] - 0s 156us/step - loss: 6.5319e-04 - mae: 0.0200 - val_loss: 0.0857 - val_mae: 0.2376\n",
            "Epoch 328/700\n",
            "675/675 [==============================] - 0s 197us/step - loss: 0.0015 - mae: 0.0300 - val_loss: 0.0469 - val_mae: 0.1634\n",
            "Epoch 329/700\n",
            "675/675 [==============================] - 0s 150us/step - loss: 0.0038 - mae: 0.0446 - val_loss: 0.1297 - val_mae: 0.3127\n",
            "Epoch 330/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 5.8857e-04 - mae: 0.0198 - val_loss: 0.1062 - val_mae: 0.2749\n",
            "Epoch 331/700\n",
            "675/675 [==============================] - 0s 145us/step - loss: 9.5736e-04 - mae: 0.0247 - val_loss: 0.1495 - val_mae: 0.3406\n",
            "Epoch 332/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0022 - mae: 0.0369 - val_loss: 0.1363 - val_mae: 0.3201\n",
            "Epoch 333/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0014 - mae: 0.0289 - val_loss: 0.0799 - val_mae: 0.2283\n",
            "Epoch 334/700\n",
            "675/675 [==============================] - 0s 145us/step - loss: 4.2150e-04 - mae: 0.0163 - val_loss: 0.1139 - val_mae: 0.2923\n",
            "Epoch 335/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0016 - mae: 0.0302 - val_loss: 0.1124 - val_mae: 0.2882\n",
            "Epoch 336/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 7.9792e-04 - mae: 0.0212 - val_loss: 0.1143 - val_mae: 0.2905\n",
            "Epoch 337/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 4.8812e-04 - mae: 0.0176 - val_loss: 0.1007 - val_mae: 0.2677\n",
            "Epoch 338/700\n",
            "675/675 [==============================] - 0s 145us/step - loss: 8.9325e-04 - mae: 0.0232 - val_loss: 0.1101 - val_mae: 0.2893\n",
            "Epoch 339/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0017 - mae: 0.0335 - val_loss: 0.0492 - val_mae: 0.1671\n",
            "Epoch 340/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 0.0022 - mae: 0.0358 - val_loss: 0.3295 - val_mae: 0.5409\n",
            "Epoch 341/700\n",
            "675/675 [==============================] - 0s 149us/step - loss: 0.0045 - mae: 0.0458 - val_loss: 1.6126 - val_mae: 1.2405\n",
            "Epoch 342/700\n",
            "675/675 [==============================] - 0s 148us/step - loss: 0.0064 - mae: 0.0584 - val_loss: 0.1296 - val_mae: 0.3001\n",
            "Epoch 343/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0048 - mae: 0.0482 - val_loss: 0.5416 - val_mae: 0.6927\n",
            "Epoch 344/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 0.0286 - mae: 0.1280 - val_loss: 0.6897 - val_mae: 0.7601\n",
            "Epoch 345/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0084 - mae: 0.0702 - val_loss: 0.4219 - val_mae: 0.5925\n",
            "Epoch 346/700\n",
            "675/675 [==============================] - 0s 144us/step - loss: 0.0012 - mae: 0.0267 - val_loss: 0.2260 - val_mae: 0.4171\n",
            "Epoch 347/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 8.1932e-04 - mae: 0.0216 - val_loss: 0.2555 - val_mae: 0.4582\n",
            "Epoch 348/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 7.6306e-04 - mae: 0.0207 - val_loss: 0.1740 - val_mae: 0.3637\n",
            "Epoch 349/700\n",
            "675/675 [==============================] - 0s 147us/step - loss: 5.9195e-04 - mae: 0.0191 - val_loss: 0.1145 - val_mae: 0.2765\n",
            "Epoch 350/700\n",
            "675/675 [==============================] - 0s 148us/step - loss: 5.8025e-04 - mae: 0.0185 - val_loss: 0.1709 - val_mae: 0.3653\n",
            "Epoch 351/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 5.5939e-04 - mae: 0.0188 - val_loss: 0.1142 - val_mae: 0.2830\n",
            "Epoch 352/700\n",
            "675/675 [==============================] - 0s 149us/step - loss: 6.2388e-04 - mae: 0.0200 - val_loss: 0.1205 - val_mae: 0.2964\n",
            "Epoch 353/700\n",
            "675/675 [==============================] - 0s 143us/step - loss: 4.4616e-04 - mae: 0.0160 - val_loss: 0.1341 - val_mae: 0.3206\n",
            "Epoch 354/700\n",
            "675/675 [==============================] - 0s 145us/step - loss: 5.5376e-04 - mae: 0.0186 - val_loss: 0.0792 - val_mae: 0.2219\n",
            "Epoch 355/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 5.4131e-04 - mae: 0.0185 - val_loss: 0.0873 - val_mae: 0.2384\n",
            "Epoch 356/700\n",
            "675/675 [==============================] - 0s 149us/step - loss: 6.7449e-04 - mae: 0.0208 - val_loss: 0.1024 - val_mae: 0.2699\n",
            "Epoch 357/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 5.8734e-04 - mae: 0.0187 - val_loss: 0.1150 - val_mae: 0.2901\n",
            "Epoch 358/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 0.0012 - mae: 0.0263 - val_loss: 0.0564 - val_mae: 0.1800\n",
            "Epoch 359/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0020 - mae: 0.0344 - val_loss: 0.1008 - val_mae: 0.2586\n",
            "Epoch 360/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 0.0010 - mae: 0.0253 - val_loss: 0.2087 - val_mae: 0.4196\n",
            "Epoch 361/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 5.1419e-04 - mae: 0.0168 - val_loss: 0.1000 - val_mae: 0.2621\n",
            "Epoch 362/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 4.2057e-04 - mae: 0.0164 - val_loss: 0.1358 - val_mae: 0.3263\n",
            "Epoch 363/700\n",
            "675/675 [==============================] - 0s 146us/step - loss: 4.6488e-04 - mae: 0.0167 - val_loss: 0.1028 - val_mae: 0.2733\n",
            "Epoch 364/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 9.4353e-04 - mae: 0.0231 - val_loss: 0.1116 - val_mae: 0.2868\n",
            "Epoch 365/700\n",
            "675/675 [==============================] - 0s 166us/step - loss: 0.0017 - mae: 0.0328 - val_loss: 0.1571 - val_mae: 0.3571\n",
            "Epoch 366/700\n",
            "675/675 [==============================] - 0s 156us/step - loss: 8.6273e-04 - mae: 0.0235 - val_loss: 0.1039 - val_mae: 0.2672\n",
            "Epoch 367/700\n",
            "675/675 [==============================] - 0s 154us/step - loss: 0.0014 - mae: 0.0287 - val_loss: 0.2009 - val_mae: 0.4118\n",
            "Epoch 368/700\n",
            "675/675 [==============================] - 0s 162us/step - loss: 0.0063 - mae: 0.0609 - val_loss: 0.1515 - val_mae: 0.3423\n",
            "Epoch 369/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0024 - mae: 0.0358 - val_loss: 0.0800 - val_mae: 0.2198\n",
            "Epoch 370/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0013 - mae: 0.0276 - val_loss: 0.1084 - val_mae: 0.2717\n",
            "Epoch 371/700\n",
            "675/675 [==============================] - 0s 161us/step - loss: 0.0012 - mae: 0.0281 - val_loss: 0.1197 - val_mae: 0.3006\n",
            "Epoch 372/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 7.2514e-04 - mae: 0.0216 - val_loss: 0.0736 - val_mae: 0.2146\n",
            "Epoch 373/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 7.0557e-04 - mae: 0.0210 - val_loss: 0.0866 - val_mae: 0.2440\n",
            "Epoch 374/700\n",
            "675/675 [==============================] - 0s 148us/step - loss: 3.3439e-04 - mae: 0.0142 - val_loss: 0.0847 - val_mae: 0.2457\n",
            "Epoch 375/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 3.2638e-04 - mae: 0.0145 - val_loss: 0.0544 - val_mae: 0.1789\n",
            "Epoch 376/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 3.9751e-04 - mae: 0.0157 - val_loss: 0.0773 - val_mae: 0.2284\n",
            "Epoch 377/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 8.2631e-04 - mae: 0.0227 - val_loss: 0.0441 - val_mae: 0.1585\n",
            "Epoch 378/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0013 - mae: 0.0256 - val_loss: 0.0791 - val_mae: 0.2314\n",
            "Epoch 379/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 0.0028 - mae: 0.0387 - val_loss: 0.4443 - val_mae: 0.6368\n",
            "Epoch 380/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0033 - mae: 0.0415 - val_loss: 0.1036 - val_mae: 0.2640\n",
            "Epoch 381/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 0.0023 - mae: 0.0384 - val_loss: 0.2506 - val_mae: 0.4646\n",
            "Epoch 382/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0026 - mae: 0.0368 - val_loss: 0.1452 - val_mae: 0.3362\n",
            "Epoch 383/700\n",
            "675/675 [==============================] - 0s 175us/step - loss: 5.9223e-04 - mae: 0.0182 - val_loss: 0.0790 - val_mae: 0.2270\n",
            "Epoch 384/700\n",
            "675/675 [==============================] - 0s 174us/step - loss: 0.0020 - mae: 0.0365 - val_loss: 0.0500 - val_mae: 0.1692\n",
            "Epoch 385/700\n",
            "675/675 [==============================] - 0s 143us/step - loss: 0.0025 - mae: 0.0397 - val_loss: 0.0717 - val_mae: 0.2067\n",
            "Epoch 386/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0030 - mae: 0.0418 - val_loss: 0.1052 - val_mae: 0.2673\n",
            "Epoch 387/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 0.0013 - mae: 0.0278 - val_loss: 0.2434 - val_mae: 0.4521\n",
            "Epoch 388/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0066 - mae: 0.0598 - val_loss: 0.5326 - val_mae: 0.6924\n",
            "Epoch 389/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0037 - mae: 0.0419 - val_loss: 0.2160 - val_mae: 0.4130\n",
            "Epoch 390/700\n",
            "675/675 [==============================] - 0s 145us/step - loss: 4.5831e-04 - mae: 0.0169 - val_loss: 0.1276 - val_mae: 0.3021\n",
            "Epoch 391/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 4.3887e-04 - mae: 0.0164 - val_loss: 0.1335 - val_mae: 0.3152\n",
            "Epoch 392/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 6.2137e-04 - mae: 0.0191 - val_loss: 0.1691 - val_mae: 0.3701\n",
            "Epoch 393/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 3.8890e-04 - mae: 0.0157 - val_loss: 0.1112 - val_mae: 0.2849\n",
            "Epoch 394/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 6.6844e-04 - mae: 0.0204 - val_loss: 0.0851 - val_mae: 0.2390\n",
            "Epoch 395/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 4.2302e-04 - mae: 0.0165 - val_loss: 0.0903 - val_mae: 0.2506\n",
            "Epoch 396/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 6.4373e-04 - mae: 0.0195 - val_loss: 0.0961 - val_mae: 0.2632\n",
            "Epoch 397/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0046 - mae: 0.0383 - val_loss: 1.5131 - val_mae: 1.1898\n",
            "Epoch 398/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 0.0119 - mae: 0.0887 - val_loss: 0.3814 - val_mae: 0.5763\n",
            "Epoch 399/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0018 - mae: 0.0307 - val_loss: 0.0888 - val_mae: 0.2353\n",
            "Epoch 400/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 0.0013 - mae: 0.0264 - val_loss: 0.0537 - val_mae: 0.1791\n",
            "Epoch 401/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 8.1290e-04 - mae: 0.0208 - val_loss: 0.1289 - val_mae: 0.3131\n",
            "Epoch 402/700\n",
            "675/675 [==============================] - 0s 183us/step - loss: 0.0010 - mae: 0.0239 - val_loss: 0.0974 - val_mae: 0.2633\n",
            "Epoch 403/700\n",
            "675/675 [==============================] - 0s 144us/step - loss: 2.7072e-04 - mae: 0.0129 - val_loss: 0.0715 - val_mae: 0.2125\n",
            "Epoch 404/700\n",
            "675/675 [==============================] - 0s 143us/step - loss: 4.4682e-04 - mae: 0.0165 - val_loss: 0.0642 - val_mae: 0.1992\n",
            "Epoch 405/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 5.1350e-04 - mae: 0.0179 - val_loss: 0.0800 - val_mae: 0.2340\n",
            "Epoch 406/700\n",
            "675/675 [==============================] - 0s 166us/step - loss: 3.5375e-04 - mae: 0.0152 - val_loss: 0.0520 - val_mae: 0.1742\n",
            "Epoch 407/700\n",
            "675/675 [==============================] - 0s 164us/step - loss: 4.6400e-04 - mae: 0.0171 - val_loss: 0.1087 - val_mae: 0.2882\n",
            "Epoch 408/700\n",
            "675/675 [==============================] - 0s 179us/step - loss: 5.8244e-04 - mae: 0.0193 - val_loss: 0.0851 - val_mae: 0.2465\n",
            "Epoch 409/700\n",
            "675/675 [==============================] - 0s 162us/step - loss: 9.4609e-04 - mae: 0.0240 - val_loss: 0.0499 - val_mae: 0.1707\n",
            "Epoch 410/700\n",
            "675/675 [==============================] - 0s 178us/step - loss: 0.0012 - mae: 0.0264 - val_loss: 0.0744 - val_mae: 0.2217\n",
            "Epoch 411/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 4.8235e-04 - mae: 0.0174 - val_loss: 0.1097 - val_mae: 0.2912\n",
            "Epoch 412/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 7.9739e-04 - mae: 0.0219 - val_loss: 0.0575 - val_mae: 0.1857\n",
            "Epoch 413/700\n",
            "675/675 [==============================] - 0s 150us/step - loss: 0.0012 - mae: 0.0249 - val_loss: 0.2802 - val_mae: 0.5022\n",
            "Epoch 414/700\n",
            "675/675 [==============================] - 0s 145us/step - loss: 0.0186 - mae: 0.1022 - val_loss: 0.3983 - val_mae: 0.5757\n",
            "Epoch 415/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0016 - mae: 0.0313 - val_loss: 0.1247 - val_mae: 0.2969\n",
            "Epoch 416/700\n",
            "675/675 [==============================] - 0s 148us/step - loss: 6.1944e-04 - mae: 0.0189 - val_loss: 0.1184 - val_mae: 0.2926\n",
            "Epoch 417/700\n",
            "675/675 [==============================] - 0s 146us/step - loss: 3.4224e-04 - mae: 0.0145 - val_loss: 0.1065 - val_mae: 0.2774\n",
            "Epoch 418/700\n",
            "675/675 [==============================] - 0s 157us/step - loss: 8.0769e-04 - mae: 0.0205 - val_loss: 0.2467 - val_mae: 0.4638\n",
            "Epoch 419/700\n",
            "675/675 [==============================] - 0s 175us/step - loss: 0.0010 - mae: 0.0249 - val_loss: 0.1242 - val_mae: 0.3092\n",
            "Epoch 420/700\n",
            "675/675 [==============================] - 0s 161us/step - loss: 8.2530e-04 - mae: 0.0229 - val_loss: 0.0730 - val_mae: 0.2172\n",
            "Epoch 421/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 8.8448e-04 - mae: 0.0224 - val_loss: 0.0984 - val_mae: 0.2704\n",
            "Epoch 422/700\n",
            "675/675 [==============================] - 0s 152us/step - loss: 6.9998e-04 - mae: 0.0212 - val_loss: 0.0777 - val_mae: 0.2328\n",
            "Epoch 423/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 5.1976e-04 - mae: 0.0179 - val_loss: 0.0861 - val_mae: 0.2534\n",
            "Epoch 424/700\n",
            "675/675 [==============================] - 0s 146us/step - loss: 7.5822e-04 - mae: 0.0219 - val_loss: 0.0645 - val_mae: 0.2079\n",
            "Epoch 425/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 8.9216e-04 - mae: 0.0246 - val_loss: 0.0779 - val_mae: 0.2376\n",
            "Epoch 426/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 6.1618e-04 - mae: 0.0191 - val_loss: 0.0707 - val_mae: 0.2226\n",
            "Epoch 427/700\n",
            "675/675 [==============================] - 0s 128us/step - loss: 5.4084e-04 - mae: 0.0186 - val_loss: 0.0787 - val_mae: 0.2384\n",
            "Epoch 428/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 5.7107e-04 - mae: 0.0192 - val_loss: 0.0535 - val_mae: 0.1839\n",
            "Epoch 429/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 4.3411e-04 - mae: 0.0164 - val_loss: 0.0495 - val_mae: 0.1749\n",
            "Epoch 430/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0014 - mae: 0.0290 - val_loss: 0.1033 - val_mae: 0.2789\n",
            "Epoch 431/700\n",
            "675/675 [==============================] - 0s 147us/step - loss: 0.0033 - mae: 0.0444 - val_loss: 0.0842 - val_mae: 0.2501\n",
            "Epoch 432/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 3.6743e-04 - mae: 0.0148 - val_loss: 0.0452 - val_mae: 0.1643\n",
            "Epoch 433/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 2.9591e-04 - mae: 0.0139 - val_loss: 0.0650 - val_mae: 0.2132\n",
            "Epoch 434/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0010 - mae: 0.0245 - val_loss: 0.0424 - val_mae: 0.1577\n",
            "Epoch 435/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0033 - mae: 0.0418 - val_loss: 0.1478 - val_mae: 0.3455\n",
            "Epoch 436/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0035 - mae: 0.0415 - val_loss: 0.1433 - val_mae: 0.3330\n",
            "Epoch 437/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0011 - mae: 0.0268 - val_loss: 0.0857 - val_mae: 0.2456\n",
            "Epoch 438/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0032 - mae: 0.0387 - val_loss: 0.3245 - val_mae: 0.5375\n",
            "Epoch 439/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0026 - mae: 0.0371 - val_loss: 0.1593 - val_mae: 0.3535\n",
            "Epoch 440/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 0.0036 - mae: 0.0447 - val_loss: 0.0708 - val_mae: 0.2082\n",
            "Epoch 441/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0010 - mae: 0.0237 - val_loss: 0.0646 - val_mae: 0.1999\n",
            "Epoch 442/700\n",
            "675/675 [==============================] - 0s 168us/step - loss: 5.7542e-04 - mae: 0.0185 - val_loss: 0.0585 - val_mae: 0.1899\n",
            "Epoch 443/700\n",
            "675/675 [==============================] - 0s 143us/step - loss: 9.3542e-04 - mae: 0.0229 - val_loss: 0.0888 - val_mae: 0.2543\n",
            "Epoch 444/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 3.9873e-04 - mae: 0.0155 - val_loss: 0.0798 - val_mae: 0.2427\n",
            "Epoch 445/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 2.9209e-04 - mae: 0.0134 - val_loss: 0.0686 - val_mae: 0.2209\n",
            "Epoch 446/700\n",
            "675/675 [==============================] - 0s 127us/step - loss: 7.6647e-04 - mae: 0.0212 - val_loss: 0.0785 - val_mae: 0.2415\n",
            "Epoch 447/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 8.7932e-04 - mae: 0.0220 - val_loss: 0.0733 - val_mae: 0.2317\n",
            "Epoch 448/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0016 - mae: 0.0316 - val_loss: 0.1584 - val_mae: 0.3692\n",
            "Epoch 449/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0025 - mae: 0.0349 - val_loss: 0.0745 - val_mae: 0.2328\n",
            "Epoch 450/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 4.0614e-04 - mae: 0.0157 - val_loss: 0.0665 - val_mae: 0.2195\n",
            "Epoch 451/700\n",
            "675/675 [==============================] - 0s 128us/step - loss: 0.0027 - mae: 0.0399 - val_loss: 0.0425 - val_mae: 0.1627\n",
            "Epoch 452/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 9.9680e-04 - mae: 0.0252 - val_loss: 0.0326 - val_mae: 0.1359\n",
            "Epoch 453/700\n",
            "675/675 [==============================] - 0s 143us/step - loss: 0.0016 - mae: 0.0308 - val_loss: 0.0553 - val_mae: 0.1980\n",
            "Epoch 454/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0013 - mae: 0.0286 - val_loss: 0.0534 - val_mae: 0.1926\n",
            "Epoch 455/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 4.5159e-04 - mae: 0.0163 - val_loss: 0.0396 - val_mae: 0.1560\n",
            "Epoch 456/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 4.9771e-04 - mae: 0.0161 - val_loss: 0.1056 - val_mae: 0.2960\n",
            "Epoch 457/700\n",
            "675/675 [==============================] - 0s 127us/step - loss: 5.5588e-04 - mae: 0.0187 - val_loss: 0.0468 - val_mae: 0.1756\n",
            "Epoch 458/700\n",
            "675/675 [==============================] - 0s 146us/step - loss: 3.1552e-04 - mae: 0.0146 - val_loss: 0.0418 - val_mae: 0.1631\n",
            "Epoch 459/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 6.5238e-04 - mae: 0.0191 - val_loss: 0.0511 - val_mae: 0.1876\n",
            "Epoch 460/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0011 - mae: 0.0260 - val_loss: 0.0357 - val_mae: 0.1451\n",
            "Epoch 461/700\n",
            "675/675 [==============================] - 0s 147us/step - loss: 0.0010 - mae: 0.0261 - val_loss: 0.0753 - val_mae: 0.2405\n",
            "Epoch 462/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 8.4142e-04 - mae: 0.0228 - val_loss: 0.0345 - val_mae: 0.1393\n",
            "Epoch 463/700\n",
            "675/675 [==============================] - 0s 153us/step - loss: 0.0012 - mae: 0.0279 - val_loss: 0.1037 - val_mae: 0.2911\n",
            "Epoch 464/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 8.4456e-04 - mae: 0.0232 - val_loss: 0.0293 - val_mae: 0.1276\n",
            "Epoch 465/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 0.0019 - mae: 0.0329 - val_loss: 0.0562 - val_mae: 0.1873\n",
            "Epoch 466/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 5.3644e-04 - mae: 0.0179 - val_loss: 0.0616 - val_mae: 0.2057\n",
            "Epoch 467/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 3.0939e-04 - mae: 0.0140 - val_loss: 0.0865 - val_mae: 0.2581\n",
            "Epoch 468/700\n",
            "675/675 [==============================] - 0s 149us/step - loss: 0.0167 - mae: 0.0827 - val_loss: 0.2020 - val_mae: 0.3816\n",
            "Epoch 469/700\n",
            "675/675 [==============================] - 0s 150us/step - loss: 0.0051 - mae: 0.0500 - val_loss: 0.2572 - val_mae: 0.4597\n",
            "Epoch 470/700\n",
            "675/675 [==============================] - 0s 154us/step - loss: 0.0012 - mae: 0.0273 - val_loss: 0.1312 - val_mae: 0.3136\n",
            "Epoch 471/700\n",
            "675/675 [==============================] - 0s 145us/step - loss: 5.9624e-04 - mae: 0.0186 - val_loss: 0.1012 - val_mae: 0.2693\n",
            "Epoch 472/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 5.6844e-04 - mae: 0.0190 - val_loss: 0.0635 - val_mae: 0.1992\n",
            "Epoch 473/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 5.3434e-04 - mae: 0.0186 - val_loss: 0.0890 - val_mae: 0.2553\n",
            "Epoch 474/700\n",
            "675/675 [==============================] - 0s 145us/step - loss: 3.3194e-04 - mae: 0.0144 - val_loss: 0.0812 - val_mae: 0.2420\n",
            "Epoch 475/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 7.0436e-04 - mae: 0.0202 - val_loss: 0.1137 - val_mae: 0.3018\n",
            "Epoch 476/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 5.2336e-04 - mae: 0.0170 - val_loss: 0.0535 - val_mae: 0.1855\n",
            "Epoch 477/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 6.1950e-04 - mae: 0.0199 - val_loss: 0.0489 - val_mae: 0.1767\n",
            "Epoch 478/700\n",
            "675/675 [==============================] - 0s 127us/step - loss: 5.6915e-04 - mae: 0.0182 - val_loss: 0.0250 - val_mae: 0.1193\n",
            "Epoch 479/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 0.0025 - mae: 0.0396 - val_loss: 0.1945 - val_mae: 0.4091\n",
            "Epoch 480/700\n",
            "675/675 [==============================] - 0s 143us/step - loss: 0.0016 - mae: 0.0285 - val_loss: 0.0545 - val_mae: 0.1893\n",
            "Epoch 481/700\n",
            "675/675 [==============================] - 0s 127us/step - loss: 4.3634e-04 - mae: 0.0163 - val_loss: 0.0497 - val_mae: 0.1796\n",
            "Epoch 482/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 8.7191e-04 - mae: 0.0227 - val_loss: 0.0811 - val_mae: 0.2483\n",
            "Epoch 483/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0021 - mae: 0.0371 - val_loss: 0.0545 - val_mae: 0.1915\n",
            "Epoch 484/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0032 - mae: 0.0434 - val_loss: 0.2182 - val_mae: 0.4345\n",
            "Epoch 485/700\n",
            "675/675 [==============================] - 0s 146us/step - loss: 0.0065 - mae: 0.0654 - val_loss: 0.0944 - val_mae: 0.2715\n",
            "Epoch 486/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0023 - mae: 0.0389 - val_loss: 0.0567 - val_mae: 0.1968\n",
            "Epoch 487/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 0.0014 - mae: 0.0306 - val_loss: 0.0493 - val_mae: 0.1769\n",
            "Epoch 488/700\n",
            "675/675 [==============================] - 0s 128us/step - loss: 2.1736e-04 - mae: 0.0111 - val_loss: 0.0828 - val_mae: 0.2533\n",
            "Epoch 489/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 3.5829e-04 - mae: 0.0148 - val_loss: 0.0416 - val_mae: 0.1617\n",
            "Epoch 490/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 4.8213e-04 - mae: 0.0170 - val_loss: 0.0550 - val_mae: 0.1940\n",
            "Epoch 491/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 8.6769e-04 - mae: 0.0230 - val_loss: 0.0296 - val_mae: 0.1282\n",
            "Epoch 492/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 7.8280e-04 - mae: 0.0207 - val_loss: 0.1569 - val_mae: 0.3686\n",
            "Epoch 493/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0107 - mae: 0.0825 - val_loss: 0.1274 - val_mae: 0.3208\n",
            "Epoch 494/700\n",
            "675/675 [==============================] - 0s 151us/step - loss: 0.0025 - mae: 0.0389 - val_loss: 0.0677 - val_mae: 0.2194\n",
            "Epoch 495/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 6.0275e-04 - mae: 0.0192 - val_loss: 0.0515 - val_mae: 0.1842\n",
            "Epoch 496/700\n",
            "675/675 [==============================] - 0s 151us/step - loss: 3.4071e-04 - mae: 0.0142 - val_loss: 0.0490 - val_mae: 0.1770\n",
            "Epoch 497/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0010 - mae: 0.0255 - val_loss: 0.0536 - val_mae: 0.1866\n",
            "Epoch 498/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 8.1518e-04 - mae: 0.0208 - val_loss: 0.0856 - val_mae: 0.2568\n",
            "Epoch 499/700\n",
            "675/675 [==============================] - 0s 144us/step - loss: 3.4399e-04 - mae: 0.0145 - val_loss: 0.0759 - val_mae: 0.2384\n",
            "Epoch 500/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 3.2103e-04 - mae: 0.0140 - val_loss: 0.0400 - val_mae: 0.1550\n",
            "Epoch 501/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 4.3162e-04 - mae: 0.0163 - val_loss: 0.0345 - val_mae: 0.1401\n",
            "Epoch 502/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 3.1764e-04 - mae: 0.0139 - val_loss: 0.0543 - val_mae: 0.1943\n",
            "Epoch 503/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 3.9977e-04 - mae: 0.0157 - val_loss: 0.0559 - val_mae: 0.2008\n",
            "Epoch 504/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 8.9028e-04 - mae: 0.0238 - val_loss: 0.0280 - val_mae: 0.1253\n",
            "Epoch 505/700\n",
            "675/675 [==============================] - 0s 147us/step - loss: 0.0015 - mae: 0.0305 - val_loss: 0.0439 - val_mae: 0.1771\n",
            "Epoch 506/700\n",
            "675/675 [==============================] - 0s 146us/step - loss: 7.2344e-04 - mae: 0.0213 - val_loss: 0.0350 - val_mae: 0.1451\n",
            "Epoch 507/700\n",
            "675/675 [==============================] - 0s 153us/step - loss: 0.0015 - mae: 0.0267 - val_loss: 0.0497 - val_mae: 0.1842\n",
            "Epoch 508/700\n",
            "675/675 [==============================] - 0s 151us/step - loss: 5.6136e-04 - mae: 0.0183 - val_loss: 0.0495 - val_mae: 0.1849\n",
            "Epoch 509/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 5.5239e-04 - mae: 0.0184 - val_loss: 0.0384 - val_mae: 0.1566\n",
            "Epoch 510/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 3.4187e-04 - mae: 0.0140 - val_loss: 0.0343 - val_mae: 0.1470\n",
            "Epoch 511/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 0.0023 - mae: 0.0363 - val_loss: 0.0554 - val_mae: 0.1985\n",
            "Epoch 512/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0017 - mae: 0.0337 - val_loss: 0.0770 - val_mae: 0.2432\n",
            "Epoch 513/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0031 - mae: 0.0434 - val_loss: 0.0374 - val_mae: 0.1495\n",
            "Epoch 514/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 4.9398e-04 - mae: 0.0175 - val_loss: 0.0386 - val_mae: 0.1511\n",
            "Epoch 515/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 4.8016e-04 - mae: 0.0172 - val_loss: 0.0723 - val_mae: 0.2311\n",
            "Epoch 516/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 8.0201e-04 - mae: 0.0220 - val_loss: 0.0498 - val_mae: 0.1848\n",
            "Epoch 517/700\n",
            "675/675 [==============================] - 0s 152us/step - loss: 0.0016 - mae: 0.0318 - val_loss: 0.0378 - val_mae: 0.1491\n",
            "Epoch 518/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0014 - mae: 0.0302 - val_loss: 0.0739 - val_mae: 0.2358\n",
            "Epoch 519/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0011 - mae: 0.0270 - val_loss: 0.0520 - val_mae: 0.1888\n",
            "Epoch 520/700\n",
            "675/675 [==============================] - 0s 163us/step - loss: 5.3789e-04 - mae: 0.0170 - val_loss: 0.0411 - val_mae: 0.1601\n",
            "Epoch 521/700\n",
            "675/675 [==============================] - 0s 183us/step - loss: 0.0019 - mae: 0.0340 - val_loss: 0.1276 - val_mae: 0.3247\n",
            "Epoch 522/700\n",
            "675/675 [==============================] - 0s 178us/step - loss: 9.7003e-04 - mae: 0.0246 - val_loss: 0.0401 - val_mae: 0.1529\n",
            "Epoch 523/700\n",
            "675/675 [==============================] - 0s 162us/step - loss: 5.8043e-04 - mae: 0.0194 - val_loss: 0.0705 - val_mae: 0.2320\n",
            "Epoch 524/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 6.3322e-04 - mae: 0.0200 - val_loss: 0.0443 - val_mae: 0.1702\n",
            "Epoch 525/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 4.3365e-04 - mae: 0.0162 - val_loss: 0.0307 - val_mae: 0.1335\n",
            "Epoch 526/700\n",
            "675/675 [==============================] - 0s 145us/step - loss: 0.0011 - mae: 0.0249 - val_loss: 0.0490 - val_mae: 0.1852\n",
            "Epoch 527/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 5.9199e-04 - mae: 0.0198 - val_loss: 0.0478 - val_mae: 0.1819\n",
            "Epoch 528/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 3.6821e-04 - mae: 0.0147 - val_loss: 0.0184 - val_mae: 0.1010\n",
            "Epoch 529/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0018 - mae: 0.0333 - val_loss: 0.0419 - val_mae: 0.1600\n",
            "Epoch 530/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0050 - mae: 0.0475 - val_loss: 0.0984 - val_mae: 0.2463\n",
            "Epoch 531/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 0.0051 - mae: 0.0576 - val_loss: 0.2130 - val_mae: 0.4197\n",
            "Epoch 532/700\n",
            "675/675 [==============================] - 0s 128us/step - loss: 0.0022 - mae: 0.0366 - val_loss: 0.1125 - val_mae: 0.2917\n",
            "Epoch 533/700\n",
            "675/675 [==============================] - 0s 127us/step - loss: 5.9693e-04 - mae: 0.0193 - val_loss: 0.0787 - val_mae: 0.2342\n",
            "Epoch 534/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 3.7834e-04 - mae: 0.0155 - val_loss: 0.0907 - val_mae: 0.2635\n",
            "Epoch 535/700\n",
            "675/675 [==============================] - 0s 147us/step - loss: 2.5929e-04 - mae: 0.0125 - val_loss: 0.0521 - val_mae: 0.1833\n",
            "Epoch 536/700\n",
            "675/675 [==============================] - 0s 154us/step - loss: 3.8256e-04 - mae: 0.0155 - val_loss: 0.0197 - val_mae: 0.1082\n",
            "Epoch 537/700\n",
            "675/675 [==============================] - 0s 155us/step - loss: 0.0064 - mae: 0.0550 - val_loss: 0.2598 - val_mae: 0.4693\n",
            "Epoch 538/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 0.0013 - mae: 0.0263 - val_loss: 0.0931 - val_mae: 0.2639\n",
            "Epoch 539/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 8.9654e-04 - mae: 0.0237 - val_loss: 0.0970 - val_mae: 0.2719\n",
            "Epoch 540/700\n",
            "675/675 [==============================] - 0s 159us/step - loss: 3.3367e-04 - mae: 0.0142 - val_loss: 0.0773 - val_mae: 0.2407\n",
            "Epoch 541/700\n",
            "675/675 [==============================] - 0s 127us/step - loss: 3.5421e-04 - mae: 0.0143 - val_loss: 0.0552 - val_mae: 0.1936\n",
            "Epoch 542/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 2.7159e-04 - mae: 0.0129 - val_loss: 0.0644 - val_mae: 0.2187\n",
            "Epoch 543/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 4.7781e-04 - mae: 0.0170 - val_loss: 0.0929 - val_mae: 0.2732\n",
            "Epoch 544/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 3.8002e-04 - mae: 0.0152 - val_loss: 0.0600 - val_mae: 0.2098\n",
            "Epoch 545/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 2.5761e-04 - mae: 0.0123 - val_loss: 0.0339 - val_mae: 0.1424\n",
            "Epoch 546/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0012 - mae: 0.0267 - val_loss: 0.0522 - val_mae: 0.1907\n",
            "Epoch 547/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0011 - mae: 0.0261 - val_loss: 0.0380 - val_mae: 0.1492\n",
            "Epoch 548/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 0.0020 - mae: 0.0331 - val_loss: 0.0241 - val_mae: 0.1161\n",
            "Epoch 549/700\n",
            "675/675 [==============================] - 0s 126us/step - loss: 0.0091 - mae: 0.0770 - val_loss: 0.0489 - val_mae: 0.1764\n",
            "Epoch 550/700\n",
            "675/675 [==============================] - 0s 128us/step - loss: 0.0012 - mae: 0.0263 - val_loss: 0.0269 - val_mae: 0.1216\n",
            "Epoch 551/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 8.8713e-04 - mae: 0.0237 - val_loss: 0.0820 - val_mae: 0.2547\n",
            "Epoch 552/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0042 - mae: 0.0501 - val_loss: 0.1112 - val_mae: 0.3020\n",
            "Epoch 553/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 0.0014 - mae: 0.0279 - val_loss: 0.0633 - val_mae: 0.2134\n",
            "Epoch 554/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 2.4799e-04 - mae: 0.0115 - val_loss: 0.0470 - val_mae: 0.1771\n",
            "Epoch 555/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 4.2633e-04 - mae: 0.0167 - val_loss: 0.0422 - val_mae: 0.1621\n",
            "Epoch 556/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 2.1950e-04 - mae: 0.0112 - val_loss: 0.0602 - val_mae: 0.2101\n",
            "Epoch 557/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 2.6057e-04 - mae: 0.0124 - val_loss: 0.0384 - val_mae: 0.1556\n",
            "Epoch 558/700\n",
            "675/675 [==============================] - 0s 155us/step - loss: 3.8450e-04 - mae: 0.0153 - val_loss: 0.0258 - val_mae: 0.1196\n",
            "Epoch 559/700\n",
            "675/675 [==============================] - 0s 170us/step - loss: 3.3586e-04 - mae: 0.0149 - val_loss: 0.0354 - val_mae: 0.1452\n",
            "Epoch 560/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 3.0559e-04 - mae: 0.0130 - val_loss: 0.0379 - val_mae: 0.1566\n",
            "Epoch 561/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 4.6601e-04 - mae: 0.0168 - val_loss: 0.0253 - val_mae: 0.1183\n",
            "Epoch 562/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 4.6083e-04 - mae: 0.0166 - val_loss: 0.0397 - val_mae: 0.1643\n",
            "Epoch 563/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 4.5116e-04 - mae: 0.0165 - val_loss: 0.0414 - val_mae: 0.1711\n",
            "Epoch 564/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0025 - mae: 0.0369 - val_loss: 0.1607 - val_mae: 0.3659\n",
            "Epoch 565/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0079 - mae: 0.0705 - val_loss: 0.1154 - val_mae: 0.2934\n",
            "Epoch 566/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 0.0017 - mae: 0.0325 - val_loss: 0.0206 - val_mae: 0.1149\n",
            "Epoch 567/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0013 - mae: 0.0288 - val_loss: 0.0526 - val_mae: 0.1810\n",
            "Epoch 568/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 9.0349e-04 - mae: 0.0236 - val_loss: 0.0407 - val_mae: 0.1599\n",
            "Epoch 569/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 2.6362e-04 - mae: 0.0119 - val_loss: 0.0486 - val_mae: 0.1822\n",
            "Epoch 570/700\n",
            "675/675 [==============================] - 0s 154us/step - loss: 4.9126e-04 - mae: 0.0172 - val_loss: 0.0363 - val_mae: 0.1512\n",
            "Epoch 571/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 3.6885e-04 - mae: 0.0147 - val_loss: 0.0400 - val_mae: 0.1618\n",
            "Epoch 572/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 3.4670e-04 - mae: 0.0139 - val_loss: 0.0660 - val_mae: 0.2231\n",
            "Epoch 573/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 3.8361e-04 - mae: 0.0154 - val_loss: 0.0472 - val_mae: 0.1817\n",
            "Epoch 574/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 7.5807e-04 - mae: 0.0222 - val_loss: 0.0759 - val_mae: 0.2485\n",
            "Epoch 575/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0018 - mae: 0.0335 - val_loss: 0.0240 - val_mae: 0.1152\n",
            "Epoch 576/700\n",
            "675/675 [==============================] - 0s 144us/step - loss: 6.6816e-04 - mae: 0.0202 - val_loss: 0.0631 - val_mae: 0.2197\n",
            "Epoch 577/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 9.0288e-04 - mae: 0.0236 - val_loss: 0.0493 - val_mae: 0.1859\n",
            "Epoch 578/700\n",
            "675/675 [==============================] - 0s 144us/step - loss: 8.5658e-04 - mae: 0.0226 - val_loss: 0.0295 - val_mae: 0.1313\n",
            "Epoch 579/700\n",
            "675/675 [==============================] - 0s 148us/step - loss: 3.0081e-04 - mae: 0.0138 - val_loss: 0.0349 - val_mae: 0.1509\n",
            "Epoch 580/700\n",
            "675/675 [==============================] - 0s 146us/step - loss: 2.7102e-04 - mae: 0.0128 - val_loss: 0.0340 - val_mae: 0.1465\n",
            "Epoch 581/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 7.9194e-04 - mae: 0.0209 - val_loss: 0.0619 - val_mae: 0.2100\n",
            "Epoch 582/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0018 - mae: 0.0322 - val_loss: 0.0524 - val_mae: 0.1948\n",
            "Epoch 583/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 0.0050 - mae: 0.0511 - val_loss: 0.0267 - val_mae: 0.1217\n",
            "Epoch 584/700\n",
            "675/675 [==============================] - 0s 153us/step - loss: 4.7381e-04 - mae: 0.0175 - val_loss: 0.0464 - val_mae: 0.1763\n",
            "Epoch 585/700\n",
            "675/675 [==============================] - 0s 154us/step - loss: 6.2457e-04 - mae: 0.0193 - val_loss: 0.0137 - val_mae: 0.0922\n",
            "Epoch 586/700\n",
            "675/675 [==============================] - 0s 155us/step - loss: 0.0043 - mae: 0.0498 - val_loss: 0.0395 - val_mae: 0.1503\n",
            "Epoch 587/700\n",
            "675/675 [==============================] - 0s 163us/step - loss: 0.0078 - mae: 0.0703 - val_loss: 0.0712 - val_mae: 0.2246\n",
            "Epoch 588/700\n",
            "675/675 [==============================] - 0s 128us/step - loss: 0.0014 - mae: 0.0277 - val_loss: 0.0585 - val_mae: 0.2005\n",
            "Epoch 589/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 5.4636e-04 - mae: 0.0190 - val_loss: 0.0491 - val_mae: 0.1824\n",
            "Epoch 590/700\n",
            "675/675 [==============================] - 0s 127us/step - loss: 3.3664e-04 - mae: 0.0143 - val_loss: 0.0502 - val_mae: 0.1858\n",
            "Epoch 591/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 3.0262e-04 - mae: 0.0128 - val_loss: 0.0322 - val_mae: 0.1385\n",
            "Epoch 592/700\n",
            "675/675 [==============================] - 0s 127us/step - loss: 3.2936e-04 - mae: 0.0132 - val_loss: 0.0689 - val_mae: 0.2326\n",
            "Epoch 593/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 8.3959e-04 - mae: 0.0228 - val_loss: 0.0412 - val_mae: 0.1628\n",
            "Epoch 594/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 0.0011 - mae: 0.0258 - val_loss: 0.0491 - val_mae: 0.1810\n",
            "Epoch 595/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 5.4903e-04 - mae: 0.0174 - val_loss: 0.0354 - val_mae: 0.1468\n",
            "Epoch 596/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 3.2766e-04 - mae: 0.0143 - val_loss: 0.0376 - val_mae: 0.1562\n",
            "Epoch 597/700\n",
            "675/675 [==============================] - 0s 126us/step - loss: 0.0013 - mae: 0.0293 - val_loss: 0.0499 - val_mae: 0.1889\n",
            "Epoch 598/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 4.0925e-04 - mae: 0.0155 - val_loss: 0.0553 - val_mae: 0.2022\n",
            "Epoch 599/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 5.0249e-04 - mae: 0.0167 - val_loss: 0.0431 - val_mae: 0.1730\n",
            "Epoch 600/700\n",
            "675/675 [==============================] - 0s 149us/step - loss: 3.3740e-04 - mae: 0.0140 - val_loss: 0.0304 - val_mae: 0.1348\n",
            "Epoch 601/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 4.8913e-04 - mae: 0.0167 - val_loss: 0.1238 - val_mae: 0.3279\n",
            "Epoch 602/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 0.0037 - mae: 0.0447 - val_loss: 0.1068 - val_mae: 0.2796\n",
            "Epoch 603/700\n",
            "675/675 [==============================] - 0s 127us/step - loss: 0.0010 - mae: 0.0248 - val_loss: 0.0589 - val_mae: 0.1983\n",
            "Epoch 604/700\n",
            "675/675 [==============================] - 0s 157us/step - loss: 3.2893e-04 - mae: 0.0139 - val_loss: 0.0521 - val_mae: 0.1904\n",
            "Epoch 605/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 4.9558e-04 - mae: 0.0175 - val_loss: 0.0988 - val_mae: 0.2874\n",
            "Epoch 606/700\n",
            "675/675 [==============================] - 0s 154us/step - loss: 4.1245e-04 - mae: 0.0162 - val_loss: 0.0462 - val_mae: 0.1780\n",
            "Epoch 607/700\n",
            "675/675 [==============================] - 0s 162us/step - loss: 5.3477e-04 - mae: 0.0179 - val_loss: 0.0709 - val_mae: 0.2368\n",
            "Epoch 608/700\n",
            "675/675 [==============================] - 0s 155us/step - loss: 4.1809e-04 - mae: 0.0164 - val_loss: 0.0613 - val_mae: 0.2176\n",
            "Epoch 609/700\n",
            "675/675 [==============================] - 0s 152us/step - loss: 8.3260e-04 - mae: 0.0229 - val_loss: 0.0529 - val_mae: 0.1981\n",
            "Epoch 610/700\n",
            "675/675 [==============================] - 0s 155us/step - loss: 6.3389e-04 - mae: 0.0199 - val_loss: 0.0373 - val_mae: 0.1544\n",
            "Epoch 611/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 7.6699e-04 - mae: 0.0219 - val_loss: 0.0552 - val_mae: 0.2039\n",
            "Epoch 612/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0013 - mae: 0.0294 - val_loss: 0.0318 - val_mae: 0.1412\n",
            "Epoch 613/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0032 - mae: 0.0440 - val_loss: 0.1161 - val_mae: 0.3145\n",
            "Epoch 614/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 0.0021 - mae: 0.0333 - val_loss: 0.0298 - val_mae: 0.1339\n",
            "Epoch 615/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 7.5835e-04 - mae: 0.0199 - val_loss: 0.0808 - val_mae: 0.2534\n",
            "Epoch 616/700\n",
            "675/675 [==============================] - 0s 145us/step - loss: 3.9897e-04 - mae: 0.0156 - val_loss: 0.0237 - val_mae: 0.1145\n",
            "Epoch 617/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 2.9798e-04 - mae: 0.0130 - val_loss: 0.0351 - val_mae: 0.1519\n",
            "Epoch 618/700\n",
            "675/675 [==============================] - 0s 128us/step - loss: 0.0012 - mae: 0.0260 - val_loss: 0.0795 - val_mae: 0.2567\n",
            "Epoch 619/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 9.0729e-04 - mae: 0.0228 - val_loss: 0.0305 - val_mae: 0.1374\n",
            "Epoch 620/700\n",
            "675/675 [==============================] - 0s 127us/step - loss: 5.4744e-04 - mae: 0.0179 - val_loss: 0.0515 - val_mae: 0.1980\n",
            "Epoch 621/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 0.0029 - mae: 0.0411 - val_loss: 0.0427 - val_mae: 0.1724\n",
            "Epoch 622/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 2.2894e-04 - mae: 0.0117 - val_loss: 0.0299 - val_mae: 0.1373\n",
            "Epoch 623/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 2.9599e-04 - mae: 0.0129 - val_loss: 0.0238 - val_mae: 0.1167\n",
            "Epoch 624/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 2.6235e-04 - mae: 0.0121 - val_loss: 0.0277 - val_mae: 0.1298\n",
            "Epoch 625/700\n",
            "675/675 [==============================] - 0s 143us/step - loss: 5.4897e-04 - mae: 0.0182 - val_loss: 0.0606 - val_mae: 0.2161\n",
            "Epoch 626/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 7.8715e-04 - mae: 0.0221 - val_loss: 0.0609 - val_mae: 0.2179\n",
            "Epoch 627/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 0.0015 - mae: 0.0287 - val_loss: 0.0366 - val_mae: 0.1571\n",
            "Epoch 628/700\n",
            "675/675 [==============================] - 0s 158us/step - loss: 0.0035 - mae: 0.0370 - val_loss: 0.0599 - val_mae: 0.2054\n",
            "Epoch 629/700\n",
            "675/675 [==============================] - 0s 150us/step - loss: 0.0049 - mae: 0.0519 - val_loss: 0.0086 - val_mae: 0.0825\n",
            "Epoch 630/700\n",
            "675/675 [==============================] - 0s 150us/step - loss: 9.4081e-04 - mae: 0.0219 - val_loss: 0.0303 - val_mae: 0.1402\n",
            "Epoch 631/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 3.0788e-04 - mae: 0.0136 - val_loss: 0.0158 - val_mae: 0.0927\n",
            "Epoch 632/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 7.9976e-04 - mae: 0.0228 - val_loss: 0.0481 - val_mae: 0.1877\n",
            "Epoch 633/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 8.9235e-04 - mae: 0.0239 - val_loss: 0.0204 - val_mae: 0.1055\n",
            "Epoch 634/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 3.2591e-04 - mae: 0.0137 - val_loss: 0.0321 - val_mae: 0.1421\n",
            "Epoch 635/700\n",
            "675/675 [==============================] - 0s 126us/step - loss: 2.9122e-04 - mae: 0.0128 - val_loss: 0.0198 - val_mae: 0.1029\n",
            "Epoch 636/700\n",
            "675/675 [==============================] - 0s 128us/step - loss: 5.9035e-04 - mae: 0.0195 - val_loss: 0.0230 - val_mae: 0.1129\n",
            "Epoch 637/700\n",
            "675/675 [==============================] - 0s 128us/step - loss: 9.5307e-04 - mae: 0.0247 - val_loss: 0.0254 - val_mae: 0.1198\n",
            "Epoch 638/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 8.0909e-04 - mae: 0.0230 - val_loss: 0.0165 - val_mae: 0.0953\n",
            "Epoch 639/700\n",
            "675/675 [==============================] - 0s 128us/step - loss: 7.8208e-04 - mae: 0.0219 - val_loss: 0.0904 - val_mae: 0.2738\n",
            "Epoch 640/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 3.1117e-04 - mae: 0.0140 - val_loss: 0.0228 - val_mae: 0.1118\n",
            "Epoch 641/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 2.1202e-04 - mae: 0.0111 - val_loss: 0.0375 - val_mae: 0.1609\n",
            "Epoch 642/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0019 - mae: 0.0342 - val_loss: 0.0299 - val_mae: 0.1320\n",
            "Epoch 643/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 0.0031 - mae: 0.0446 - val_loss: 0.0554 - val_mae: 0.1945\n",
            "Epoch 644/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 9.0960e-04 - mae: 0.0224 - val_loss: 0.0353 - val_mae: 0.1517\n",
            "Epoch 645/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 3.1088e-04 - mae: 0.0138 - val_loss: 0.0291 - val_mae: 0.1318\n",
            "Epoch 646/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 2.5130e-04 - mae: 0.0121 - val_loss: 0.0309 - val_mae: 0.1402\n",
            "Epoch 647/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 3.0847e-04 - mae: 0.0135 - val_loss: 0.0140 - val_mae: 0.0896\n",
            "Epoch 648/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 7.2319e-04 - mae: 0.0216 - val_loss: 0.0214 - val_mae: 0.1083\n",
            "Epoch 649/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 0.0012 - mae: 0.0244 - val_loss: 0.0245 - val_mae: 0.1176\n",
            "Epoch 650/700\n",
            "675/675 [==============================] - 0s 162us/step - loss: 3.0184e-04 - mae: 0.0133 - val_loss: 0.0305 - val_mae: 0.1391\n",
            "Epoch 651/700\n",
            "675/675 [==============================] - 0s 145us/step - loss: 1.9663e-04 - mae: 0.0111 - val_loss: 0.0310 - val_mae: 0.1422\n",
            "Epoch 652/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 6.1964e-04 - mae: 0.0192 - val_loss: 0.0335 - val_mae: 0.1451\n",
            "Epoch 653/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 4.6806e-04 - mae: 0.0170 - val_loss: 0.0507 - val_mae: 0.1941\n",
            "Epoch 654/700\n",
            "675/675 [==============================] - 0s 151us/step - loss: 0.0012 - mae: 0.0268 - val_loss: 0.0673 - val_mae: 0.2283\n",
            "Epoch 655/700\n",
            "675/675 [==============================] - 0s 181us/step - loss: 3.9436e-04 - mae: 0.0154 - val_loss: 0.0295 - val_mae: 0.1328\n",
            "Epoch 656/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 5.9697e-04 - mae: 0.0192 - val_loss: 0.0452 - val_mae: 0.1799\n",
            "Epoch 657/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 6.4091e-04 - mae: 0.0203 - val_loss: 0.0385 - val_mae: 0.1629\n",
            "Epoch 658/700\n",
            "675/675 [==============================] - 0s 147us/step - loss: 2.2057e-04 - mae: 0.0117 - val_loss: 0.0334 - val_mae: 0.1456\n",
            "Epoch 659/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 7.8297e-04 - mae: 0.0219 - val_loss: 0.0473 - val_mae: 0.1925\n",
            "Epoch 660/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0041 - mae: 0.0515 - val_loss: 0.1617 - val_mae: 0.3768\n",
            "Epoch 661/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 0.0028 - mae: 0.0425 - val_loss: 0.0199 - val_mae: 0.1103\n",
            "Epoch 662/700\n",
            "675/675 [==============================] - 0s 147us/step - loss: 0.0024 - mae: 0.0360 - val_loss: 0.0454 - val_mae: 0.1707\n",
            "Epoch 663/700\n",
            "675/675 [==============================] - 0s 157us/step - loss: 0.0015 - mae: 0.0322 - val_loss: 0.0451 - val_mae: 0.1732\n",
            "Epoch 664/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 5.0831e-04 - mae: 0.0171 - val_loss: 0.0287 - val_mae: 0.1291\n",
            "Epoch 665/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 7.8473e-04 - mae: 0.0226 - val_loss: 0.0531 - val_mae: 0.1974\n",
            "Epoch 666/700\n",
            "675/675 [==============================] - 0s 144us/step - loss: 8.2990e-04 - mae: 0.0233 - val_loss: 0.0467 - val_mae: 0.1843\n",
            "Epoch 667/700\n",
            "675/675 [==============================] - 0s 160us/step - loss: 9.6304e-04 - mae: 0.0197 - val_loss: 0.0490 - val_mae: 0.1855\n",
            "Epoch 668/700\n",
            "675/675 [==============================] - 0s 166us/step - loss: 3.8054e-04 - mae: 0.0148 - val_loss: 0.0873 - val_mae: 0.2682\n",
            "Epoch 669/700\n",
            "675/675 [==============================] - 0s 156us/step - loss: 8.8529e-04 - mae: 0.0233 - val_loss: 0.0089 - val_mae: 0.0834\n",
            "Epoch 670/700\n",
            "675/675 [==============================] - 0s 153us/step - loss: 9.6576e-04 - mae: 0.0240 - val_loss: 0.0428 - val_mae: 0.1706\n",
            "Epoch 671/700\n",
            "675/675 [==============================] - 0s 154us/step - loss: 3.2508e-04 - mae: 0.0137 - val_loss: 0.0315 - val_mae: 0.1401\n",
            "Epoch 672/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 5.5591e-04 - mae: 0.0180 - val_loss: 0.0461 - val_mae: 0.1843\n",
            "Epoch 673/700\n",
            "675/675 [==============================] - 0s 146us/step - loss: 5.9562e-04 - mae: 0.0183 - val_loss: 0.0534 - val_mae: 0.2006\n",
            "Epoch 674/700\n",
            "675/675 [==============================] - 0s 143us/step - loss: 7.2772e-04 - mae: 0.0219 - val_loss: 0.0399 - val_mae: 0.1654\n",
            "Epoch 675/700\n",
            "675/675 [==============================] - 0s 162us/step - loss: 4.3034e-04 - mae: 0.0166 - val_loss: 0.0198 - val_mae: 0.1042\n",
            "Epoch 676/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 5.9226e-04 - mae: 0.0191 - val_loss: 0.0129 - val_mae: 0.0828\n",
            "Epoch 677/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 3.3800e-04 - mae: 0.0141 - val_loss: 0.0189 - val_mae: 0.1008\n",
            "Epoch 678/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 8.7935e-04 - mae: 0.0227 - val_loss: 0.0545 - val_mae: 0.2011\n",
            "Epoch 679/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 6.3037e-04 - mae: 0.0190 - val_loss: 0.0231 - val_mae: 0.1140\n",
            "Epoch 680/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 0.0037 - mae: 0.0432 - val_loss: 0.2558 - val_mae: 0.4709\n",
            "Epoch 681/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0112 - mae: 0.0848 - val_loss: 0.1170 - val_mae: 0.3057\n",
            "Epoch 682/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0010 - mae: 0.0234 - val_loss: 0.0437 - val_mae: 0.1707\n",
            "Epoch 683/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 2.4248e-04 - mae: 0.0122 - val_loss: 0.0388 - val_mae: 0.1626\n",
            "Epoch 684/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 1.9988e-04 - mae: 0.0109 - val_loss: 0.0327 - val_mae: 0.1466\n",
            "Epoch 685/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 3.0965e-04 - mae: 0.0136 - val_loss: 0.0246 - val_mae: 0.1213\n",
            "Epoch 686/700\n",
            "675/675 [==============================] - 0s 176us/step - loss: 3.9482e-04 - mae: 0.0156 - val_loss: 0.0308 - val_mae: 0.1417\n",
            "Epoch 687/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 3.0610e-04 - mae: 0.0142 - val_loss: 0.0315 - val_mae: 0.1445\n",
            "Epoch 688/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 1.5569e-04 - mae: 0.0097 - val_loss: 0.0179 - val_mae: 0.0983\n",
            "Epoch 689/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 4.2230e-04 - mae: 0.0165 - val_loss: 0.0119 - val_mae: 0.0822\n",
            "Epoch 690/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 4.4246e-04 - mae: 0.0166 - val_loss: 0.0256 - val_mae: 0.1286\n",
            "Epoch 691/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 2.0570e-04 - mae: 0.0112 - val_loss: 0.0296 - val_mae: 0.1428\n",
            "Epoch 692/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 2.3325e-04 - mae: 0.0118 - val_loss: 0.0192 - val_mae: 0.1028\n",
            "Epoch 693/700\n",
            "675/675 [==============================] - 0s 147us/step - loss: 4.7637e-04 - mae: 0.0164 - val_loss: 0.0114 - val_mae: 0.0798\n",
            "Epoch 694/700\n",
            "675/675 [==============================] - 0s 143us/step - loss: 7.5155e-04 - mae: 0.0212 - val_loss: 0.0341 - val_mae: 0.1541\n",
            "Epoch 695/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 2.0822e-04 - mae: 0.0114 - val_loss: 0.0245 - val_mae: 0.1228\n",
            "Epoch 696/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 5.3642e-04 - mae: 0.0187 - val_loss: 0.0432 - val_mae: 0.1781\n",
            "Epoch 697/700\n",
            "675/675 [==============================] - 0s 125us/step - loss: 3.4263e-04 - mae: 0.0145 - val_loss: 0.0308 - val_mae: 0.1435\n",
            "Epoch 698/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 4.5016e-04 - mae: 0.0158 - val_loss: 0.0063 - val_mae: 0.0691\n",
            "Epoch 699/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0026 - mae: 0.0374 - val_loss: 0.3132 - val_mae: 0.5342\n",
            "Epoch 700/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 0.0023 - mae: 0.0360 - val_loss: 0.1216 - val_mae: 0.3171\n",
            "[4.6744762855567314, 4.655614488376917, 4.636687130294606, 4.617696104029839, 4.5986433086694145, 4.57953064947699, 4.560360037702557, 4.5411333903913205, 4.5218526301919875, 4.502519685164511, 4.483136488587283, 4.463704978763809, 4.444227098828877, 4.424704796554252, 4.405140024153889, 4.385534738088728, 4.365890898871035, 4.346210470868363, 4.3264954221071115, 4.306747724075729, 4.286969351527561, 4.267162282283381, 4.247328497033605, 4.227469979140233, 4.207588714438506, 4.187686691038325, 4.1677658991254445, 4.147828330762454, 4.12787597968958, 4.1079108411253, 4.0879349115668315, 4.067950188590484, 4.047958670651905, 4.027962356886226, 4.00796324690816, 3.987963340612037, 3.967964637971824, 3.947969138841118, 3.9279788427531694, 3.9079957487209263, 3.8880218550371426, 3.8680591590745403, 3.84810965708608, 3.828175344005335, 3.8082582132470044, 3.7883602565075645, 3.7684834635661084, 3.748629822085364, 3.728801317412938, 3.708999932382773, 3.68922764711687, 3.66948643882728, 3.6497782816183797, 3.630105146289467, 3.610469000137677, 3.5908718067612693, 3.571315525863246, 3.5518021130554005, 3.5323335196627528, 3.5129116925284176, 3.4935385738189244, 3.474216100829999, 3.454946205792836, 3.435730815680887, 3.416571852017147, 3.397471230682017, 3.378430861721713, 3.359452649157264, 3.340538490794112, 3.3216902780323307, 3.3029098956774874, 3.2841992217521723, 3.265560127308184, 3.2469944762394336, 3.228504125095556, 3.2100909228962586, 3.191756710946416, 3.173503322651945, 3.1553325833364636, 3.1372463100587584, 3.1192463114310915, 3.1013343874383223, 3.083512329257924, 3.0657819190808624, 3.0481449299333807, 3.0306031254996957, 3.0131582599456324, 2.9958120777432065, 2.978566313496188, 2.9614226917666286, 2.9443829269024144, 2.927448722865832, 2.9106217730631734, 2.893903760175396, 2.877296355989857, 2.8608012212331335, 2.8444200054049595, 2.8281543466132657, 2.812005871410376, 2.7959761946303527, 2.780066919227517, 2.764279636116151, 2.7486159240114096, 2.7330773492714506, 2.7176654657407964, 2.702381814594962, 2.687227924186324, 2.6722053098912935, 2.657315473958782, 2.642559905359975, 2.6279400796394357, 2.6134574587675528, 2.599113490994342, 2.584909610704633, 2.570847238274616, 2.5569277799298145, 2.5431526276044636, 2.5295231588023173, 2.5160407364588973, 2.502706708805203, 2.489522409232885, 2.4764891561609192, 2.4636082529037493, 2.450880987540967, 2.4383086327885017, 2.425892445871353, 2.413633668397865, 2.4015335262355695, 2.3895932293886, 2.377813971876688, 2.3661969316157734, 2.354743270300199, 2.343454133286552, 2.3323306494791263, 2.3213739312170327, 2.310585074162967, 2.2999651571936437, 2.289515242291908, 2.279236374440546, 2.2691295815177757, 2.2591958741944658, 2.249436245833071, 2.239851672388295, 2.2304431123094943, 2.2212115064448374, 2.2121577779472155, 2.2032828321819364, 2.19458755663618, 2.186072820830255, 2.177739476230646, 2.1695883561648683, 2.1616202757381364, 2.1538360317518523, 2.1462364026239285, 2.1388221483109433, 2.1315940102321513, 2.124552711195337, 2.117698955324535, 2.111033427989622, 2.104556795737776, 2.098269706226826, 2.0921727881604832, 2.0862666512254724, 2.080551886030568, 2.0750290640475253, 2.0696987375539413, 2.0645614395780223, 2.0596176838452855, 2.0548679647271846, 2.050312757191673, 2.0459525167557073, 2.0417876794396994, 2.037818661723909, 2.0340458605068013, 2.030469653065353, 2.0270903970173277, 2.0239084302855153, 2.020924071063937, 2.018137617786028, 2.015549349094793, 2.013159523814946, 2.0109683809270225, 2.008976139543485, 2.0071829988868126, 2.005589138269576, 2.0041947170765093, 2.002999874748569, 2.0020047307689928, 2.0012093846513492, 2.000613915929587, 2.0002183841500827, 2.000022828865684, 2.0000272696317563, 2.000231706004228, 2.0006361175396306, 2.001240463797148, 2.0020446843426587, 2.0030486987547778, 2.004252406632901, 2.005655687607243, 2.0072584013508763, 2.0090603875937623, 2.011061466138778, 2.013261436879737, 2.015660079821398, 2.0182571551014656, 2.021052403014576, 2.024045544038268, 2.027236278860932, 2.030624288411746, 2.034209233892578, 2.037990756811867, 2.0419684790204737, 2.0461420027494897, 2.0505109106500212, 2.0550747658349193, 2.059833111922468, 2.064785473082024, 2.0699313540815982, 2.0752702403373795, 2.0808015979651917, 2.0865248738338797, 2.0924394956206274, 2.0985448718681843, 2.1048403920440135, 2.1113254266013444, 2.1179993270421242, 2.1248614259818717, 2.1319110372164105, 2.1391474557904937, 2.146569958068291, 2.154177801805761, 2.161970226224871, 2.1699464520896723, 2.178105681784225, 2.1864470993923586, 2.194969870779264, 2.203673143674907, 2.2125560477592465, 2.2216176947492783, 2.230857178487855, 2.2402735750343017, 2.2498659427568115, 2.2596333224266054, 2.2695747373138557, 2.2796891932853605, 2.2899756789039474, 2.300433165529628, 2.311060607422455]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHvWqr856HwC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "2931b954-e0e8-4f76-9a2d-f35f1c0225d1"
      },
      "source": [
        "pyplot.scatter(xtrain,yhat, label='Predicted')\n",
        "pyplot.scatter(xtrain,ytrain,label='Real')"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f39dda83400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}