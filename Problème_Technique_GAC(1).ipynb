{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Problème Technique GAC",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7RMjTu_l7uO",
        "colab_type": "text"
      },
      "source": [
        "Objectif approximer la fonction f:x->2*cos(x)+4 sur [0,10]\n",
        "grâce à un percpetron multicouche(soit un réseaux de neuronnes)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pA6ycWMb0xC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "d9a03665-f991-4a91-83f2-e50851530928"
      },
      "source": [
        "# example of creating a univariate dataset with a given mapping function\n",
        "from matplotlib import pyplot\n",
        "from numpy import cos\n",
        "# define the input data\n",
        "x = [i/100 for i in range(0,1000)]\n",
        "# define the output data\n",
        "y = [2.0*cos(i)+4 for i in x]\n",
        "\n",
        "# plot the input versus the output\n",
        "pyplot.scatter(x,y)\n",
        "pyplot.title('Input (x) versus Output (y)')\n",
        "pyplot.xlabel('Input Variable (x)')\n",
        "pyplot.ylabel('Output Variable (y)')\n",
        "pyplot.show()\n",
        "\n"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3D5KYQGb3F4",
        "colab_type": "text"
      },
      "source": [
        "La fonction à approximer correpsond donc à un signal sinusoïdale qu'il faudra discrétiser afin de permettre la prédiction via le réseaux de neuronnes.\n",
        "L'étude du signal par un réseaux de neuronnes nécessite le formatage de notre data set x vecteurs des données en entrée et y vecteurs des données attendues en résultats afin de pouvoir calculer l'erreur d'approximation faite par le réseaux."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHrf1YvYoXgJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d7309611-2d56-40e7-b66f-3335601d32f4"
      },
      "source": [
        "# define the dataset\n",
        "import numpy as np\n",
        "x = np.asarray([i/100 for i in range(0,1001)])\n",
        "y = np.asarray([2.0*cos(i)+4 for i in x])\n",
        "print(x.min(), x.max(), y.min(), y.max())"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0 10.0 2.0000025365449208 6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX30l4Wjo67s",
        "colab_type": "text"
      },
      "source": [
        "Les données x et y sont reformatés sous forme de vecteurs afin d'être adapté aux algorithme d'apprentissage automatique sous python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhbi8nAHpHsm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6a397dae-15fd-447a-bb02-d6e941c19af0"
      },
      "source": [
        "import sklearn.preprocessing\n",
        "\n",
        "## prediction sur un x_test plus grand\n",
        "x_t = np.asarray([i/100 for i in range(0,1001)])\n",
        "x_t=x_t.reshape((len(x_t), 1))\n",
        "scale_xt = sklearn.preprocessing.MinMaxScaler()\n",
        "x_t = scale_xt.fit_transform(x_t)\n",
        "##\n",
        "\n",
        "x = x.reshape((len(x), 1))\n",
        "y = y.reshape((len(y), 1))\n",
        "x_t=x_t.reshape(len(x_t), 1)\n",
        "\n",
        "# separately scale the input and output variables\n",
        "scale_x = sklearn.preprocessing.MinMaxScaler()\n",
        "x = scale_x.fit_transform(x)\n",
        "scale_y = sklearn.preprocessing.MinMaxScaler()\n",
        "y = scale_y.fit_transform(y)\n",
        "print(x.min(), x.max(), y.min(), y.max())"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0 1.0 0.0 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6gb_Nb3rb-S",
        "colab_type": "text"
      },
      "source": [
        "Normalisation des data input et output pour faciliter la prédiction par le réseaux de neurones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EViK4L2Rtfke",
        "colab_type": "text"
      },
      "source": [
        "Construction du réseaux de neurones pour modéliser la fonction f.\n",
        "Un perceptron c’est 3 couches de neurones : couche d’entrée, couche(s) cachée(s), couche de sortie\n",
        "formule de sortie d'un neurone caché y=factivation(b+∑iwi⋅xi)\n",
        "neurone de sortie sera y=∑iwi⋅xi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xgco_qAlsvGY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Dense, Activation\n",
        "# design the neural network model\n",
        "##initialisation du réseaux de neuronnes vides\n",
        "model = keras.Sequential()\n",
        "#première couche cachée avec 10 noeuds\n",
        "model.add(Dense(10, input_dim=1, activation='softplus', kernel_initializer='he_uniform'))\n",
        "### fonction d'activation est une fonction mathématique appliquée à un signal en sortie d'un neurone artificiel\n",
        "## Softplus activation function, softplus(x) = log(exp(x) + 1) remplacement de la fonction d'activation relu par softplus\n",
        "### résultats beaucoup plus précis \n",
        "#ajout deuxièle couche cachée avec 10 noeuds\n",
        "model.add(Dense(10, activation='softplus', kernel_initializer='he_uniform'))\n",
        "#ajout troisième couche cachée\n",
        "model.add(Dense(10, activation='softplus', kernel_initializer='he_uniform'))\n",
        "#ajout 4eme\n",
        "#model.add(Dense(10, activation='relu', kernel_initializer='he_uniform'))\n",
        "#ajout couche de sortie\n",
        "model.add(Dense(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvT9u2Mkteyh",
        "colab_type": "text"
      },
      "source": [
        "Phase d’apprentissage :\n",
        " -repose intégralement sur la \"descente de gradient\" .\n",
        "On va adapter le modèle du signal en se basant sur une erreur d'approximation MSE soit l'erreure quadratique moyenne entre les valeurs de sorties attendues et les prédections calculées par le réseaux de neuronnes. \n",
        "De plus on optimise le modèle grâce à l'optimizer 'adam' disponible dans la librairie keras. Cet optimizer permet d'effectuer un gradiant descendant qui permet de diminuer l'erreur d'approximation donc d'optimiser le modèle.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i00TG6p3wDwt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "97db636a-3ec1-4b20-9a30-232159a42f89"
      },
      "source": [
        "# define the loss function and optimization algorithm\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "##argument loss déterlinant le type d'erreur qu'on choisit de calculer pour déterminer la précision du modèle\n",
        "##optimizer: adam optimisation algorithme ce choix d'optimizer détermine la méthode d'apprentissage et donc de réduction de l'erreur du modèle\n",
        "#### cet algorithme remplace la méthode du gradiant descendant\n",
        "\n",
        "## version alternative avec la méthode du gradiant descendant\n",
        "#model.compile(loss='mse', optimizer='sgd')\n",
        "\n",
        "# fit the model on the training dataset\n",
        "model.fit(x, y, epochs=500, batch_size=10, verbose=0)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f39dd07b1d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icrCI7fWxmeC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9deaca58-ab8c-4d54-e4ba-2072f85a32a9"
      },
      "source": [
        "# make predictions for the input data\n",
        "import numpy as np\n",
        "#x_t=np.random.uniform(10, 30,2001)\n",
        "print(len(x_t))\n",
        "\n",
        "yhat = model.predict(x)\n",
        "#print(yhat)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oLVotGkx4_q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "95c6fd97-e53a-4326-8e32-3764ea9a1e5d"
      },
      "source": [
        "# inverse transforms\n",
        "x_plot = scale_x.inverse_transform(x)\n",
        "y_plot = scale_y.inverse_transform(y)\n",
        "yhat_plot = scale_y.inverse_transform(yhat)\n",
        "\n",
        "##\n",
        "#xt_plot = scale_xt.inverse_transform(x_t)\n",
        "\n",
        "\n",
        "\n",
        "print(len(xt_plot))\n",
        "print(len(yhat))\n",
        "pyplot.scatter(x_plot,y_plot, label='Actual')\n",
        "# plot x vs yhat\n",
        "pyplot.scatter(x_plot,yhat_plot, label='Predicted')\n",
        "# plot x vs yhat\n",
        "pyplot.title('Input (x) versus Output (y)')\n",
        "pyplot.xlabel('Input Variable (x)')\n",
        "pyplot.ylabel('Output Variable (y)')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "501\n",
            "1001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np-ZKRXb412E",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQvgyZZs42MI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "277d4213-8cd2-4a44-e842-23456044a237"
      },
      "source": [
        "import keras\n",
        "from keras.layers import Dense, Activation\n",
        "\n",
        "import numpy as np\n",
        "# define the input data\n",
        "x = [i/100 for i in range(0,1000)]\n",
        "# define the output data\n",
        "y = [2.0*cos(i)+4 for i in x]\n",
        "\n",
        "xtrain = x[0:750]\n",
        "ytrain = y[0:750]\n",
        "\n",
        "xtest = x[751:1000]\n",
        "ytest = y[751:1000]\n",
        "model = keras.Sequential()\n",
        "\n",
        "model.add(Dense(10, input_dim=1, activation='relu'))\n",
        "model.add(Dense(10, activation='softplus', kernel_initializer='he_uniform'))\n",
        "#ajout troisième couche cachée\n",
        "model.add(Dense(10, activation='softplus', kernel_initializer='he_uniform'))\n",
        "model.add(Dense(1))\n",
        "model.summary()\n",
        "sgd = keras.optimizers.Adam(lr=0.01, decay=1e-6)\n",
        "model.compile(loss='mean_squared_error', \n",
        "          optimizer=sgd, \n",
        "          metrics=['mae'])\n",
        "\n",
        "history = model.fit(xtrain, ytrain,\n",
        "                batch_size=10, \n",
        "                epochs=700, \n",
        "                verbose=1, \n",
        "                validation_split=0.1)\n",
        "\n",
        "\n",
        "yhat = model.predict(xtrain)\n",
        "print(ytest)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_37 (Dense)             (None, 10)                20        \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 251\n",
            "Trainable params: 251\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 675 samples, validate on 75 samples\n",
            "Epoch 1/700\n",
            "675/675 [==============================] - 0s 325us/step - loss: 6.1578 - mae: 1.9014 - val_loss: 0.2802 - val_mae: 0.4458\n",
            "Epoch 2/700\n",
            "675/675 [==============================] - 0s 176us/step - loss: 0.7590 - mae: 0.7558 - val_loss: 0.4344 - val_mae: 0.5255\n",
            "Epoch 3/700\n",
            "675/675 [==============================] - 0s 149us/step - loss: 0.3833 - mae: 0.5097 - val_loss: 1.0717 - val_mae: 0.8985\n",
            "Epoch 4/700\n",
            "675/675 [==============================] - 0s 143us/step - loss: 0.2171 - mae: 0.3738 - val_loss: 2.7210 - val_mae: 1.5512\n",
            "Epoch 5/700\n",
            "675/675 [==============================] - 0s 185us/step - loss: 0.1102 - mae: 0.2723 - val_loss: 3.3450 - val_mae: 1.7290\n",
            "Epoch 6/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 0.0706 - mae: 0.2031 - val_loss: 3.5429 - val_mae: 1.7835\n",
            "Epoch 7/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0611 - mae: 0.1804 - val_loss: 3.9780 - val_mae: 1.9020\n",
            "Epoch 8/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 0.0614 - mae: 0.1852 - val_loss: 3.8965 - val_mae: 1.8813\n",
            "Epoch 9/700\n",
            "675/675 [==============================] - 0s 143us/step - loss: 0.0494 - mae: 0.1563 - val_loss: 3.6397 - val_mae: 1.8122\n",
            "Epoch 10/700\n",
            "675/675 [==============================] - 0s 149us/step - loss: 0.0493 - mae: 0.1544 - val_loss: 3.6292 - val_mae: 1.8096\n",
            "Epoch 11/700\n",
            "675/675 [==============================] - 0s 159us/step - loss: 0.0492 - mae: 0.1530 - val_loss: 3.9160 - val_mae: 1.8880\n",
            "Epoch 12/700\n",
            "675/675 [==============================] - 0s 162us/step - loss: 0.0529 - mae: 0.1529 - val_loss: 2.9443 - val_mae: 1.6149\n",
            "Epoch 13/700\n",
            "675/675 [==============================] - 0s 126us/step - loss: 0.0496 - mae: 0.1604 - val_loss: 4.7650 - val_mae: 2.1007\n",
            "Epoch 14/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0491 - mae: 0.1575 - val_loss: 3.5892 - val_mae: 1.8032\n",
            "Epoch 15/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0483 - mae: 0.1493 - val_loss: 3.8087 - val_mae: 1.8634\n",
            "Epoch 16/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0466 - mae: 0.1537 - val_loss: 4.6990 - val_mae: 2.0870\n",
            "Epoch 17/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0538 - mae: 0.1670 - val_loss: 3.5322 - val_mae: 1.7893\n",
            "Epoch 18/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0388 - mae: 0.1362 - val_loss: 4.1134 - val_mae: 1.9451\n",
            "Epoch 19/700\n",
            "675/675 [==============================] - 0s 143us/step - loss: 0.0382 - mae: 0.1355 - val_loss: 3.4232 - val_mae: 1.7627\n",
            "Epoch 20/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0361 - mae: 0.1354 - val_loss: 3.1979 - val_mae: 1.7000\n",
            "Epoch 21/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 0.0283 - mae: 0.1144 - val_loss: 2.6532 - val_mae: 1.5357\n",
            "Epoch 22/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0278 - mae: 0.1142 - val_loss: 2.8923 - val_mae: 1.6130\n",
            "Epoch 23/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0270 - mae: 0.1101 - val_loss: 2.8016 - val_mae: 1.5879\n",
            "Epoch 24/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 0.0220 - mae: 0.0976 - val_loss: 2.8198 - val_mae: 1.5959\n",
            "Epoch 25/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 0.0251 - mae: 0.1131 - val_loss: 3.1721 - val_mae: 1.7036\n",
            "Epoch 26/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 0.0307 - mae: 0.1249 - val_loss: 2.2815 - val_mae: 1.4241\n",
            "Epoch 27/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0267 - mae: 0.1182 - val_loss: 1.7772 - val_mae: 1.2390\n",
            "Epoch 28/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0191 - mae: 0.0966 - val_loss: 2.0857 - val_mae: 1.3592\n",
            "Epoch 29/700\n",
            "675/675 [==============================] - 0s 145us/step - loss: 0.0162 - mae: 0.0864 - val_loss: 2.1087 - val_mae: 1.3706\n",
            "Epoch 30/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0361 - mae: 0.1427 - val_loss: 2.2410 - val_mae: 1.4189\n",
            "Epoch 31/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 0.0158 - mae: 0.0861 - val_loss: 1.8796 - val_mae: 1.2889\n",
            "Epoch 32/700\n",
            "675/675 [==============================] - 0s 158us/step - loss: 0.0134 - mae: 0.0776 - val_loss: 2.1231 - val_mae: 1.3819\n",
            "Epoch 33/700\n",
            "675/675 [==============================] - 0s 169us/step - loss: 0.0177 - mae: 0.0982 - val_loss: 1.0154 - val_mae: 0.9051\n",
            "Epoch 34/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0122 - mae: 0.0753 - val_loss: 1.6663 - val_mae: 1.2119\n",
            "Epoch 35/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 0.0122 - mae: 0.0755 - val_loss: 1.3746 - val_mae: 1.0896\n",
            "Epoch 36/700\n",
            "675/675 [==============================] - 0s 150us/step - loss: 0.0135 - mae: 0.0817 - val_loss: 1.1205 - val_mae: 0.9703\n",
            "Epoch 37/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0176 - mae: 0.1001 - val_loss: 1.5410 - val_mae: 1.1659\n",
            "Epoch 38/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0097 - mae: 0.0721 - val_loss: 1.3868 - val_mae: 1.1014\n",
            "Epoch 39/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 0.0090 - mae: 0.0695 - val_loss: 1.2707 - val_mae: 1.0509\n",
            "Epoch 40/700\n",
            "675/675 [==============================] - 0s 166us/step - loss: 0.0088 - mae: 0.0692 - val_loss: 1.4216 - val_mae: 1.1219\n",
            "Epoch 41/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0073 - mae: 0.0622 - val_loss: 1.1804 - val_mae: 1.0118\n",
            "Epoch 42/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 0.0083 - mae: 0.0659 - val_loss: 1.3775 - val_mae: 1.1058\n",
            "Epoch 43/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 0.0110 - mae: 0.0789 - val_loss: 0.8490 - val_mae: 0.8380\n",
            "Epoch 44/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 0.0163 - mae: 0.0970 - val_loss: 1.1914 - val_mae: 1.0210\n",
            "Epoch 45/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 0.0086 - mae: 0.0693 - val_loss: 1.2292 - val_mae: 1.0398\n",
            "Epoch 46/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 0.0063 - mae: 0.0589 - val_loss: 0.9537 - val_mae: 0.9017\n",
            "Epoch 47/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0076 - mae: 0.0616 - val_loss: 0.9911 - val_mae: 0.9230\n",
            "Epoch 48/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0056 - mae: 0.0542 - val_loss: 1.0698 - val_mae: 0.9663\n",
            "Epoch 49/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 0.0060 - mae: 0.0539 - val_loss: 1.0097 - val_mae: 0.9373\n",
            "Epoch 50/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 0.0065 - mae: 0.0531 - val_loss: 0.1401 - val_mae: 0.3089\n",
            "Epoch 51/700\n",
            "675/675 [==============================] - 0s 144us/step - loss: 0.0123 - mae: 0.0786 - val_loss: 0.6130 - val_mae: 0.6984\n",
            "Epoch 52/700\n",
            "675/675 [==============================] - 0s 150us/step - loss: 0.0094 - mae: 0.0735 - val_loss: 0.6201 - val_mae: 0.7053\n",
            "Epoch 53/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0078 - mae: 0.0685 - val_loss: 0.4839 - val_mae: 0.6025\n",
            "Epoch 54/700\n",
            "675/675 [==============================] - 0s 147us/step - loss: 0.0056 - mae: 0.0556 - val_loss: 1.0196 - val_mae: 0.9472\n",
            "Epoch 55/700\n",
            "675/675 [==============================] - 0s 149us/step - loss: 0.0072 - mae: 0.0622 - val_loss: 0.8805 - val_mae: 0.8722\n",
            "Epoch 56/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0048 - mae: 0.0527 - val_loss: 0.7075 - val_mae: 0.7706\n",
            "Epoch 57/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 0.0083 - mae: 0.0704 - val_loss: 1.0242 - val_mae: 0.9529\n",
            "Epoch 58/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0102 - mae: 0.0791 - val_loss: 0.6734 - val_mae: 0.7474\n",
            "Epoch 59/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0088 - mae: 0.0710 - val_loss: 0.9298 - val_mae: 0.9020\n",
            "Epoch 60/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 0.0102 - mae: 0.0772 - val_loss: 0.7008 - val_mae: 0.7647\n",
            "Epoch 61/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0132 - mae: 0.0861 - val_loss: 0.9656 - val_mae: 0.9206\n",
            "Epoch 62/700\n",
            "675/675 [==============================] - 0s 149us/step - loss: 0.0080 - mae: 0.0685 - val_loss: 0.8464 - val_mae: 0.8547\n",
            "Epoch 63/700\n",
            "675/675 [==============================] - 0s 146us/step - loss: 0.0048 - mae: 0.0517 - val_loss: 0.7080 - val_mae: 0.7741\n",
            "Epoch 64/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 0.0039 - mae: 0.0464 - val_loss: 0.6193 - val_mae: 0.7184\n",
            "Epoch 65/700\n",
            "675/675 [==============================] - 0s 146us/step - loss: 0.0036 - mae: 0.0455 - val_loss: 0.5998 - val_mae: 0.7061\n",
            "Epoch 66/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 0.0054 - mae: 0.0521 - val_loss: 0.4586 - val_mae: 0.5993\n",
            "Epoch 67/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 0.0051 - mae: 0.0567 - val_loss: 0.5769 - val_mae: 0.6906\n",
            "Epoch 68/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 0.0061 - mae: 0.0571 - val_loss: 0.4562 - val_mae: 0.6003\n",
            "Epoch 69/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 0.0038 - mae: 0.0452 - val_loss: 0.6062 - val_mae: 0.7140\n",
            "Epoch 70/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0059 - mae: 0.0547 - val_loss: 0.5464 - val_mae: 0.6721\n",
            "Epoch 71/700\n",
            "675/675 [==============================] - 0s 145us/step - loss: 0.0043 - mae: 0.0477 - val_loss: 0.2687 - val_mae: 0.4251\n",
            "Epoch 72/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0091 - mae: 0.0725 - val_loss: 0.2322 - val_mae: 0.3852\n",
            "Epoch 73/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0080 - mae: 0.0718 - val_loss: 0.5710 - val_mae: 0.6899\n",
            "Epoch 74/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0045 - mae: 0.0516 - val_loss: 0.5662 - val_mae: 0.6900\n",
            "Epoch 75/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0092 - mae: 0.0718 - val_loss: 0.6879 - val_mae: 0.7717\n",
            "Epoch 76/700\n",
            "675/675 [==============================] - 0s 155us/step - loss: 0.0062 - mae: 0.0558 - val_loss: 0.4358 - val_mae: 0.5892\n",
            "Epoch 77/700\n",
            "675/675 [==============================] - 0s 154us/step - loss: 0.0024 - mae: 0.0357 - val_loss: 0.4439 - val_mae: 0.5995\n",
            "Epoch 78/700\n",
            "675/675 [==============================] - 0s 162us/step - loss: 0.0072 - mae: 0.0642 - val_loss: 0.8541 - val_mae: 0.8694\n",
            "Epoch 79/700\n",
            "675/675 [==============================] - 0s 152us/step - loss: 0.0184 - mae: 0.0988 - val_loss: 0.6059 - val_mae: 0.7100\n",
            "Epoch 80/700\n",
            "675/675 [==============================] - 0s 163us/step - loss: 0.0097 - mae: 0.0664 - val_loss: 0.6408 - val_mae: 0.7328\n",
            "Epoch 81/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0041 - mae: 0.0499 - val_loss: 0.4363 - val_mae: 0.5820\n",
            "Epoch 82/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 0.0064 - mae: 0.0601 - val_loss: 0.4538 - val_mae: 0.6000\n",
            "Epoch 83/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 0.0032 - mae: 0.0431 - val_loss: 0.7009 - val_mae: 0.7809\n",
            "Epoch 84/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0027 - mae: 0.0394 - val_loss: 0.2451 - val_mae: 0.4050\n",
            "Epoch 85/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0037 - mae: 0.0443 - val_loss: 0.1853 - val_mae: 0.3403\n",
            "Epoch 86/700\n",
            "675/675 [==============================] - 0s 144us/step - loss: 0.0050 - mae: 0.0545 - val_loss: 0.5166 - val_mae: 0.6583\n",
            "Epoch 87/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 0.0035 - mae: 0.0442 - val_loss: 0.5252 - val_mae: 0.6680\n",
            "Epoch 88/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0058 - mae: 0.0546 - val_loss: 0.6063 - val_mae: 0.7264\n",
            "Epoch 89/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0076 - mae: 0.0636 - val_loss: 0.8811 - val_mae: 0.8942\n",
            "Epoch 90/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0036 - mae: 0.0465 - val_loss: 0.4802 - val_mae: 0.6364\n",
            "Epoch 91/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0050 - mae: 0.0518 - val_loss: 0.2616 - val_mae: 0.4360\n",
            "Epoch 92/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0052 - mae: 0.0520 - val_loss: 0.5265 - val_mae: 0.6739\n",
            "Epoch 93/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 0.0038 - mae: 0.0457 - val_loss: 0.4895 - val_mae: 0.6448\n",
            "Epoch 94/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0038 - mae: 0.0467 - val_loss: 0.2592 - val_mae: 0.4362\n",
            "Epoch 95/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0037 - mae: 0.0474 - val_loss: 0.2512 - val_mae: 0.4312\n",
            "Epoch 96/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 0.0030 - mae: 0.0425 - val_loss: 0.3320 - val_mae: 0.5164\n",
            "Epoch 97/700\n",
            "675/675 [==============================] - 0s 155us/step - loss: 0.0034 - mae: 0.0450 - val_loss: 0.5130 - val_mae: 0.6694\n",
            "Epoch 98/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0031 - mae: 0.0427 - val_loss: 0.1213 - val_mae: 0.2719\n",
            "Epoch 99/700\n",
            "675/675 [==============================] - 0s 152us/step - loss: 0.0032 - mae: 0.0434 - val_loss: 0.2723 - val_mae: 0.4621\n",
            "Epoch 100/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0043 - mae: 0.0499 - val_loss: 0.8222 - val_mae: 0.8707\n",
            "Epoch 101/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 0.0630 - mae: 0.1800 - val_loss: 0.8101 - val_mae: 0.8267\n",
            "Epoch 102/700\n",
            "675/675 [==============================] - 0s 149us/step - loss: 0.0242 - mae: 0.1175 - val_loss: 0.8356 - val_mae: 0.8385\n",
            "Epoch 103/700\n",
            "675/675 [==============================] - 0s 150us/step - loss: 0.0040 - mae: 0.0454 - val_loss: 0.7858 - val_mae: 0.8195\n",
            "Epoch 104/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0063 - mae: 0.0555 - val_loss: 0.3935 - val_mae: 0.5413\n",
            "Epoch 105/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 0.0067 - mae: 0.0616 - val_loss: 0.6377 - val_mae: 0.7310\n",
            "Epoch 106/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0084 - mae: 0.0707 - val_loss: 0.6830 - val_mae: 0.7611\n",
            "Epoch 107/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0032 - mae: 0.0422 - val_loss: 0.4329 - val_mae: 0.5819\n",
            "Epoch 108/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 0.0047 - mae: 0.0472 - val_loss: 0.3585 - val_mae: 0.5187\n",
            "Epoch 109/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 0.0109 - mae: 0.0753 - val_loss: 0.4588 - val_mae: 0.6078\n",
            "Epoch 110/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 0.0137 - mae: 0.0893 - val_loss: 0.8416 - val_mae: 0.8620\n",
            "Epoch 111/700\n",
            "675/675 [==============================] - 0s 127us/step - loss: 0.0056 - mae: 0.0576 - val_loss: 0.6004 - val_mae: 0.7149\n",
            "Epoch 112/700\n",
            "675/675 [==============================] - 0s 128us/step - loss: 0.0041 - mae: 0.0468 - val_loss: 0.3186 - val_mae: 0.4874\n",
            "Epoch 113/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 0.0027 - mae: 0.0393 - val_loss: 0.5439 - val_mae: 0.6840\n",
            "Epoch 114/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 0.0024 - mae: 0.0365 - val_loss: 0.3576 - val_mae: 0.5386\n",
            "Epoch 115/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 0.0025 - mae: 0.0379 - val_loss: 0.3025 - val_mae: 0.4884\n",
            "Epoch 116/700\n",
            "675/675 [==============================] - 0s 126us/step - loss: 0.0034 - mae: 0.0452 - val_loss: 0.2411 - val_mae: 0.4259\n",
            "Epoch 117/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0029 - mae: 0.0409 - val_loss: 0.2235 - val_mae: 0.4021\n",
            "Epoch 118/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 0.0035 - mae: 0.0463 - val_loss: 0.2717 - val_mae: 0.4636\n",
            "Epoch 119/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 0.0031 - mae: 0.0428 - val_loss: 0.1681 - val_mae: 0.3387\n",
            "Epoch 120/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 0.0125 - mae: 0.0878 - val_loss: 0.3466 - val_mae: 0.5318\n",
            "Epoch 121/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0032 - mae: 0.0442 - val_loss: 0.2576 - val_mae: 0.4441\n",
            "Epoch 122/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0017 - mae: 0.0303 - val_loss: 0.3465 - val_mae: 0.5371\n",
            "Epoch 123/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0027 - mae: 0.0411 - val_loss: 0.2264 - val_mae: 0.4174\n",
            "Epoch 124/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 0.0025 - mae: 0.0387 - val_loss: 0.2016 - val_mae: 0.3907\n",
            "Epoch 125/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0029 - mae: 0.0400 - val_loss: 0.1851 - val_mae: 0.3717\n",
            "Epoch 126/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0011 - mae: 0.0256 - val_loss: 0.2382 - val_mae: 0.4386\n",
            "Epoch 127/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 0.0026 - mae: 0.0392 - val_loss: 0.2578 - val_mae: 0.4599\n",
            "Epoch 128/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 0.0045 - mae: 0.0521 - val_loss: 0.2263 - val_mae: 0.4185\n",
            "Epoch 129/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0054 - mae: 0.0532 - val_loss: 0.5052 - val_mae: 0.6751\n",
            "Epoch 130/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0105 - mae: 0.0786 - val_loss: 0.1455 - val_mae: 0.3014\n",
            "Epoch 131/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0032 - mae: 0.0451 - val_loss: 0.3306 - val_mae: 0.5232\n",
            "Epoch 132/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 0.0018 - mae: 0.0327 - val_loss: 0.1607 - val_mae: 0.3339\n",
            "Epoch 133/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0015 - mae: 0.0303 - val_loss: 0.1741 - val_mae: 0.3599\n",
            "Epoch 134/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0080 - mae: 0.0683 - val_loss: 0.0901 - val_mae: 0.2332\n",
            "Epoch 135/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 0.0049 - mae: 0.0524 - val_loss: 0.3075 - val_mae: 0.4990\n",
            "Epoch 136/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0016 - mae: 0.0311 - val_loss: 0.3931 - val_mae: 0.5838\n",
            "Epoch 137/700\n",
            "675/675 [==============================] - 0s 125us/step - loss: 0.0048 - mae: 0.0512 - val_loss: 0.2553 - val_mae: 0.4562\n",
            "Epoch 138/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 0.0044 - mae: 0.0489 - val_loss: 0.2179 - val_mae: 0.4082\n",
            "Epoch 139/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0035 - mae: 0.0448 - val_loss: 0.2989 - val_mae: 0.4933\n",
            "Epoch 140/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 0.0015 - mae: 0.0292 - val_loss: 0.1347 - val_mae: 0.2964\n",
            "Epoch 141/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 0.0015 - mae: 0.0290 - val_loss: 0.1685 - val_mae: 0.3555\n",
            "Epoch 142/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0012 - mae: 0.0264 - val_loss: 0.1606 - val_mae: 0.3476\n",
            "Epoch 143/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0018 - mae: 0.0332 - val_loss: 0.0972 - val_mae: 0.2457\n",
            "Epoch 144/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0074 - mae: 0.0676 - val_loss: 0.1224 - val_mae: 0.2809\n",
            "Epoch 145/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0082 - mae: 0.0683 - val_loss: 0.8487 - val_mae: 0.8896\n",
            "Epoch 146/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 0.0119 - mae: 0.0783 - val_loss: 0.0574 - val_mae: 0.2095\n",
            "Epoch 147/700\n",
            "675/675 [==============================] - 0s 156us/step - loss: 0.0101 - mae: 0.0722 - val_loss: 0.4420 - val_mae: 0.6104\n",
            "Epoch 148/700\n",
            "675/675 [==============================] - 0s 161us/step - loss: 0.0018 - mae: 0.0326 - val_loss: 0.2495 - val_mae: 0.4434\n",
            "Epoch 149/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0037 - mae: 0.0458 - val_loss: 0.3510 - val_mae: 0.5446\n",
            "Epoch 150/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0027 - mae: 0.0407 - val_loss: 0.2184 - val_mae: 0.4104\n",
            "Epoch 151/700\n",
            "675/675 [==============================] - 0s 147us/step - loss: 0.0036 - mae: 0.0465 - val_loss: 0.3339 - val_mae: 0.5352\n",
            "Epoch 152/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0069 - mae: 0.0609 - val_loss: 0.4970 - val_mae: 0.6582\n",
            "Epoch 153/700\n",
            "675/675 [==============================] - 0s 144us/step - loss: 0.0073 - mae: 0.0684 - val_loss: 0.4726 - val_mae: 0.6395\n",
            "Epoch 154/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 0.0187 - mae: 0.1028 - val_loss: 1.3558 - val_mae: 1.1213\n",
            "Epoch 155/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0115 - mae: 0.0810 - val_loss: 0.3958 - val_mae: 0.5540\n",
            "Epoch 156/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 0.0066 - mae: 0.0625 - val_loss: 0.6762 - val_mae: 0.7774\n",
            "Epoch 157/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 0.0104 - mae: 0.0750 - val_loss: 0.4122 - val_mae: 0.5866\n",
            "Epoch 158/700\n",
            "675/675 [==============================] - 0s 165us/step - loss: 0.0019 - mae: 0.0346 - val_loss: 0.2271 - val_mae: 0.4140\n",
            "Epoch 159/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 0.0025 - mae: 0.0391 - val_loss: 0.1620 - val_mae: 0.3359\n",
            "Epoch 160/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 0.0022 - mae: 0.0352 - val_loss: 0.3439 - val_mae: 0.5413\n",
            "Epoch 161/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0011 - mae: 0.0251 - val_loss: 0.1321 - val_mae: 0.2984\n",
            "Epoch 162/700\n",
            "675/675 [==============================] - 0s 147us/step - loss: 0.0011 - mae: 0.0247 - val_loss: 0.3145 - val_mae: 0.5170\n",
            "Epoch 163/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0022 - mae: 0.0351 - val_loss: 0.1701 - val_mae: 0.3621\n",
            "Epoch 164/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 0.0014 - mae: 0.0287 - val_loss: 0.1308 - val_mae: 0.3059\n",
            "Epoch 165/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0023 - mae: 0.0375 - val_loss: 0.2563 - val_mae: 0.4681\n",
            "Epoch 166/700\n",
            "675/675 [==============================] - 0s 143us/step - loss: 0.0022 - mae: 0.0358 - val_loss: 0.1601 - val_mae: 0.3510\n",
            "Epoch 167/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0022 - mae: 0.0312 - val_loss: 0.0377 - val_mae: 0.1597\n",
            "Epoch 168/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0091 - mae: 0.0701 - val_loss: 0.0487 - val_mae: 0.1844\n",
            "Epoch 169/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 0.0091 - mae: 0.0710 - val_loss: 0.3041 - val_mae: 0.5027\n",
            "Epoch 170/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 0.0016 - mae: 0.0316 - val_loss: 0.0828 - val_mae: 0.2231\n",
            "Epoch 171/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0033 - mae: 0.0421 - val_loss: 0.0673 - val_mae: 0.1994\n",
            "Epoch 172/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 0.0147 - mae: 0.0913 - val_loss: 0.7254 - val_mae: 0.8021\n",
            "Epoch 173/700\n",
            "675/675 [==============================] - 0s 148us/step - loss: 0.0036 - mae: 0.0482 - val_loss: 0.2329 - val_mae: 0.4210\n",
            "Epoch 174/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0014 - mae: 0.0296 - val_loss: 0.1965 - val_mae: 0.3870\n",
            "Epoch 175/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 0.0030 - mae: 0.0387 - val_loss: 0.1378 - val_mae: 0.3073\n",
            "Epoch 176/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 0.0039 - mae: 0.0478 - val_loss: 0.1764 - val_mae: 0.3682\n",
            "Epoch 177/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0035 - mae: 0.0475 - val_loss: 0.1231 - val_mae: 0.2913\n",
            "Epoch 178/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0119 - mae: 0.0840 - val_loss: 0.0523 - val_mae: 0.1915\n",
            "Epoch 179/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0102 - mae: 0.0768 - val_loss: 0.3856 - val_mae: 0.5713\n",
            "Epoch 180/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0019 - mae: 0.0334 - val_loss: 0.2563 - val_mae: 0.4550\n",
            "Epoch 181/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 0.0015 - mae: 0.0295 - val_loss: 0.1711 - val_mae: 0.3582\n",
            "Epoch 182/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0011 - mae: 0.0268 - val_loss: 0.2564 - val_mae: 0.4636\n",
            "Epoch 183/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 8.9043e-04 - mae: 0.0234 - val_loss: 0.1002 - val_mae: 0.2540\n",
            "Epoch 184/700\n",
            "675/675 [==============================] - 0s 151us/step - loss: 0.0011 - mae: 0.0255 - val_loss: 0.1122 - val_mae: 0.2824\n",
            "Epoch 185/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0011 - mae: 0.0260 - val_loss: 0.1147 - val_mae: 0.2894\n",
            "Epoch 186/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 0.0020 - mae: 0.0316 - val_loss: 0.1593 - val_mae: 0.3511\n",
            "Epoch 187/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 9.4424e-04 - mae: 0.0240 - val_loss: 0.1026 - val_mae: 0.2672\n",
            "Epoch 188/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0036 - mae: 0.0372 - val_loss: 0.2061 - val_mae: 0.4102\n",
            "Epoch 189/700\n",
            "675/675 [==============================] - 0s 158us/step - loss: 0.0163 - mae: 0.0997 - val_loss: 0.3509 - val_mae: 0.5342\n",
            "Epoch 190/700\n",
            "675/675 [==============================] - 0s 153us/step - loss: 0.0191 - mae: 0.1078 - val_loss: 0.6091 - val_mae: 0.7212\n",
            "Epoch 191/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0052 - mae: 0.0521 - val_loss: 0.3025 - val_mae: 0.4923\n",
            "Epoch 192/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0023 - mae: 0.0355 - val_loss: 0.2799 - val_mae: 0.4745\n",
            "Epoch 193/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0022 - mae: 0.0355 - val_loss: 0.4724 - val_mae: 0.6503\n",
            "Epoch 194/700\n",
            "675/675 [==============================] - 0s 144us/step - loss: 0.0015 - mae: 0.0287 - val_loss: 0.1603 - val_mae: 0.3406\n",
            "Epoch 195/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 6.3079e-04 - mae: 0.0194 - val_loss: 0.1586 - val_mae: 0.3427\n",
            "Epoch 196/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 7.4308e-04 - mae: 0.0209 - val_loss: 0.1200 - val_mae: 0.2883\n",
            "Epoch 197/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 8.8366e-04 - mae: 0.0234 - val_loss: 0.1251 - val_mae: 0.3039\n",
            "Epoch 198/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 5.7294e-04 - mae: 0.0185 - val_loss: 0.0883 - val_mae: 0.2360\n",
            "Epoch 199/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0036 - mae: 0.0437 - val_loss: 0.0836 - val_mae: 0.2233\n",
            "Epoch 200/700\n",
            "675/675 [==============================] - 0s 127us/step - loss: 0.0042 - mae: 0.0449 - val_loss: 0.2265 - val_mae: 0.4196\n",
            "Epoch 201/700\n",
            "675/675 [==============================] - 0s 145us/step - loss: 0.0011 - mae: 0.0251 - val_loss: 0.1589 - val_mae: 0.3441\n",
            "Epoch 202/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0014 - mae: 0.0302 - val_loss: 0.1184 - val_mae: 0.2847\n",
            "Epoch 203/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 8.0153e-04 - mae: 0.0209 - val_loss: 0.0724 - val_mae: 0.2066\n",
            "Epoch 204/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 0.0019 - mae: 0.0345 - val_loss: 0.0603 - val_mae: 0.1872\n",
            "Epoch 205/700\n",
            "675/675 [==============================] - 0s 152us/step - loss: 0.0032 - mae: 0.0447 - val_loss: 0.0962 - val_mae: 0.2570\n",
            "Epoch 206/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 0.0021 - mae: 0.0349 - val_loss: 0.0589 - val_mae: 0.1851\n",
            "Epoch 207/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0022 - mae: 0.0343 - val_loss: 0.0961 - val_mae: 0.2447\n",
            "Epoch 208/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 9.2143e-04 - mae: 0.0238 - val_loss: 0.1515 - val_mae: 0.3423\n",
            "Epoch 209/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 6.4236e-04 - mae: 0.0191 - val_loss: 0.1332 - val_mae: 0.3194\n",
            "Epoch 210/700\n",
            "675/675 [==============================] - 0s 146us/step - loss: 0.0011 - mae: 0.0256 - val_loss: 0.1026 - val_mae: 0.2682\n",
            "Epoch 211/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 8.3729e-04 - mae: 0.0221 - val_loss: 0.0935 - val_mae: 0.2539\n",
            "Epoch 212/700\n",
            "675/675 [==============================] - 0s 151us/step - loss: 0.0033 - mae: 0.0394 - val_loss: 0.0394 - val_mae: 0.1744\n",
            "Epoch 213/700\n",
            "675/675 [==============================] - 0s 152us/step - loss: 0.0051 - mae: 0.0550 - val_loss: 0.1454 - val_mae: 0.3193\n",
            "Epoch 214/700\n",
            "675/675 [==============================] - 0s 146us/step - loss: 0.0038 - mae: 0.0465 - val_loss: 0.1888 - val_mae: 0.3872\n",
            "Epoch 215/700\n",
            "675/675 [==============================] - 0s 150us/step - loss: 0.0089 - mae: 0.0719 - val_loss: 0.6910 - val_mae: 0.7890\n",
            "Epoch 216/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0051 - mae: 0.0546 - val_loss: 0.1976 - val_mae: 0.3845\n",
            "Epoch 217/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0025 - mae: 0.0374 - val_loss: 0.2356 - val_mae: 0.4343\n",
            "Epoch 218/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0169 - mae: 0.0889 - val_loss: 0.2768 - val_mae: 0.4170\n",
            "Epoch 219/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0323 - mae: 0.1352 - val_loss: 0.6789 - val_mae: 0.7565\n",
            "Epoch 220/700\n",
            "675/675 [==============================] - 0s 151us/step - loss: 0.0031 - mae: 0.0429 - val_loss: 0.2216 - val_mae: 0.3996\n",
            "Epoch 221/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 0.0010 - mae: 0.0252 - val_loss: 0.2083 - val_mae: 0.3949\n",
            "Epoch 222/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 8.8650e-04 - mae: 0.0219 - val_loss: 0.2017 - val_mae: 0.3946\n",
            "Epoch 223/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 9.6395e-04 - mae: 0.0241 - val_loss: 0.3197 - val_mae: 0.5216\n",
            "Epoch 224/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 0.0011 - mae: 0.0245 - val_loss: 0.1412 - val_mae: 0.3176\n",
            "Epoch 225/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 9.5747e-04 - mae: 0.0248 - val_loss: 0.1316 - val_mae: 0.3102\n",
            "Epoch 226/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 4.3308e-04 - mae: 0.0159 - val_loss: 0.1751 - val_mae: 0.3753\n",
            "Epoch 227/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0046 - mae: 0.0461 - val_loss: 0.4722 - val_mae: 0.6484\n",
            "Epoch 228/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0041 - mae: 0.0492 - val_loss: 0.2906 - val_mae: 0.4855\n",
            "Epoch 229/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0011 - mae: 0.0246 - val_loss: 0.1511 - val_mae: 0.3258\n",
            "Epoch 230/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 9.6094e-04 - mae: 0.0227 - val_loss: 0.2181 - val_mae: 0.4179\n",
            "Epoch 231/700\n",
            "675/675 [==============================] - 0s 148us/step - loss: 5.6956e-04 - mae: 0.0183 - val_loss: 0.0854 - val_mae: 0.2261\n",
            "Epoch 232/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 6.3969e-04 - mae: 0.0191 - val_loss: 0.1224 - val_mae: 0.2949\n",
            "Epoch 233/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 4.1672e-04 - mae: 0.0159 - val_loss: 0.1478 - val_mae: 0.3354\n",
            "Epoch 234/700\n",
            "675/675 [==============================] - 0s 147us/step - loss: 5.4878e-04 - mae: 0.0179 - val_loss: 0.1568 - val_mae: 0.3500\n",
            "Epoch 235/700\n",
            "675/675 [==============================] - 0s 161us/step - loss: 0.0035 - mae: 0.0432 - val_loss: 0.1502 - val_mae: 0.3323\n",
            "Epoch 236/700\n",
            "675/675 [==============================] - 0s 194us/step - loss: 9.2654e-04 - mae: 0.0240 - val_loss: 0.1164 - val_mae: 0.2850\n",
            "Epoch 237/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 4.0980e-04 - mae: 0.0161 - val_loss: 0.1144 - val_mae: 0.2846\n",
            "Epoch 238/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 9.0959e-04 - mae: 0.0236 - val_loss: 0.1253 - val_mae: 0.3035\n",
            "Epoch 239/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 0.0020 - mae: 0.0312 - val_loss: 0.0668 - val_mae: 0.1985\n",
            "Epoch 240/700\n",
            "675/675 [==============================] - 0s 151us/step - loss: 0.0044 - mae: 0.0474 - val_loss: 0.1766 - val_mae: 0.3593\n",
            "Epoch 241/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0014 - mae: 0.0290 - val_loss: 0.2217 - val_mae: 0.4222\n",
            "Epoch 242/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 0.0030 - mae: 0.0410 - val_loss: 0.1516 - val_mae: 0.3182\n",
            "Epoch 243/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0061 - mae: 0.0539 - val_loss: 0.3187 - val_mae: 0.5181\n",
            "Epoch 244/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 0.0147 - mae: 0.0908 - val_loss: 0.2214 - val_mae: 0.3745\n",
            "Epoch 245/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0039 - mae: 0.0449 - val_loss: 0.2949 - val_mae: 0.4879\n",
            "Epoch 246/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 0.0015 - mae: 0.0296 - val_loss: 0.1633 - val_mae: 0.3386\n",
            "Epoch 247/700\n",
            "675/675 [==============================] - 0s 150us/step - loss: 5.1410e-04 - mae: 0.0172 - val_loss: 0.1879 - val_mae: 0.3795\n",
            "Epoch 248/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 5.4181e-04 - mae: 0.0183 - val_loss: 0.1713 - val_mae: 0.3642\n",
            "Epoch 249/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 7.3972e-04 - mae: 0.0210 - val_loss: 0.1097 - val_mae: 0.2698\n",
            "Epoch 250/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 8.1054e-04 - mae: 0.0211 - val_loss: 0.0826 - val_mae: 0.2210\n",
            "Epoch 251/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0031 - mae: 0.0384 - val_loss: 0.1948 - val_mae: 0.3886\n",
            "Epoch 252/700\n",
            "675/675 [==============================] - 0s 156us/step - loss: 0.0014 - mae: 0.0294 - val_loss: 0.1097 - val_mae: 0.2622\n",
            "Epoch 253/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 6.4270e-04 - mae: 0.0178 - val_loss: 0.1598 - val_mae: 0.3457\n",
            "Epoch 254/700\n",
            "675/675 [==============================] - 0s 150us/step - loss: 5.3583e-04 - mae: 0.0175 - val_loss: 0.1407 - val_mae: 0.3208\n",
            "Epoch 255/700\n",
            "675/675 [==============================] - 0s 181us/step - loss: 3.8459e-04 - mae: 0.0153 - val_loss: 0.1433 - val_mae: 0.3291\n",
            "Epoch 256/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 5.2427e-04 - mae: 0.0178 - val_loss: 0.1234 - val_mae: 0.3021\n",
            "Epoch 257/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 4.1091e-04 - mae: 0.0165 - val_loss: 0.1324 - val_mae: 0.3166\n",
            "Epoch 258/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 6.8858e-04 - mae: 0.0203 - val_loss: 0.1912 - val_mae: 0.3956\n",
            "Epoch 259/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 0.0017 - mae: 0.0316 - val_loss: 0.1307 - val_mae: 0.3102\n",
            "Epoch 260/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 0.0092 - mae: 0.0561 - val_loss: 0.2734 - val_mae: 0.4579\n",
            "Epoch 261/700\n",
            "675/675 [==============================] - 0s 147us/step - loss: 0.0128 - mae: 0.0833 - val_loss: 0.2591 - val_mae: 0.4513\n",
            "Epoch 262/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0012 - mae: 0.0276 - val_loss: 0.2289 - val_mae: 0.4271\n",
            "Epoch 263/700\n",
            "675/675 [==============================] - 0s 148us/step - loss: 4.1940e-04 - mae: 0.0162 - val_loss: 0.1564 - val_mae: 0.3385\n",
            "Epoch 264/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 5.5864e-04 - mae: 0.0190 - val_loss: 0.1410 - val_mae: 0.3215\n",
            "Epoch 265/700\n",
            "675/675 [==============================] - 0s 151us/step - loss: 0.0017 - mae: 0.0324 - val_loss: 0.2010 - val_mae: 0.3951\n",
            "Epoch 266/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 0.0011 - mae: 0.0253 - val_loss: 0.1420 - val_mae: 0.3136\n",
            "Epoch 267/700\n",
            "675/675 [==============================] - 0s 150us/step - loss: 0.0026 - mae: 0.0395 - val_loss: 0.0559 - val_mae: 0.1865\n",
            "Epoch 268/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 0.0072 - mae: 0.0637 - val_loss: 0.1911 - val_mae: 0.3617\n",
            "Epoch 269/700\n",
            "675/675 [==============================] - 0s 153us/step - loss: 0.0046 - mae: 0.0541 - val_loss: 0.1845 - val_mae: 0.3663\n",
            "Epoch 270/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 7.3445e-04 - mae: 0.0210 - val_loss: 0.1692 - val_mae: 0.3531\n",
            "Epoch 271/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 0.0012 - mae: 0.0270 - val_loss: 0.1835 - val_mae: 0.3726\n",
            "Epoch 272/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 6.5721e-04 - mae: 0.0203 - val_loss: 0.1103 - val_mae: 0.2680\n",
            "Epoch 273/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 6.4669e-04 - mae: 0.0195 - val_loss: 0.1550 - val_mae: 0.3416\n",
            "Epoch 274/700\n",
            "675/675 [==============================] - 0s 147us/step - loss: 0.0046 - mae: 0.0454 - val_loss: 0.1080 - val_mae: 0.2578\n",
            "Epoch 275/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0032 - mae: 0.0407 - val_loss: 0.1685 - val_mae: 0.3576\n",
            "Epoch 276/700\n",
            "675/675 [==============================] - 0s 145us/step - loss: 9.0058e-04 - mae: 0.0232 - val_loss: 0.1964 - val_mae: 0.4038\n",
            "Epoch 277/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 0.0041 - mae: 0.0516 - val_loss: 0.1278 - val_mae: 0.3007\n",
            "Epoch 278/700\n",
            "675/675 [==============================] - 0s 199us/step - loss: 0.0015 - mae: 0.0301 - val_loss: 0.1260 - val_mae: 0.3033\n",
            "Epoch 279/700\n",
            "675/675 [==============================] - 0s 156us/step - loss: 4.4081e-04 - mae: 0.0168 - val_loss: 0.1194 - val_mae: 0.2920\n",
            "Epoch 280/700\n",
            "675/675 [==============================] - 0s 150us/step - loss: 0.0011 - mae: 0.0261 - val_loss: 0.0561 - val_mae: 0.1802\n",
            "Epoch 281/700\n",
            "675/675 [==============================] - 0s 143us/step - loss: 0.0014 - mae: 0.0289 - val_loss: 0.0437 - val_mae: 0.1632\n",
            "Epoch 282/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0034 - mae: 0.0411 - val_loss: 0.2241 - val_mae: 0.4213\n",
            "Epoch 283/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0026 - mae: 0.0357 - val_loss: 0.3322 - val_mae: 0.5268\n",
            "Epoch 284/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0011 - mae: 0.0261 - val_loss: 0.1590 - val_mae: 0.3481\n",
            "Epoch 285/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0057 - mae: 0.0495 - val_loss: 0.3607 - val_mae: 0.5465\n",
            "Epoch 286/700\n",
            "675/675 [==============================] - 0s 154us/step - loss: 0.0045 - mae: 0.0535 - val_loss: 0.1511 - val_mae: 0.3101\n",
            "Epoch 287/700\n",
            "675/675 [==============================] - 0s 148us/step - loss: 0.0024 - mae: 0.0355 - val_loss: 0.1761 - val_mae: 0.3526\n",
            "Epoch 288/700\n",
            "675/675 [==============================] - 0s 154us/step - loss: 0.0014 - mae: 0.0289 - val_loss: 0.1452 - val_mae: 0.3248\n",
            "Epoch 289/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 9.5542e-04 - mae: 0.0236 - val_loss: 0.1938 - val_mae: 0.3939\n",
            "Epoch 290/700\n",
            "675/675 [==============================] - 0s 182us/step - loss: 9.4646e-04 - mae: 0.0235 - val_loss: 0.0587 - val_mae: 0.1852\n",
            "Epoch 291/700\n",
            "675/675 [==============================] - 0s 146us/step - loss: 0.0011 - mae: 0.0242 - val_loss: 0.1636 - val_mae: 0.3562\n",
            "Epoch 292/700\n",
            "675/675 [==============================] - 0s 147us/step - loss: 0.0012 - mae: 0.0269 - val_loss: 0.1130 - val_mae: 0.2749\n",
            "Epoch 293/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 0.0017 - mae: 0.0333 - val_loss: 0.2479 - val_mae: 0.4553\n",
            "Epoch 294/700\n",
            "675/675 [==============================] - 0s 162us/step - loss: 0.0257 - mae: 0.1239 - val_loss: 0.2295 - val_mae: 0.3820\n",
            "Epoch 295/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 0.0081 - mae: 0.0648 - val_loss: 0.2379 - val_mae: 0.4228\n",
            "Epoch 296/700\n",
            "675/675 [==============================] - 0s 150us/step - loss: 0.0012 - mae: 0.0261 - val_loss: 0.1815 - val_mae: 0.3616\n",
            "Epoch 297/700\n",
            "675/675 [==============================] - 0s 157us/step - loss: 7.9990e-04 - mae: 0.0223 - val_loss: 0.1751 - val_mae: 0.3601\n",
            "Epoch 298/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 6.9431e-04 - mae: 0.0204 - val_loss: 0.1620 - val_mae: 0.3478\n",
            "Epoch 299/700\n",
            "675/675 [==============================] - 0s 154us/step - loss: 6.0449e-04 - mae: 0.0192 - val_loss: 0.1932 - val_mae: 0.3940\n",
            "Epoch 300/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 7.0598e-04 - mae: 0.0198 - val_loss: 0.1135 - val_mae: 0.2786\n",
            "Epoch 301/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 5.0642e-04 - mae: 0.0175 - val_loss: 0.0860 - val_mae: 0.2290\n",
            "Epoch 302/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 8.4742e-04 - mae: 0.0225 - val_loss: 0.1947 - val_mae: 0.3979\n",
            "Epoch 303/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 7.7757e-04 - mae: 0.0199 - val_loss: 0.1190 - val_mae: 0.2909\n",
            "Epoch 304/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 6.9224e-04 - mae: 0.0212 - val_loss: 0.1662 - val_mae: 0.3631\n",
            "Epoch 305/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 7.2710e-04 - mae: 0.0211 - val_loss: 0.1122 - val_mae: 0.2820\n",
            "Epoch 306/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 8.6966e-04 - mae: 0.0230 - val_loss: 0.0706 - val_mae: 0.2030\n",
            "Epoch 307/700\n",
            "675/675 [==============================] - 0s 148us/step - loss: 0.0043 - mae: 0.0519 - val_loss: 0.3826 - val_mae: 0.5802\n",
            "Epoch 308/700\n",
            "675/675 [==============================] - 0s 155us/step - loss: 0.0095 - mae: 0.0760 - val_loss: 0.3049 - val_mae: 0.4976\n",
            "Epoch 309/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0016 - mae: 0.0282 - val_loss: 0.2557 - val_mae: 0.4611\n",
            "Epoch 310/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0011 - mae: 0.0245 - val_loss: 0.1335 - val_mae: 0.3041\n",
            "Epoch 311/700\n",
            "675/675 [==============================] - 0s 149us/step - loss: 0.0015 - mae: 0.0290 - val_loss: 0.2326 - val_mae: 0.4313\n",
            "Epoch 312/700\n",
            "675/675 [==============================] - 0s 151us/step - loss: 0.0013 - mae: 0.0279 - val_loss: 0.1375 - val_mae: 0.3155\n",
            "Epoch 313/700\n",
            "675/675 [==============================] - 0s 146us/step - loss: 6.5525e-04 - mae: 0.0190 - val_loss: 0.1147 - val_mae: 0.2808\n",
            "Epoch 314/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 0.0016 - mae: 0.0314 - val_loss: 0.1100 - val_mae: 0.2641\n",
            "Epoch 315/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 7.3708e-04 - mae: 0.0218 - val_loss: 0.0824 - val_mae: 0.2219\n",
            "Epoch 316/700\n",
            "675/675 [==============================] - 0s 146us/step - loss: 9.2282e-04 - mae: 0.0239 - val_loss: 0.1711 - val_mae: 0.3642\n",
            "Epoch 317/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 8.5611e-04 - mae: 0.0209 - val_loss: 0.0915 - val_mae: 0.2366\n",
            "Epoch 318/700\n",
            "675/675 [==============================] - 0s 160us/step - loss: 7.3114e-04 - mae: 0.0211 - val_loss: 0.1581 - val_mae: 0.3505\n",
            "Epoch 319/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 8.4735e-04 - mae: 0.0233 - val_loss: 0.0990 - val_mae: 0.2573\n",
            "Epoch 320/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 7.7466e-04 - mae: 0.0217 - val_loss: 0.0735 - val_mae: 0.2093\n",
            "Epoch 321/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0021 - mae: 0.0356 - val_loss: 0.1177 - val_mae: 0.2889\n",
            "Epoch 322/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 6.3586e-04 - mae: 0.0190 - val_loss: 0.1188 - val_mae: 0.2955\n",
            "Epoch 323/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 0.0024 - mae: 0.0337 - val_loss: 0.0769 - val_mae: 0.2157\n",
            "Epoch 324/700\n",
            "675/675 [==============================] - 0s 149us/step - loss: 0.0011 - mae: 0.0263 - val_loss: 0.1217 - val_mae: 0.3009\n",
            "Epoch 325/700\n",
            "675/675 [==============================] - 0s 155us/step - loss: 8.1533e-04 - mae: 0.0225 - val_loss: 0.0966 - val_mae: 0.2579\n",
            "Epoch 326/700\n",
            "675/675 [==============================] - 0s 164us/step - loss: 7.5245e-04 - mae: 0.0224 - val_loss: 0.0588 - val_mae: 0.1838\n",
            "Epoch 327/700\n",
            "675/675 [==============================] - 0s 156us/step - loss: 6.5319e-04 - mae: 0.0200 - val_loss: 0.0857 - val_mae: 0.2376\n",
            "Epoch 328/700\n",
            "675/675 [==============================] - 0s 197us/step - loss: 0.0015 - mae: 0.0300 - val_loss: 0.0469 - val_mae: 0.1634\n",
            "Epoch 329/700\n",
            "675/675 [==============================] - 0s 150us/step - loss: 0.0038 - mae: 0.0446 - val_loss: 0.1297 - val_mae: 0.3127\n",
            "Epoch 330/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 5.8857e-04 - mae: 0.0198 - val_loss: 0.1062 - val_mae: 0.2749\n",
            "Epoch 331/700\n",
            "675/675 [==============================] - 0s 145us/step - loss: 9.5736e-04 - mae: 0.0247 - val_loss: 0.1495 - val_mae: 0.3406\n",
            "Epoch 332/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0022 - mae: 0.0369 - val_loss: 0.1363 - val_mae: 0.3201\n",
            "Epoch 333/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0014 - mae: 0.0289 - val_loss: 0.0799 - val_mae: 0.2283\n",
            "Epoch 334/700\n",
            "675/675 [==============================] - 0s 145us/step - loss: 4.2150e-04 - mae: 0.0163 - val_loss: 0.1139 - val_mae: 0.2923\n",
            "Epoch 335/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0016 - mae: 0.0302 - val_loss: 0.1124 - val_mae: 0.2882\n",
            "Epoch 336/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 7.9792e-04 - mae: 0.0212 - val_loss: 0.1143 - val_mae: 0.2905\n",
            "Epoch 337/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 4.8812e-04 - mae: 0.0176 - val_loss: 0.1007 - val_mae: 0.2677\n",
            "Epoch 338/700\n",
            "675/675 [==============================] - 0s 145us/step - loss: 8.9325e-04 - mae: 0.0232 - val_loss: 0.1101 - val_mae: 0.2893\n",
            "Epoch 339/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0017 - mae: 0.0335 - val_loss: 0.0492 - val_mae: 0.1671\n",
            "Epoch 340/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 0.0022 - mae: 0.0358 - val_loss: 0.3295 - val_mae: 0.5409\n",
            "Epoch 341/700\n",
            "675/675 [==============================] - 0s 149us/step - loss: 0.0045 - mae: 0.0458 - val_loss: 1.6126 - val_mae: 1.2405\n",
            "Epoch 342/700\n",
            "675/675 [==============================] - 0s 148us/step - loss: 0.0064 - mae: 0.0584 - val_loss: 0.1296 - val_mae: 0.3001\n",
            "Epoch 343/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0048 - mae: 0.0482 - val_loss: 0.5416 - val_mae: 0.6927\n",
            "Epoch 344/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 0.0286 - mae: 0.1280 - val_loss: 0.6897 - val_mae: 0.7601\n",
            "Epoch 345/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0084 - mae: 0.0702 - val_loss: 0.4219 - val_mae: 0.5925\n",
            "Epoch 346/700\n",
            "675/675 [==============================] - 0s 144us/step - loss: 0.0012 - mae: 0.0267 - val_loss: 0.2260 - val_mae: 0.4171\n",
            "Epoch 347/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 8.1932e-04 - mae: 0.0216 - val_loss: 0.2555 - val_mae: 0.4582\n",
            "Epoch 348/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 7.6306e-04 - mae: 0.0207 - val_loss: 0.1740 - val_mae: 0.3637\n",
            "Epoch 349/700\n",
            "675/675 [==============================] - 0s 147us/step - loss: 5.9195e-04 - mae: 0.0191 - val_loss: 0.1145 - val_mae: 0.2765\n",
            "Epoch 350/700\n",
            "675/675 [==============================] - 0s 148us/step - loss: 5.8025e-04 - mae: 0.0185 - val_loss: 0.1709 - val_mae: 0.3653\n",
            "Epoch 351/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 5.5939e-04 - mae: 0.0188 - val_loss: 0.1142 - val_mae: 0.2830\n",
            "Epoch 352/700\n",
            "675/675 [==============================] - 0s 149us/step - loss: 6.2388e-04 - mae: 0.0200 - val_loss: 0.1205 - val_mae: 0.2964\n",
            "Epoch 353/700\n",
            "675/675 [==============================] - 0s 143us/step - loss: 4.4616e-04 - mae: 0.0160 - val_loss: 0.1341 - val_mae: 0.3206\n",
            "Epoch 354/700\n",
            "675/675 [==============================] - 0s 145us/step - loss: 5.5376e-04 - mae: 0.0186 - val_loss: 0.0792 - val_mae: 0.2219\n",
            "Epoch 355/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 5.4131e-04 - mae: 0.0185 - val_loss: 0.0873 - val_mae: 0.2384\n",
            "Epoch 356/700\n",
            "675/675 [==============================] - 0s 149us/step - loss: 6.7449e-04 - mae: 0.0208 - val_loss: 0.1024 - val_mae: 0.2699\n",
            "Epoch 357/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 5.8734e-04 - mae: 0.0187 - val_loss: 0.1150 - val_mae: 0.2901\n",
            "Epoch 358/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 0.0012 - mae: 0.0263 - val_loss: 0.0564 - val_mae: 0.1800\n",
            "Epoch 359/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0020 - mae: 0.0344 - val_loss: 0.1008 - val_mae: 0.2586\n",
            "Epoch 360/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 0.0010 - mae: 0.0253 - val_loss: 0.2087 - val_mae: 0.4196\n",
            "Epoch 361/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 5.1419e-04 - mae: 0.0168 - val_loss: 0.1000 - val_mae: 0.2621\n",
            "Epoch 362/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 4.2057e-04 - mae: 0.0164 - val_loss: 0.1358 - val_mae: 0.3263\n",
            "Epoch 363/700\n",
            "675/675 [==============================] - 0s 146us/step - loss: 4.6488e-04 - mae: 0.0167 - val_loss: 0.1028 - val_mae: 0.2733\n",
            "Epoch 364/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 9.4353e-04 - mae: 0.0231 - val_loss: 0.1116 - val_mae: 0.2868\n",
            "Epoch 365/700\n",
            "675/675 [==============================] - 0s 166us/step - loss: 0.0017 - mae: 0.0328 - val_loss: 0.1571 - val_mae: 0.3571\n",
            "Epoch 366/700\n",
            "675/675 [==============================] - 0s 156us/step - loss: 8.6273e-04 - mae: 0.0235 - val_loss: 0.1039 - val_mae: 0.2672\n",
            "Epoch 367/700\n",
            "675/675 [==============================] - 0s 154us/step - loss: 0.0014 - mae: 0.0287 - val_loss: 0.2009 - val_mae: 0.4118\n",
            "Epoch 368/700\n",
            "675/675 [==============================] - 0s 162us/step - loss: 0.0063 - mae: 0.0609 - val_loss: 0.1515 - val_mae: 0.3423\n",
            "Epoch 369/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0024 - mae: 0.0358 - val_loss: 0.0800 - val_mae: 0.2198\n",
            "Epoch 370/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0013 - mae: 0.0276 - val_loss: 0.1084 - val_mae: 0.2717\n",
            "Epoch 371/700\n",
            "675/675 [==============================] - 0s 161us/step - loss: 0.0012 - mae: 0.0281 - val_loss: 0.1197 - val_mae: 0.3006\n",
            "Epoch 372/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 7.2514e-04 - mae: 0.0216 - val_loss: 0.0736 - val_mae: 0.2146\n",
            "Epoch 373/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 7.0557e-04 - mae: 0.0210 - val_loss: 0.0866 - val_mae: 0.2440\n",
            "Epoch 374/700\n",
            "675/675 [==============================] - 0s 148us/step - loss: 3.3439e-04 - mae: 0.0142 - val_loss: 0.0847 - val_mae: 0.2457\n",
            "Epoch 375/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 3.2638e-04 - mae: 0.0145 - val_loss: 0.0544 - val_mae: 0.1789\n",
            "Epoch 376/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 3.9751e-04 - mae: 0.0157 - val_loss: 0.0773 - val_mae: 0.2284\n",
            "Epoch 377/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 8.2631e-04 - mae: 0.0227 - val_loss: 0.0441 - val_mae: 0.1585\n",
            "Epoch 378/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0013 - mae: 0.0256 - val_loss: 0.0791 - val_mae: 0.2314\n",
            "Epoch 379/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 0.0028 - mae: 0.0387 - val_loss: 0.4443 - val_mae: 0.6368\n",
            "Epoch 380/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0033 - mae: 0.0415 - val_loss: 0.1036 - val_mae: 0.2640\n",
            "Epoch 381/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 0.0023 - mae: 0.0384 - val_loss: 0.2506 - val_mae: 0.4646\n",
            "Epoch 382/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0026 - mae: 0.0368 - val_loss: 0.1452 - val_mae: 0.3362\n",
            "Epoch 383/700\n",
            "675/675 [==============================] - 0s 175us/step - loss: 5.9223e-04 - mae: 0.0182 - val_loss: 0.0790 - val_mae: 0.2270\n",
            "Epoch 384/700\n",
            "675/675 [==============================] - 0s 174us/step - loss: 0.0020 - mae: 0.0365 - val_loss: 0.0500 - val_mae: 0.1692\n",
            "Epoch 385/700\n",
            "675/675 [==============================] - 0s 143us/step - loss: 0.0025 - mae: 0.0397 - val_loss: 0.0717 - val_mae: 0.2067\n",
            "Epoch 386/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0030 - mae: 0.0418 - val_loss: 0.1052 - val_mae: 0.2673\n",
            "Epoch 387/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 0.0013 - mae: 0.0278 - val_loss: 0.2434 - val_mae: 0.4521\n",
            "Epoch 388/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0066 - mae: 0.0598 - val_loss: 0.5326 - val_mae: 0.6924\n",
            "Epoch 389/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0037 - mae: 0.0419 - val_loss: 0.2160 - val_mae: 0.4130\n",
            "Epoch 390/700\n",
            "675/675 [==============================] - 0s 145us/step - loss: 4.5831e-04 - mae: 0.0169 - val_loss: 0.1276 - val_mae: 0.3021\n",
            "Epoch 391/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 4.3887e-04 - mae: 0.0164 - val_loss: 0.1335 - val_mae: 0.3152\n",
            "Epoch 392/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 6.2137e-04 - mae: 0.0191 - val_loss: 0.1691 - val_mae: 0.3701\n",
            "Epoch 393/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 3.8890e-04 - mae: 0.0157 - val_loss: 0.1112 - val_mae: 0.2849\n",
            "Epoch 394/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 6.6844e-04 - mae: 0.0204 - val_loss: 0.0851 - val_mae: 0.2390\n",
            "Epoch 395/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 4.2302e-04 - mae: 0.0165 - val_loss: 0.0903 - val_mae: 0.2506\n",
            "Epoch 396/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 6.4373e-04 - mae: 0.0195 - val_loss: 0.0961 - val_mae: 0.2632\n",
            "Epoch 397/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0046 - mae: 0.0383 - val_loss: 1.5131 - val_mae: 1.1898\n",
            "Epoch 398/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 0.0119 - mae: 0.0887 - val_loss: 0.3814 - val_mae: 0.5763\n",
            "Epoch 399/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0018 - mae: 0.0307 - val_loss: 0.0888 - val_mae: 0.2353\n",
            "Epoch 400/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 0.0013 - mae: 0.0264 - val_loss: 0.0537 - val_mae: 0.1791\n",
            "Epoch 401/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 8.1290e-04 - mae: 0.0208 - val_loss: 0.1289 - val_mae: 0.3131\n",
            "Epoch 402/700\n",
            "675/675 [==============================] - 0s 183us/step - loss: 0.0010 - mae: 0.0239 - val_loss: 0.0974 - val_mae: 0.2633\n",
            "Epoch 403/700\n",
            "675/675 [==============================] - 0s 144us/step - loss: 2.7072e-04 - mae: 0.0129 - val_loss: 0.0715 - val_mae: 0.2125\n",
            "Epoch 404/700\n",
            "675/675 [==============================] - 0s 143us/step - loss: 4.4682e-04 - mae: 0.0165 - val_loss: 0.0642 - val_mae: 0.1992\n",
            "Epoch 405/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 5.1350e-04 - mae: 0.0179 - val_loss: 0.0800 - val_mae: 0.2340\n",
            "Epoch 406/700\n",
            "675/675 [==============================] - 0s 166us/step - loss: 3.5375e-04 - mae: 0.0152 - val_loss: 0.0520 - val_mae: 0.1742\n",
            "Epoch 407/700\n",
            "675/675 [==============================] - 0s 164us/step - loss: 4.6400e-04 - mae: 0.0171 - val_loss: 0.1087 - val_mae: 0.2882\n",
            "Epoch 408/700\n",
            "675/675 [==============================] - 0s 179us/step - loss: 5.8244e-04 - mae: 0.0193 - val_loss: 0.0851 - val_mae: 0.2465\n",
            "Epoch 409/700\n",
            "675/675 [==============================] - 0s 162us/step - loss: 9.4609e-04 - mae: 0.0240 - val_loss: 0.0499 - val_mae: 0.1707\n",
            "Epoch 410/700\n",
            "675/675 [==============================] - 0s 178us/step - loss: 0.0012 - mae: 0.0264 - val_loss: 0.0744 - val_mae: 0.2217\n",
            "Epoch 411/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 4.8235e-04 - mae: 0.0174 - val_loss: 0.1097 - val_mae: 0.2912\n",
            "Epoch 412/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 7.9739e-04 - mae: 0.0219 - val_loss: 0.0575 - val_mae: 0.1857\n",
            "Epoch 413/700\n",
            "675/675 [==============================] - 0s 150us/step - loss: 0.0012 - mae: 0.0249 - val_loss: 0.2802 - val_mae: 0.5022\n",
            "Epoch 414/700\n",
            "675/675 [==============================] - 0s 145us/step - loss: 0.0186 - mae: 0.1022 - val_loss: 0.3983 - val_mae: 0.5757\n",
            "Epoch 415/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0016 - mae: 0.0313 - val_loss: 0.1247 - val_mae: 0.2969\n",
            "Epoch 416/700\n",
            "675/675 [==============================] - 0s 148us/step - loss: 6.1944e-04 - mae: 0.0189 - val_loss: 0.1184 - val_mae: 0.2926\n",
            "Epoch 417/700\n",
            "675/675 [==============================] - 0s 146us/step - loss: 3.4224e-04 - mae: 0.0145 - val_loss: 0.1065 - val_mae: 0.2774\n",
            "Epoch 418/700\n",
            "675/675 [==============================] - 0s 157us/step - loss: 8.0769e-04 - mae: 0.0205 - val_loss: 0.2467 - val_mae: 0.4638\n",
            "Epoch 419/700\n",
            "675/675 [==============================] - 0s 175us/step - loss: 0.0010 - mae: 0.0249 - val_loss: 0.1242 - val_mae: 0.3092\n",
            "Epoch 420/700\n",
            "675/675 [==============================] - 0s 161us/step - loss: 8.2530e-04 - mae: 0.0229 - val_loss: 0.0730 - val_mae: 0.2172\n",
            "Epoch 421/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 8.8448e-04 - mae: 0.0224 - val_loss: 0.0984 - val_mae: 0.2704\n",
            "Epoch 422/700\n",
            "675/675 [==============================] - 0s 152us/step - loss: 6.9998e-04 - mae: 0.0212 - val_loss: 0.0777 - val_mae: 0.2328\n",
            "Epoch 423/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 5.1976e-04 - mae: 0.0179 - val_loss: 0.0861 - val_mae: 0.2534\n",
            "Epoch 424/700\n",
            "675/675 [==============================] - 0s 146us/step - loss: 7.5822e-04 - mae: 0.0219 - val_loss: 0.0645 - val_mae: 0.2079\n",
            "Epoch 425/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 8.9216e-04 - mae: 0.0246 - val_loss: 0.0779 - val_mae: 0.2376\n",
            "Epoch 426/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 6.1618e-04 - mae: 0.0191 - val_loss: 0.0707 - val_mae: 0.2226\n",
            "Epoch 427/700\n",
            "675/675 [==============================] - 0s 128us/step - loss: 5.4084e-04 - mae: 0.0186 - val_loss: 0.0787 - val_mae: 0.2384\n",
            "Epoch 428/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 5.7107e-04 - mae: 0.0192 - val_loss: 0.0535 - val_mae: 0.1839\n",
            "Epoch 429/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 4.3411e-04 - mae: 0.0164 - val_loss: 0.0495 - val_mae: 0.1749\n",
            "Epoch 430/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0014 - mae: 0.0290 - val_loss: 0.1033 - val_mae: 0.2789\n",
            "Epoch 431/700\n",
            "675/675 [==============================] - 0s 147us/step - loss: 0.0033 - mae: 0.0444 - val_loss: 0.0842 - val_mae: 0.2501\n",
            "Epoch 432/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 3.6743e-04 - mae: 0.0148 - val_loss: 0.0452 - val_mae: 0.1643\n",
            "Epoch 433/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 2.9591e-04 - mae: 0.0139 - val_loss: 0.0650 - val_mae: 0.2132\n",
            "Epoch 434/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0010 - mae: 0.0245 - val_loss: 0.0424 - val_mae: 0.1577\n",
            "Epoch 435/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0033 - mae: 0.0418 - val_loss: 0.1478 - val_mae: 0.3455\n",
            "Epoch 436/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0035 - mae: 0.0415 - val_loss: 0.1433 - val_mae: 0.3330\n",
            "Epoch 437/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0011 - mae: 0.0268 - val_loss: 0.0857 - val_mae: 0.2456\n",
            "Epoch 438/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0032 - mae: 0.0387 - val_loss: 0.3245 - val_mae: 0.5375\n",
            "Epoch 439/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0026 - mae: 0.0371 - val_loss: 0.1593 - val_mae: 0.3535\n",
            "Epoch 440/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 0.0036 - mae: 0.0447 - val_loss: 0.0708 - val_mae: 0.2082\n",
            "Epoch 441/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0010 - mae: 0.0237 - val_loss: 0.0646 - val_mae: 0.1999\n",
            "Epoch 442/700\n",
            "675/675 [==============================] - 0s 168us/step - loss: 5.7542e-04 - mae: 0.0185 - val_loss: 0.0585 - val_mae: 0.1899\n",
            "Epoch 443/700\n",
            "675/675 [==============================] - 0s 143us/step - loss: 9.3542e-04 - mae: 0.0229 - val_loss: 0.0888 - val_mae: 0.2543\n",
            "Epoch 444/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 3.9873e-04 - mae: 0.0155 - val_loss: 0.0798 - val_mae: 0.2427\n",
            "Epoch 445/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 2.9209e-04 - mae: 0.0134 - val_loss: 0.0686 - val_mae: 0.2209\n",
            "Epoch 446/700\n",
            "675/675 [==============================] - 0s 127us/step - loss: 7.6647e-04 - mae: 0.0212 - val_loss: 0.0785 - val_mae: 0.2415\n",
            "Epoch 447/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 8.7932e-04 - mae: 0.0220 - val_loss: 0.0733 - val_mae: 0.2317\n",
            "Epoch 448/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0016 - mae: 0.0316 - val_loss: 0.1584 - val_mae: 0.3692\n",
            "Epoch 449/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0025 - mae: 0.0349 - val_loss: 0.0745 - val_mae: 0.2328\n",
            "Epoch 450/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 4.0614e-04 - mae: 0.0157 - val_loss: 0.0665 - val_mae: 0.2195\n",
            "Epoch 451/700\n",
            "675/675 [==============================] - 0s 128us/step - loss: 0.0027 - mae: 0.0399 - val_loss: 0.0425 - val_mae: 0.1627\n",
            "Epoch 452/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 9.9680e-04 - mae: 0.0252 - val_loss: 0.0326 - val_mae: 0.1359\n",
            "Epoch 453/700\n",
            "675/675 [==============================] - 0s 143us/step - loss: 0.0016 - mae: 0.0308 - val_loss: 0.0553 - val_mae: 0.1980\n",
            "Epoch 454/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0013 - mae: 0.0286 - val_loss: 0.0534 - val_mae: 0.1926\n",
            "Epoch 455/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 4.5159e-04 - mae: 0.0163 - val_loss: 0.0396 - val_mae: 0.1560\n",
            "Epoch 456/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 4.9771e-04 - mae: 0.0161 - val_loss: 0.1056 - val_mae: 0.2960\n",
            "Epoch 457/700\n",
            "675/675 [==============================] - 0s 127us/step - loss: 5.5588e-04 - mae: 0.0187 - val_loss: 0.0468 - val_mae: 0.1756\n",
            "Epoch 458/700\n",
            "675/675 [==============================] - 0s 146us/step - loss: 3.1552e-04 - mae: 0.0146 - val_loss: 0.0418 - val_mae: 0.1631\n",
            "Epoch 459/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 6.5238e-04 - mae: 0.0191 - val_loss: 0.0511 - val_mae: 0.1876\n",
            "Epoch 460/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0011 - mae: 0.0260 - val_loss: 0.0357 - val_mae: 0.1451\n",
            "Epoch 461/700\n",
            "675/675 [==============================] - 0s 147us/step - loss: 0.0010 - mae: 0.0261 - val_loss: 0.0753 - val_mae: 0.2405\n",
            "Epoch 462/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 8.4142e-04 - mae: 0.0228 - val_loss: 0.0345 - val_mae: 0.1393\n",
            "Epoch 463/700\n",
            "675/675 [==============================] - 0s 153us/step - loss: 0.0012 - mae: 0.0279 - val_loss: 0.1037 - val_mae: 0.2911\n",
            "Epoch 464/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 8.4456e-04 - mae: 0.0232 - val_loss: 0.0293 - val_mae: 0.1276\n",
            "Epoch 465/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 0.0019 - mae: 0.0329 - val_loss: 0.0562 - val_mae: 0.1873\n",
            "Epoch 466/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 5.3644e-04 - mae: 0.0179 - val_loss: 0.0616 - val_mae: 0.2057\n",
            "Epoch 467/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 3.0939e-04 - mae: 0.0140 - val_loss: 0.0865 - val_mae: 0.2581\n",
            "Epoch 468/700\n",
            "675/675 [==============================] - 0s 149us/step - loss: 0.0167 - mae: 0.0827 - val_loss: 0.2020 - val_mae: 0.3816\n",
            "Epoch 469/700\n",
            "675/675 [==============================] - 0s 150us/step - loss: 0.0051 - mae: 0.0500 - val_loss: 0.2572 - val_mae: 0.4597\n",
            "Epoch 470/700\n",
            "675/675 [==============================] - 0s 154us/step - loss: 0.0012 - mae: 0.0273 - val_loss: 0.1312 - val_mae: 0.3136\n",
            "Epoch 471/700\n",
            "675/675 [==============================] - 0s 145us/step - loss: 5.9624e-04 - mae: 0.0186 - val_loss: 0.1012 - val_mae: 0.2693\n",
            "Epoch 472/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 5.6844e-04 - mae: 0.0190 - val_loss: 0.0635 - val_mae: 0.1992\n",
            "Epoch 473/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 5.3434e-04 - mae: 0.0186 - val_loss: 0.0890 - val_mae: 0.2553\n",
            "Epoch 474/700\n",
            "675/675 [==============================] - 0s 145us/step - loss: 3.3194e-04 - mae: 0.0144 - val_loss: 0.0812 - val_mae: 0.2420\n",
            "Epoch 475/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 7.0436e-04 - mae: 0.0202 - val_loss: 0.1137 - val_mae: 0.3018\n",
            "Epoch 476/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 5.2336e-04 - mae: 0.0170 - val_loss: 0.0535 - val_mae: 0.1855\n",
            "Epoch 477/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 6.1950e-04 - mae: 0.0199 - val_loss: 0.0489 - val_mae: 0.1767\n",
            "Epoch 478/700\n",
            "675/675 [==============================] - 0s 127us/step - loss: 5.6915e-04 - mae: 0.0182 - val_loss: 0.0250 - val_mae: 0.1193\n",
            "Epoch 479/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 0.0025 - mae: 0.0396 - val_loss: 0.1945 - val_mae: 0.4091\n",
            "Epoch 480/700\n",
            "675/675 [==============================] - 0s 143us/step - loss: 0.0016 - mae: 0.0285 - val_loss: 0.0545 - val_mae: 0.1893\n",
            "Epoch 481/700\n",
            "675/675 [==============================] - 0s 127us/step - loss: 4.3634e-04 - mae: 0.0163 - val_loss: 0.0497 - val_mae: 0.1796\n",
            "Epoch 482/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 8.7191e-04 - mae: 0.0227 - val_loss: 0.0811 - val_mae: 0.2483\n",
            "Epoch 483/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0021 - mae: 0.0371 - val_loss: 0.0545 - val_mae: 0.1915\n",
            "Epoch 484/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0032 - mae: 0.0434 - val_loss: 0.2182 - val_mae: 0.4345\n",
            "Epoch 485/700\n",
            "675/675 [==============================] - 0s 146us/step - loss: 0.0065 - mae: 0.0654 - val_loss: 0.0944 - val_mae: 0.2715\n",
            "Epoch 486/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0023 - mae: 0.0389 - val_loss: 0.0567 - val_mae: 0.1968\n",
            "Epoch 487/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 0.0014 - mae: 0.0306 - val_loss: 0.0493 - val_mae: 0.1769\n",
            "Epoch 488/700\n",
            "675/675 [==============================] - 0s 128us/step - loss: 2.1736e-04 - mae: 0.0111 - val_loss: 0.0828 - val_mae: 0.2533\n",
            "Epoch 489/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 3.5829e-04 - mae: 0.0148 - val_loss: 0.0416 - val_mae: 0.1617\n",
            "Epoch 490/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 4.8213e-04 - mae: 0.0170 - val_loss: 0.0550 - val_mae: 0.1940\n",
            "Epoch 491/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 8.6769e-04 - mae: 0.0230 - val_loss: 0.0296 - val_mae: 0.1282\n",
            "Epoch 492/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 7.8280e-04 - mae: 0.0207 - val_loss: 0.1569 - val_mae: 0.3686\n",
            "Epoch 493/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 0.0107 - mae: 0.0825 - val_loss: 0.1274 - val_mae: 0.3208\n",
            "Epoch 494/700\n",
            "675/675 [==============================] - 0s 151us/step - loss: 0.0025 - mae: 0.0389 - val_loss: 0.0677 - val_mae: 0.2194\n",
            "Epoch 495/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 6.0275e-04 - mae: 0.0192 - val_loss: 0.0515 - val_mae: 0.1842\n",
            "Epoch 496/700\n",
            "675/675 [==============================] - 0s 151us/step - loss: 3.4071e-04 - mae: 0.0142 - val_loss: 0.0490 - val_mae: 0.1770\n",
            "Epoch 497/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0010 - mae: 0.0255 - val_loss: 0.0536 - val_mae: 0.1866\n",
            "Epoch 498/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 8.1518e-04 - mae: 0.0208 - val_loss: 0.0856 - val_mae: 0.2568\n",
            "Epoch 499/700\n",
            "675/675 [==============================] - 0s 144us/step - loss: 3.4399e-04 - mae: 0.0145 - val_loss: 0.0759 - val_mae: 0.2384\n",
            "Epoch 500/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 3.2103e-04 - mae: 0.0140 - val_loss: 0.0400 - val_mae: 0.1550\n",
            "Epoch 501/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 4.3162e-04 - mae: 0.0163 - val_loss: 0.0345 - val_mae: 0.1401\n",
            "Epoch 502/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 3.1764e-04 - mae: 0.0139 - val_loss: 0.0543 - val_mae: 0.1943\n",
            "Epoch 503/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 3.9977e-04 - mae: 0.0157 - val_loss: 0.0559 - val_mae: 0.2008\n",
            "Epoch 504/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 8.9028e-04 - mae: 0.0238 - val_loss: 0.0280 - val_mae: 0.1253\n",
            "Epoch 505/700\n",
            "675/675 [==============================] - 0s 147us/step - loss: 0.0015 - mae: 0.0305 - val_loss: 0.0439 - val_mae: 0.1771\n",
            "Epoch 506/700\n",
            "675/675 [==============================] - 0s 146us/step - loss: 7.2344e-04 - mae: 0.0213 - val_loss: 0.0350 - val_mae: 0.1451\n",
            "Epoch 507/700\n",
            "675/675 [==============================] - 0s 153us/step - loss: 0.0015 - mae: 0.0267 - val_loss: 0.0497 - val_mae: 0.1842\n",
            "Epoch 508/700\n",
            "675/675 [==============================] - 0s 151us/step - loss: 5.6136e-04 - mae: 0.0183 - val_loss: 0.0495 - val_mae: 0.1849\n",
            "Epoch 509/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 5.5239e-04 - mae: 0.0184 - val_loss: 0.0384 - val_mae: 0.1566\n",
            "Epoch 510/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 3.4187e-04 - mae: 0.0140 - val_loss: 0.0343 - val_mae: 0.1470\n",
            "Epoch 511/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 0.0023 - mae: 0.0363 - val_loss: 0.0554 - val_mae: 0.1985\n",
            "Epoch 512/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0017 - mae: 0.0337 - val_loss: 0.0770 - val_mae: 0.2432\n",
            "Epoch 513/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0031 - mae: 0.0434 - val_loss: 0.0374 - val_mae: 0.1495\n",
            "Epoch 514/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 4.9398e-04 - mae: 0.0175 - val_loss: 0.0386 - val_mae: 0.1511\n",
            "Epoch 515/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 4.8016e-04 - mae: 0.0172 - val_loss: 0.0723 - val_mae: 0.2311\n",
            "Epoch 516/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 8.0201e-04 - mae: 0.0220 - val_loss: 0.0498 - val_mae: 0.1848\n",
            "Epoch 517/700\n",
            "675/675 [==============================] - 0s 152us/step - loss: 0.0016 - mae: 0.0318 - val_loss: 0.0378 - val_mae: 0.1491\n",
            "Epoch 518/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0014 - mae: 0.0302 - val_loss: 0.0739 - val_mae: 0.2358\n",
            "Epoch 519/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0011 - mae: 0.0270 - val_loss: 0.0520 - val_mae: 0.1888\n",
            "Epoch 520/700\n",
            "675/675 [==============================] - 0s 163us/step - loss: 5.3789e-04 - mae: 0.0170 - val_loss: 0.0411 - val_mae: 0.1601\n",
            "Epoch 521/700\n",
            "675/675 [==============================] - 0s 183us/step - loss: 0.0019 - mae: 0.0340 - val_loss: 0.1276 - val_mae: 0.3247\n",
            "Epoch 522/700\n",
            "675/675 [==============================] - 0s 178us/step - loss: 9.7003e-04 - mae: 0.0246 - val_loss: 0.0401 - val_mae: 0.1529\n",
            "Epoch 523/700\n",
            "675/675 [==============================] - 0s 162us/step - loss: 5.8043e-04 - mae: 0.0194 - val_loss: 0.0705 - val_mae: 0.2320\n",
            "Epoch 524/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 6.3322e-04 - mae: 0.0200 - val_loss: 0.0443 - val_mae: 0.1702\n",
            "Epoch 525/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 4.3365e-04 - mae: 0.0162 - val_loss: 0.0307 - val_mae: 0.1335\n",
            "Epoch 526/700\n",
            "675/675 [==============================] - 0s 145us/step - loss: 0.0011 - mae: 0.0249 - val_loss: 0.0490 - val_mae: 0.1852\n",
            "Epoch 527/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 5.9199e-04 - mae: 0.0198 - val_loss: 0.0478 - val_mae: 0.1819\n",
            "Epoch 528/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 3.6821e-04 - mae: 0.0147 - val_loss: 0.0184 - val_mae: 0.1010\n",
            "Epoch 529/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0018 - mae: 0.0333 - val_loss: 0.0419 - val_mae: 0.1600\n",
            "Epoch 530/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0050 - mae: 0.0475 - val_loss: 0.0984 - val_mae: 0.2463\n",
            "Epoch 531/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 0.0051 - mae: 0.0576 - val_loss: 0.2130 - val_mae: 0.4197\n",
            "Epoch 532/700\n",
            "675/675 [==============================] - 0s 128us/step - loss: 0.0022 - mae: 0.0366 - val_loss: 0.1125 - val_mae: 0.2917\n",
            "Epoch 533/700\n",
            "675/675 [==============================] - 0s 127us/step - loss: 5.9693e-04 - mae: 0.0193 - val_loss: 0.0787 - val_mae: 0.2342\n",
            "Epoch 534/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 3.7834e-04 - mae: 0.0155 - val_loss: 0.0907 - val_mae: 0.2635\n",
            "Epoch 535/700\n",
            "675/675 [==============================] - 0s 147us/step - loss: 2.5929e-04 - mae: 0.0125 - val_loss: 0.0521 - val_mae: 0.1833\n",
            "Epoch 536/700\n",
            "675/675 [==============================] - 0s 154us/step - loss: 3.8256e-04 - mae: 0.0155 - val_loss: 0.0197 - val_mae: 0.1082\n",
            "Epoch 537/700\n",
            "675/675 [==============================] - 0s 155us/step - loss: 0.0064 - mae: 0.0550 - val_loss: 0.2598 - val_mae: 0.4693\n",
            "Epoch 538/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 0.0013 - mae: 0.0263 - val_loss: 0.0931 - val_mae: 0.2639\n",
            "Epoch 539/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 8.9654e-04 - mae: 0.0237 - val_loss: 0.0970 - val_mae: 0.2719\n",
            "Epoch 540/700\n",
            "675/675 [==============================] - 0s 159us/step - loss: 3.3367e-04 - mae: 0.0142 - val_loss: 0.0773 - val_mae: 0.2407\n",
            "Epoch 541/700\n",
            "675/675 [==============================] - 0s 127us/step - loss: 3.5421e-04 - mae: 0.0143 - val_loss: 0.0552 - val_mae: 0.1936\n",
            "Epoch 542/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 2.7159e-04 - mae: 0.0129 - val_loss: 0.0644 - val_mae: 0.2187\n",
            "Epoch 543/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 4.7781e-04 - mae: 0.0170 - val_loss: 0.0929 - val_mae: 0.2732\n",
            "Epoch 544/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 3.8002e-04 - mae: 0.0152 - val_loss: 0.0600 - val_mae: 0.2098\n",
            "Epoch 545/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 2.5761e-04 - mae: 0.0123 - val_loss: 0.0339 - val_mae: 0.1424\n",
            "Epoch 546/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0012 - mae: 0.0267 - val_loss: 0.0522 - val_mae: 0.1907\n",
            "Epoch 547/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 0.0011 - mae: 0.0261 - val_loss: 0.0380 - val_mae: 0.1492\n",
            "Epoch 548/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 0.0020 - mae: 0.0331 - val_loss: 0.0241 - val_mae: 0.1161\n",
            "Epoch 549/700\n",
            "675/675 [==============================] - 0s 126us/step - loss: 0.0091 - mae: 0.0770 - val_loss: 0.0489 - val_mae: 0.1764\n",
            "Epoch 550/700\n",
            "675/675 [==============================] - 0s 128us/step - loss: 0.0012 - mae: 0.0263 - val_loss: 0.0269 - val_mae: 0.1216\n",
            "Epoch 551/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 8.8713e-04 - mae: 0.0237 - val_loss: 0.0820 - val_mae: 0.2547\n",
            "Epoch 552/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0042 - mae: 0.0501 - val_loss: 0.1112 - val_mae: 0.3020\n",
            "Epoch 553/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 0.0014 - mae: 0.0279 - val_loss: 0.0633 - val_mae: 0.2134\n",
            "Epoch 554/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 2.4799e-04 - mae: 0.0115 - val_loss: 0.0470 - val_mae: 0.1771\n",
            "Epoch 555/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 4.2633e-04 - mae: 0.0167 - val_loss: 0.0422 - val_mae: 0.1621\n",
            "Epoch 556/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 2.1950e-04 - mae: 0.0112 - val_loss: 0.0602 - val_mae: 0.2101\n",
            "Epoch 557/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 2.6057e-04 - mae: 0.0124 - val_loss: 0.0384 - val_mae: 0.1556\n",
            "Epoch 558/700\n",
            "675/675 [==============================] - 0s 155us/step - loss: 3.8450e-04 - mae: 0.0153 - val_loss: 0.0258 - val_mae: 0.1196\n",
            "Epoch 559/700\n",
            "675/675 [==============================] - 0s 170us/step - loss: 3.3586e-04 - mae: 0.0149 - val_loss: 0.0354 - val_mae: 0.1452\n",
            "Epoch 560/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 3.0559e-04 - mae: 0.0130 - val_loss: 0.0379 - val_mae: 0.1566\n",
            "Epoch 561/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 4.6601e-04 - mae: 0.0168 - val_loss: 0.0253 - val_mae: 0.1183\n",
            "Epoch 562/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 4.6083e-04 - mae: 0.0166 - val_loss: 0.0397 - val_mae: 0.1643\n",
            "Epoch 563/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 4.5116e-04 - mae: 0.0165 - val_loss: 0.0414 - val_mae: 0.1711\n",
            "Epoch 564/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0025 - mae: 0.0369 - val_loss: 0.1607 - val_mae: 0.3659\n",
            "Epoch 565/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0079 - mae: 0.0705 - val_loss: 0.1154 - val_mae: 0.2934\n",
            "Epoch 566/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 0.0017 - mae: 0.0325 - val_loss: 0.0206 - val_mae: 0.1149\n",
            "Epoch 567/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 0.0013 - mae: 0.0288 - val_loss: 0.0526 - val_mae: 0.1810\n",
            "Epoch 568/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 9.0349e-04 - mae: 0.0236 - val_loss: 0.0407 - val_mae: 0.1599\n",
            "Epoch 569/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 2.6362e-04 - mae: 0.0119 - val_loss: 0.0486 - val_mae: 0.1822\n",
            "Epoch 570/700\n",
            "675/675 [==============================] - 0s 154us/step - loss: 4.9126e-04 - mae: 0.0172 - val_loss: 0.0363 - val_mae: 0.1512\n",
            "Epoch 571/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 3.6885e-04 - mae: 0.0147 - val_loss: 0.0400 - val_mae: 0.1618\n",
            "Epoch 572/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 3.4670e-04 - mae: 0.0139 - val_loss: 0.0660 - val_mae: 0.2231\n",
            "Epoch 573/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 3.8361e-04 - mae: 0.0154 - val_loss: 0.0472 - val_mae: 0.1817\n",
            "Epoch 574/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 7.5807e-04 - mae: 0.0222 - val_loss: 0.0759 - val_mae: 0.2485\n",
            "Epoch 575/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0018 - mae: 0.0335 - val_loss: 0.0240 - val_mae: 0.1152\n",
            "Epoch 576/700\n",
            "675/675 [==============================] - 0s 144us/step - loss: 6.6816e-04 - mae: 0.0202 - val_loss: 0.0631 - val_mae: 0.2197\n",
            "Epoch 577/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 9.0288e-04 - mae: 0.0236 - val_loss: 0.0493 - val_mae: 0.1859\n",
            "Epoch 578/700\n",
            "675/675 [==============================] - 0s 144us/step - loss: 8.5658e-04 - mae: 0.0226 - val_loss: 0.0295 - val_mae: 0.1313\n",
            "Epoch 579/700\n",
            "675/675 [==============================] - 0s 148us/step - loss: 3.0081e-04 - mae: 0.0138 - val_loss: 0.0349 - val_mae: 0.1509\n",
            "Epoch 580/700\n",
            "675/675 [==============================] - 0s 146us/step - loss: 2.7102e-04 - mae: 0.0128 - val_loss: 0.0340 - val_mae: 0.1465\n",
            "Epoch 581/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 7.9194e-04 - mae: 0.0209 - val_loss: 0.0619 - val_mae: 0.2100\n",
            "Epoch 582/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0018 - mae: 0.0322 - val_loss: 0.0524 - val_mae: 0.1948\n",
            "Epoch 583/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 0.0050 - mae: 0.0511 - val_loss: 0.0267 - val_mae: 0.1217\n",
            "Epoch 584/700\n",
            "675/675 [==============================] - 0s 153us/step - loss: 4.7381e-04 - mae: 0.0175 - val_loss: 0.0464 - val_mae: 0.1763\n",
            "Epoch 585/700\n",
            "675/675 [==============================] - 0s 154us/step - loss: 6.2457e-04 - mae: 0.0193 - val_loss: 0.0137 - val_mae: 0.0922\n",
            "Epoch 586/700\n",
            "675/675 [==============================] - 0s 155us/step - loss: 0.0043 - mae: 0.0498 - val_loss: 0.0395 - val_mae: 0.1503\n",
            "Epoch 587/700\n",
            "675/675 [==============================] - 0s 163us/step - loss: 0.0078 - mae: 0.0703 - val_loss: 0.0712 - val_mae: 0.2246\n",
            "Epoch 588/700\n",
            "675/675 [==============================] - 0s 128us/step - loss: 0.0014 - mae: 0.0277 - val_loss: 0.0585 - val_mae: 0.2005\n",
            "Epoch 589/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 5.4636e-04 - mae: 0.0190 - val_loss: 0.0491 - val_mae: 0.1824\n",
            "Epoch 590/700\n",
            "675/675 [==============================] - 0s 127us/step - loss: 3.3664e-04 - mae: 0.0143 - val_loss: 0.0502 - val_mae: 0.1858\n",
            "Epoch 591/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 3.0262e-04 - mae: 0.0128 - val_loss: 0.0322 - val_mae: 0.1385\n",
            "Epoch 592/700\n",
            "675/675 [==============================] - 0s 127us/step - loss: 3.2936e-04 - mae: 0.0132 - val_loss: 0.0689 - val_mae: 0.2326\n",
            "Epoch 593/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 8.3959e-04 - mae: 0.0228 - val_loss: 0.0412 - val_mae: 0.1628\n",
            "Epoch 594/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 0.0011 - mae: 0.0258 - val_loss: 0.0491 - val_mae: 0.1810\n",
            "Epoch 595/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 5.4903e-04 - mae: 0.0174 - val_loss: 0.0354 - val_mae: 0.1468\n",
            "Epoch 596/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 3.2766e-04 - mae: 0.0143 - val_loss: 0.0376 - val_mae: 0.1562\n",
            "Epoch 597/700\n",
            "675/675 [==============================] - 0s 126us/step - loss: 0.0013 - mae: 0.0293 - val_loss: 0.0499 - val_mae: 0.1889\n",
            "Epoch 598/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 4.0925e-04 - mae: 0.0155 - val_loss: 0.0553 - val_mae: 0.2022\n",
            "Epoch 599/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 5.0249e-04 - mae: 0.0167 - val_loss: 0.0431 - val_mae: 0.1730\n",
            "Epoch 600/700\n",
            "675/675 [==============================] - 0s 149us/step - loss: 3.3740e-04 - mae: 0.0140 - val_loss: 0.0304 - val_mae: 0.1348\n",
            "Epoch 601/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 4.8913e-04 - mae: 0.0167 - val_loss: 0.1238 - val_mae: 0.3279\n",
            "Epoch 602/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 0.0037 - mae: 0.0447 - val_loss: 0.1068 - val_mae: 0.2796\n",
            "Epoch 603/700\n",
            "675/675 [==============================] - 0s 127us/step - loss: 0.0010 - mae: 0.0248 - val_loss: 0.0589 - val_mae: 0.1983\n",
            "Epoch 604/700\n",
            "675/675 [==============================] - 0s 157us/step - loss: 3.2893e-04 - mae: 0.0139 - val_loss: 0.0521 - val_mae: 0.1904\n",
            "Epoch 605/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 4.9558e-04 - mae: 0.0175 - val_loss: 0.0988 - val_mae: 0.2874\n",
            "Epoch 606/700\n",
            "675/675 [==============================] - 0s 154us/step - loss: 4.1245e-04 - mae: 0.0162 - val_loss: 0.0462 - val_mae: 0.1780\n",
            "Epoch 607/700\n",
            "675/675 [==============================] - 0s 162us/step - loss: 5.3477e-04 - mae: 0.0179 - val_loss: 0.0709 - val_mae: 0.2368\n",
            "Epoch 608/700\n",
            "675/675 [==============================] - 0s 155us/step - loss: 4.1809e-04 - mae: 0.0164 - val_loss: 0.0613 - val_mae: 0.2176\n",
            "Epoch 609/700\n",
            "675/675 [==============================] - 0s 152us/step - loss: 8.3260e-04 - mae: 0.0229 - val_loss: 0.0529 - val_mae: 0.1981\n",
            "Epoch 610/700\n",
            "675/675 [==============================] - 0s 155us/step - loss: 6.3389e-04 - mae: 0.0199 - val_loss: 0.0373 - val_mae: 0.1544\n",
            "Epoch 611/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 7.6699e-04 - mae: 0.0219 - val_loss: 0.0552 - val_mae: 0.2039\n",
            "Epoch 612/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0013 - mae: 0.0294 - val_loss: 0.0318 - val_mae: 0.1412\n",
            "Epoch 613/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0032 - mae: 0.0440 - val_loss: 0.1161 - val_mae: 0.3145\n",
            "Epoch 614/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 0.0021 - mae: 0.0333 - val_loss: 0.0298 - val_mae: 0.1339\n",
            "Epoch 615/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 7.5835e-04 - mae: 0.0199 - val_loss: 0.0808 - val_mae: 0.2534\n",
            "Epoch 616/700\n",
            "675/675 [==============================] - 0s 145us/step - loss: 3.9897e-04 - mae: 0.0156 - val_loss: 0.0237 - val_mae: 0.1145\n",
            "Epoch 617/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 2.9798e-04 - mae: 0.0130 - val_loss: 0.0351 - val_mae: 0.1519\n",
            "Epoch 618/700\n",
            "675/675 [==============================] - 0s 128us/step - loss: 0.0012 - mae: 0.0260 - val_loss: 0.0795 - val_mae: 0.2567\n",
            "Epoch 619/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 9.0729e-04 - mae: 0.0228 - val_loss: 0.0305 - val_mae: 0.1374\n",
            "Epoch 620/700\n",
            "675/675 [==============================] - 0s 127us/step - loss: 5.4744e-04 - mae: 0.0179 - val_loss: 0.0515 - val_mae: 0.1980\n",
            "Epoch 621/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 0.0029 - mae: 0.0411 - val_loss: 0.0427 - val_mae: 0.1724\n",
            "Epoch 622/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 2.2894e-04 - mae: 0.0117 - val_loss: 0.0299 - val_mae: 0.1373\n",
            "Epoch 623/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 2.9599e-04 - mae: 0.0129 - val_loss: 0.0238 - val_mae: 0.1167\n",
            "Epoch 624/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 2.6235e-04 - mae: 0.0121 - val_loss: 0.0277 - val_mae: 0.1298\n",
            "Epoch 625/700\n",
            "675/675 [==============================] - 0s 143us/step - loss: 5.4897e-04 - mae: 0.0182 - val_loss: 0.0606 - val_mae: 0.2161\n",
            "Epoch 626/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 7.8715e-04 - mae: 0.0221 - val_loss: 0.0609 - val_mae: 0.2179\n",
            "Epoch 627/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 0.0015 - mae: 0.0287 - val_loss: 0.0366 - val_mae: 0.1571\n",
            "Epoch 628/700\n",
            "675/675 [==============================] - 0s 158us/step - loss: 0.0035 - mae: 0.0370 - val_loss: 0.0599 - val_mae: 0.2054\n",
            "Epoch 629/700\n",
            "675/675 [==============================] - 0s 150us/step - loss: 0.0049 - mae: 0.0519 - val_loss: 0.0086 - val_mae: 0.0825\n",
            "Epoch 630/700\n",
            "675/675 [==============================] - 0s 150us/step - loss: 9.4081e-04 - mae: 0.0219 - val_loss: 0.0303 - val_mae: 0.1402\n",
            "Epoch 631/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 3.0788e-04 - mae: 0.0136 - val_loss: 0.0158 - val_mae: 0.0927\n",
            "Epoch 632/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 7.9976e-04 - mae: 0.0228 - val_loss: 0.0481 - val_mae: 0.1877\n",
            "Epoch 633/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 8.9235e-04 - mae: 0.0239 - val_loss: 0.0204 - val_mae: 0.1055\n",
            "Epoch 634/700\n",
            "675/675 [==============================] - 0s 131us/step - loss: 3.2591e-04 - mae: 0.0137 - val_loss: 0.0321 - val_mae: 0.1421\n",
            "Epoch 635/700\n",
            "675/675 [==============================] - 0s 126us/step - loss: 2.9122e-04 - mae: 0.0128 - val_loss: 0.0198 - val_mae: 0.1029\n",
            "Epoch 636/700\n",
            "675/675 [==============================] - 0s 128us/step - loss: 5.9035e-04 - mae: 0.0195 - val_loss: 0.0230 - val_mae: 0.1129\n",
            "Epoch 637/700\n",
            "675/675 [==============================] - 0s 128us/step - loss: 9.5307e-04 - mae: 0.0247 - val_loss: 0.0254 - val_mae: 0.1198\n",
            "Epoch 638/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 8.0909e-04 - mae: 0.0230 - val_loss: 0.0165 - val_mae: 0.0953\n",
            "Epoch 639/700\n",
            "675/675 [==============================] - 0s 128us/step - loss: 7.8208e-04 - mae: 0.0219 - val_loss: 0.0904 - val_mae: 0.2738\n",
            "Epoch 640/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 3.1117e-04 - mae: 0.0140 - val_loss: 0.0228 - val_mae: 0.1118\n",
            "Epoch 641/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 2.1202e-04 - mae: 0.0111 - val_loss: 0.0375 - val_mae: 0.1609\n",
            "Epoch 642/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0019 - mae: 0.0342 - val_loss: 0.0299 - val_mae: 0.1320\n",
            "Epoch 643/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 0.0031 - mae: 0.0446 - val_loss: 0.0554 - val_mae: 0.1945\n",
            "Epoch 644/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 9.0960e-04 - mae: 0.0224 - val_loss: 0.0353 - val_mae: 0.1517\n",
            "Epoch 645/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 3.1088e-04 - mae: 0.0138 - val_loss: 0.0291 - val_mae: 0.1318\n",
            "Epoch 646/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 2.5130e-04 - mae: 0.0121 - val_loss: 0.0309 - val_mae: 0.1402\n",
            "Epoch 647/700\n",
            "675/675 [==============================] - 0s 140us/step - loss: 3.0847e-04 - mae: 0.0135 - val_loss: 0.0140 - val_mae: 0.0896\n",
            "Epoch 648/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 7.2319e-04 - mae: 0.0216 - val_loss: 0.0214 - val_mae: 0.1083\n",
            "Epoch 649/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 0.0012 - mae: 0.0244 - val_loss: 0.0245 - val_mae: 0.1176\n",
            "Epoch 650/700\n",
            "675/675 [==============================] - 0s 162us/step - loss: 3.0184e-04 - mae: 0.0133 - val_loss: 0.0305 - val_mae: 0.1391\n",
            "Epoch 651/700\n",
            "675/675 [==============================] - 0s 145us/step - loss: 1.9663e-04 - mae: 0.0111 - val_loss: 0.0310 - val_mae: 0.1422\n",
            "Epoch 652/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 6.1964e-04 - mae: 0.0192 - val_loss: 0.0335 - val_mae: 0.1451\n",
            "Epoch 653/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 4.6806e-04 - mae: 0.0170 - val_loss: 0.0507 - val_mae: 0.1941\n",
            "Epoch 654/700\n",
            "675/675 [==============================] - 0s 151us/step - loss: 0.0012 - mae: 0.0268 - val_loss: 0.0673 - val_mae: 0.2283\n",
            "Epoch 655/700\n",
            "675/675 [==============================] - 0s 181us/step - loss: 3.9436e-04 - mae: 0.0154 - val_loss: 0.0295 - val_mae: 0.1328\n",
            "Epoch 656/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 5.9697e-04 - mae: 0.0192 - val_loss: 0.0452 - val_mae: 0.1799\n",
            "Epoch 657/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 6.4091e-04 - mae: 0.0203 - val_loss: 0.0385 - val_mae: 0.1629\n",
            "Epoch 658/700\n",
            "675/675 [==============================] - 0s 147us/step - loss: 2.2057e-04 - mae: 0.0117 - val_loss: 0.0334 - val_mae: 0.1456\n",
            "Epoch 659/700\n",
            "675/675 [==============================] - 0s 133us/step - loss: 7.8297e-04 - mae: 0.0219 - val_loss: 0.0473 - val_mae: 0.1925\n",
            "Epoch 660/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 0.0041 - mae: 0.0515 - val_loss: 0.1617 - val_mae: 0.3768\n",
            "Epoch 661/700\n",
            "675/675 [==============================] - 0s 138us/step - loss: 0.0028 - mae: 0.0425 - val_loss: 0.0199 - val_mae: 0.1103\n",
            "Epoch 662/700\n",
            "675/675 [==============================] - 0s 147us/step - loss: 0.0024 - mae: 0.0360 - val_loss: 0.0454 - val_mae: 0.1707\n",
            "Epoch 663/700\n",
            "675/675 [==============================] - 0s 157us/step - loss: 0.0015 - mae: 0.0322 - val_loss: 0.0451 - val_mae: 0.1732\n",
            "Epoch 664/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 5.0831e-04 - mae: 0.0171 - val_loss: 0.0287 - val_mae: 0.1291\n",
            "Epoch 665/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 7.8473e-04 - mae: 0.0226 - val_loss: 0.0531 - val_mae: 0.1974\n",
            "Epoch 666/700\n",
            "675/675 [==============================] - 0s 144us/step - loss: 8.2990e-04 - mae: 0.0233 - val_loss: 0.0467 - val_mae: 0.1843\n",
            "Epoch 667/700\n",
            "675/675 [==============================] - 0s 160us/step - loss: 9.6304e-04 - mae: 0.0197 - val_loss: 0.0490 - val_mae: 0.1855\n",
            "Epoch 668/700\n",
            "675/675 [==============================] - 0s 166us/step - loss: 3.8054e-04 - mae: 0.0148 - val_loss: 0.0873 - val_mae: 0.2682\n",
            "Epoch 669/700\n",
            "675/675 [==============================] - 0s 156us/step - loss: 8.8529e-04 - mae: 0.0233 - val_loss: 0.0089 - val_mae: 0.0834\n",
            "Epoch 670/700\n",
            "675/675 [==============================] - 0s 153us/step - loss: 9.6576e-04 - mae: 0.0240 - val_loss: 0.0428 - val_mae: 0.1706\n",
            "Epoch 671/700\n",
            "675/675 [==============================] - 0s 154us/step - loss: 3.2508e-04 - mae: 0.0137 - val_loss: 0.0315 - val_mae: 0.1401\n",
            "Epoch 672/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 5.5591e-04 - mae: 0.0180 - val_loss: 0.0461 - val_mae: 0.1843\n",
            "Epoch 673/700\n",
            "675/675 [==============================] - 0s 146us/step - loss: 5.9562e-04 - mae: 0.0183 - val_loss: 0.0534 - val_mae: 0.2006\n",
            "Epoch 674/700\n",
            "675/675 [==============================] - 0s 143us/step - loss: 7.2772e-04 - mae: 0.0219 - val_loss: 0.0399 - val_mae: 0.1654\n",
            "Epoch 675/700\n",
            "675/675 [==============================] - 0s 162us/step - loss: 4.3034e-04 - mae: 0.0166 - val_loss: 0.0198 - val_mae: 0.1042\n",
            "Epoch 676/700\n",
            "675/675 [==============================] - 0s 137us/step - loss: 5.9226e-04 - mae: 0.0191 - val_loss: 0.0129 - val_mae: 0.0828\n",
            "Epoch 677/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 3.3800e-04 - mae: 0.0141 - val_loss: 0.0189 - val_mae: 0.1008\n",
            "Epoch 678/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 8.7935e-04 - mae: 0.0227 - val_loss: 0.0545 - val_mae: 0.2011\n",
            "Epoch 679/700\n",
            "675/675 [==============================] - 0s 130us/step - loss: 6.3037e-04 - mae: 0.0190 - val_loss: 0.0231 - val_mae: 0.1140\n",
            "Epoch 680/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 0.0037 - mae: 0.0432 - val_loss: 0.2558 - val_mae: 0.4709\n",
            "Epoch 681/700\n",
            "675/675 [==============================] - 0s 134us/step - loss: 0.0112 - mae: 0.0848 - val_loss: 0.1170 - val_mae: 0.3057\n",
            "Epoch 682/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 0.0010 - mae: 0.0234 - val_loss: 0.0437 - val_mae: 0.1707\n",
            "Epoch 683/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 2.4248e-04 - mae: 0.0122 - val_loss: 0.0388 - val_mae: 0.1626\n",
            "Epoch 684/700\n",
            "675/675 [==============================] - 0s 142us/step - loss: 1.9988e-04 - mae: 0.0109 - val_loss: 0.0327 - val_mae: 0.1466\n",
            "Epoch 685/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 3.0965e-04 - mae: 0.0136 - val_loss: 0.0246 - val_mae: 0.1213\n",
            "Epoch 686/700\n",
            "675/675 [==============================] - 0s 176us/step - loss: 3.9482e-04 - mae: 0.0156 - val_loss: 0.0308 - val_mae: 0.1417\n",
            "Epoch 687/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 3.0610e-04 - mae: 0.0142 - val_loss: 0.0315 - val_mae: 0.1445\n",
            "Epoch 688/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 1.5569e-04 - mae: 0.0097 - val_loss: 0.0179 - val_mae: 0.0983\n",
            "Epoch 689/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 4.2230e-04 - mae: 0.0165 - val_loss: 0.0119 - val_mae: 0.0822\n",
            "Epoch 690/700\n",
            "675/675 [==============================] - 0s 132us/step - loss: 4.4246e-04 - mae: 0.0166 - val_loss: 0.0256 - val_mae: 0.1286\n",
            "Epoch 691/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 2.0570e-04 - mae: 0.0112 - val_loss: 0.0296 - val_mae: 0.1428\n",
            "Epoch 692/700\n",
            "675/675 [==============================] - 0s 135us/step - loss: 2.3325e-04 - mae: 0.0118 - val_loss: 0.0192 - val_mae: 0.1028\n",
            "Epoch 693/700\n",
            "675/675 [==============================] - 0s 147us/step - loss: 4.7637e-04 - mae: 0.0164 - val_loss: 0.0114 - val_mae: 0.0798\n",
            "Epoch 694/700\n",
            "675/675 [==============================] - 0s 143us/step - loss: 7.5155e-04 - mae: 0.0212 - val_loss: 0.0341 - val_mae: 0.1541\n",
            "Epoch 695/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 2.0822e-04 - mae: 0.0114 - val_loss: 0.0245 - val_mae: 0.1228\n",
            "Epoch 696/700\n",
            "675/675 [==============================] - 0s 141us/step - loss: 5.3642e-04 - mae: 0.0187 - val_loss: 0.0432 - val_mae: 0.1781\n",
            "Epoch 697/700\n",
            "675/675 [==============================] - 0s 125us/step - loss: 3.4263e-04 - mae: 0.0145 - val_loss: 0.0308 - val_mae: 0.1435\n",
            "Epoch 698/700\n",
            "675/675 [==============================] - 0s 136us/step - loss: 4.5016e-04 - mae: 0.0158 - val_loss: 0.0063 - val_mae: 0.0691\n",
            "Epoch 699/700\n",
            "675/675 [==============================] - 0s 139us/step - loss: 0.0026 - mae: 0.0374 - val_loss: 0.3132 - val_mae: 0.5342\n",
            "Epoch 700/700\n",
            "675/675 [==============================] - 0s 129us/step - loss: 0.0023 - mae: 0.0360 - val_loss: 0.1216 - val_mae: 0.3171\n",
            "[4.6744762855567314, 4.655614488376917, 4.636687130294606, 4.617696104029839, 4.5986433086694145, 4.57953064947699, 4.560360037702557, 4.5411333903913205, 4.5218526301919875, 4.502519685164511, 4.483136488587283, 4.463704978763809, 4.444227098828877, 4.424704796554252, 4.405140024153889, 4.385534738088728, 4.365890898871035, 4.346210470868363, 4.3264954221071115, 4.306747724075729, 4.286969351527561, 4.267162282283381, 4.247328497033605, 4.227469979140233, 4.207588714438506, 4.187686691038325, 4.1677658991254445, 4.147828330762454, 4.12787597968958, 4.1079108411253, 4.0879349115668315, 4.067950188590484, 4.047958670651905, 4.027962356886226, 4.00796324690816, 3.987963340612037, 3.967964637971824, 3.947969138841118, 3.9279788427531694, 3.9079957487209263, 3.8880218550371426, 3.8680591590745403, 3.84810965708608, 3.828175344005335, 3.8082582132470044, 3.7883602565075645, 3.7684834635661084, 3.748629822085364, 3.728801317412938, 3.708999932382773, 3.68922764711687, 3.66948643882728, 3.6497782816183797, 3.630105146289467, 3.610469000137677, 3.5908718067612693, 3.571315525863246, 3.5518021130554005, 3.5323335196627528, 3.5129116925284176, 3.4935385738189244, 3.474216100829999, 3.454946205792836, 3.435730815680887, 3.416571852017147, 3.397471230682017, 3.378430861721713, 3.359452649157264, 3.340538490794112, 3.3216902780323307, 3.3029098956774874, 3.2841992217521723, 3.265560127308184, 3.2469944762394336, 3.228504125095556, 3.2100909228962586, 3.191756710946416, 3.173503322651945, 3.1553325833364636, 3.1372463100587584, 3.1192463114310915, 3.1013343874383223, 3.083512329257924, 3.0657819190808624, 3.0481449299333807, 3.0306031254996957, 3.0131582599456324, 2.9958120777432065, 2.978566313496188, 2.9614226917666286, 2.9443829269024144, 2.927448722865832, 2.9106217730631734, 2.893903760175396, 2.877296355989857, 2.8608012212331335, 2.8444200054049595, 2.8281543466132657, 2.812005871410376, 2.7959761946303527, 2.780066919227517, 2.764279636116151, 2.7486159240114096, 2.7330773492714506, 2.7176654657407964, 2.702381814594962, 2.687227924186324, 2.6722053098912935, 2.657315473958782, 2.642559905359975, 2.6279400796394357, 2.6134574587675528, 2.599113490994342, 2.584909610704633, 2.570847238274616, 2.5569277799298145, 2.5431526276044636, 2.5295231588023173, 2.5160407364588973, 2.502706708805203, 2.489522409232885, 2.4764891561609192, 2.4636082529037493, 2.450880987540967, 2.4383086327885017, 2.425892445871353, 2.413633668397865, 2.4015335262355695, 2.3895932293886, 2.377813971876688, 2.3661969316157734, 2.354743270300199, 2.343454133286552, 2.3323306494791263, 2.3213739312170327, 2.310585074162967, 2.2999651571936437, 2.289515242291908, 2.279236374440546, 2.2691295815177757, 2.2591958741944658, 2.249436245833071, 2.239851672388295, 2.2304431123094943, 2.2212115064448374, 2.2121577779472155, 2.2032828321819364, 2.19458755663618, 2.186072820830255, 2.177739476230646, 2.1695883561648683, 2.1616202757381364, 2.1538360317518523, 2.1462364026239285, 2.1388221483109433, 2.1315940102321513, 2.124552711195337, 2.117698955324535, 2.111033427989622, 2.104556795737776, 2.098269706226826, 2.0921727881604832, 2.0862666512254724, 2.080551886030568, 2.0750290640475253, 2.0696987375539413, 2.0645614395780223, 2.0596176838452855, 2.0548679647271846, 2.050312757191673, 2.0459525167557073, 2.0417876794396994, 2.037818661723909, 2.0340458605068013, 2.030469653065353, 2.0270903970173277, 2.0239084302855153, 2.020924071063937, 2.018137617786028, 2.015549349094793, 2.013159523814946, 2.0109683809270225, 2.008976139543485, 2.0071829988868126, 2.005589138269576, 2.0041947170765093, 2.002999874748569, 2.0020047307689928, 2.0012093846513492, 2.000613915929587, 2.0002183841500827, 2.000022828865684, 2.0000272696317563, 2.000231706004228, 2.0006361175396306, 2.001240463797148, 2.0020446843426587, 2.0030486987547778, 2.004252406632901, 2.005655687607243, 2.0072584013508763, 2.0090603875937623, 2.011061466138778, 2.013261436879737, 2.015660079821398, 2.0182571551014656, 2.021052403014576, 2.024045544038268, 2.027236278860932, 2.030624288411746, 2.034209233892578, 2.037990756811867, 2.0419684790204737, 2.0461420027494897, 2.0505109106500212, 2.0550747658349193, 2.059833111922468, 2.064785473082024, 2.0699313540815982, 2.0752702403373795, 2.0808015979651917, 2.0865248738338797, 2.0924394956206274, 2.0985448718681843, 2.1048403920440135, 2.1113254266013444, 2.1179993270421242, 2.1248614259818717, 2.1319110372164105, 2.1391474557904937, 2.146569958068291, 2.154177801805761, 2.161970226224871, 2.1699464520896723, 2.178105681784225, 2.1864470993923586, 2.194969870779264, 2.203673143674907, 2.2125560477592465, 2.2216176947492783, 2.230857178487855, 2.2402735750343017, 2.2498659427568115, 2.2596333224266054, 2.2695747373138557, 2.2796891932853605, 2.2899756789039474, 2.300433165529628, 2.311060607422455]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHvWqr856HwC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "2931b954-e0e8-4f76-9a2d-f35f1c0225d1"
      },
      "source": [
        "pyplot.scatter(xtrain,yhat, label='Predicted')\n",
        "pyplot.scatter(xtrain,ytrain,label='Real')"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f39dda83400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}